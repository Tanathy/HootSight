{
  "language_name": "Español",
  "activations": {
    "not_supported": "Activación no soportada: {activation}. Opciones disponibles: {available}",
    "created": "Función de activación creada correctamente: {activation} con parámetros {params}",
    "creation_failed": "Error al crear función de activación: {error}",
    "invalid_params": "Parámetros inválidos para función de activación: {error}",
    "config_missing_type": "Falta 'type' en configuración de activación",
    "recommendations_removed": "Recomendaciones de activación eliminadas del sistema"
  },
  "augmentation": {
    "not_supported": "Aumento no soportado: {aug}. Opciones disponibles: {available}",
    "created": "Aumento creado correctamente: {aug} con parámetros {params}",
    "creation_failed": "Error al crear aumento: {error}",
    "invalid_params": "Parámetros inválidos para aumento: {error}",
    "config_missing_type": "Falta 'type' en configuración de aumento",
    "preview_invalid_phase": "Fase de aumento inválida: {phase}",
    "preview_no_images": "No hay imágenes disponibles para vista previa de aumento en proyecto {project}.",
    "preview_failed": "Error al generar vista previa de aumento: {error}",
    "preview_generated": "Vista previa de aumento generada para fase {phase}"
  },
  "coordinator_settings": {
    "user_settings_saved": "Configuración de usuario guardada en {path}",
    "user_settings_save_failed": "Error al guardar configuración de usuario: {error}"
  },
  "recommendations": {
    "critical_shortage": "Escasez crítica de datos detectada en: {labels}",
    "reduce_oversampled": "Considera reducir etiquetas sobremuestreadas: {labels}",
    "augment_undersampled": "Considera aumentar etiquetas submuestreadas: {labels}",
    "weighted_loss": "Considera usar funciones de pérdida ponderadas",
    "stratified_sampling": "Considera usar muestreo estratificado",
    "hierarchical_imbalance": "Desequilibrio jerárquico detectado en: {categories}",
    "small_dataset": "Conjunto de datos pequeño detectado - considera aumento de datos",
    "tiny_dataset": "Conjunto de datos muy pequeño - alto riesgo de sobreajuste"
  },
  "losses": {
    "not_supported": "Función de pérdida no soportada: {loss}. Opciones disponibles: {available}",
    "created": "Función de pérdida creada correctamente: {loss} con parámetros {params}",
    "creation_failed": "Error al crear función de pérdida: {error}",
    "invalid_params": "Parámetros inválidos para función de pérdida: {error}",
    "config_missing_type": "Falta 'type' en configuración de pérdida",
    "recommendations_removed": "Recomendaciones de pérdida eliminadas del sistema"
  },
  "normalization": {
    "not_supported": "Normalización no soportada: {norm}. Opciones disponibles: {available}",
    "missing_num_features": "Falta parámetro 'num_features' para {norm}",
    "missing_num_channels": "Falta parámetro 'num_channels' para {norm}",
    "missing_normalized_shape": "Falta parámetro 'normalized_shape' para {norm}",
    "created": "Capa de normalización creada correctamente: {norm} con parámetros {params}",
    "creation_failed": "Error al crear capa de normalización: {error}",
    "invalid_params": "Parámetros inválidos para capa de normalización: {error}",
    "config_missing_type": "Falta 'type' en configuración de normalización",
    "recommendations_removed": "Recomendaciones de normalización eliminadas del sistema"
  },
  "optimizers": {
    "not_supported": "Optimizador no soportado: {optimizer}. Opciones disponibles: {available}",
    "created": "Optimizador creado correctamente: {optimizer} con parámetros {params}",
    "creation_failed": "Error al crear optimizador: {error}",
    "invalid_params": "Parámetros inválidos para optimizador: {error}",
    "config_missing_type": "Falta 'type' en configuración de optimizador",
    "recommendations_removed": "Recomendaciones de optimizador eliminadas del sistema"
  },
  "pooling": {
    "not_supported": "Capa de pooling no soportada: {pool}. Opciones disponibles: {available}",
    "created": "Capa de pooling creada correctamente: {pool} con parámetros {params}",
    "creation_failed": "Error al crear capa de pooling: {error}",
    "invalid_params": "Parámetros inválidos para capa de pooling: {error}",
    "config_missing_type": "Falta 'type' en configuración de pooling",
    "recommendations_removed": "Recomendaciones de pooling eliminadas del sistema"
  },
  "regularization": {
    "not_supported": "Regularización no soportada: {reg}. Opciones disponibles: {available}",
    "created": "Capa de regularización creada correctamente: {reg} con parámetros {params}",
    "creation_failed": "Error al crear capa de regularización: {error}",
    "invalid_params": "Parámetros inválidos para capa de regularización: {error}",
    "config_missing_type": "Falta 'type' en configuración de regularización",
    "recommendations_removed": "Recomendaciones de regularización eliminadas del sistema"
  },
  "schedulers": {
    "not_supported": "Scheduler no soportado: {scheduler}. Opciones disponibles: {available}",
    "created": "Scheduler creado correctamente: {scheduler} con parámetros {params}",
    "creation_failed": "Error al crear scheduler: {error}",
    "invalid_params": "Parámetros inválidos para scheduler: {error}",
    "config_missing_type": "Falta 'type' en configuración de scheduler",
    "recommendations_removed": "Recomendaciones de scheduler eliminadas del sistema"
  },
  "weight_init": {
    "not_supported": "Inicialización de pesos no soportada: {init}. Opciones disponibles: {available}",
    "applied": "Inicialización de pesos aplicada correctamente: {init} a {module} con parámetros {params}",
    "application_failed": "Error al aplicar inicialización de pesos: {error}",
    "invalid_params": "Parámetros inválidos para inicialización de pesos: {error}",
    "recommendations_removed": "Recomendaciones de inicialización de pesos eliminadas del sistema"
  },
  "heatmap": {
    "no_images": "No se encontraron imágenes para proyecto {project}",
    "checkpoint_missing": "Checkpoint del modelo no encontrado en directorio: {dir}",
    "project_or_dataset_missing": "Proyecto {project} o su conjunto de datos no encontrado",
    "generated": "Mapa de calor generado para {project} usando imagen {image} (clase {clazz})"
  },
  "app": {
    "brand": "Hootsight"
  },
  "nav": {
    "training_group": "Entrenamiento",
    "projects": "Proyectos",
    "dataset": "Conjunto de datos",
    "training_setup": "Configuración de entrenamiento",
    "augmentation": "Aumento",
    "status_group": "Estado",
    "status": "Estado",
    "heatmap": "Mapa de calor",
    "memory": "Memoria",
    "system_group": "Sistema",
    "updates": "Actualizaciones",
    "about": "Acerca de"
  },
  "page": {
    "projects": "Proyectos",
    "dataset": "Conjunto de datos",
    "training": "Configuración de entrenamiento",
    "augmentation": "Aumento",
    "status": "Estado",
    "heatmap": "Mapa de calor",
    "memory": "Memoria",
    "updates": "Actualizaciones",
    "about": "Acerca de"
  },
  "config": {
    "sections": {
      "training": "Entrenamiento",
      "optimizers": "Optimizadores",
      "schedulers": "Schedulers",
      "losses": "Pérdidas",
      "models": "Modelos"
    },
    "entities": {
      "optimizers": {
        "sgd": "SGD (Gradiente Descendente Estocástico)",
        "adam": "Optimizador Adam",
        "adamw": "Optimizador AdamW",
        "adamax": "AdaMax",
        "nadam": "Nesterov Adam",
        "radam": "Adam Rectangular",
        "rmsprop": "RMSprop",
        "rprop": "Retropropagación Resiliente",
        "adagrad": "AdaGrad",
        "adadelta": "AdaDelta",
        "sparse_adam": "Adam Disperso",
        "lbfgs": "L-BFGS",
        "asgd": "SGD Promediado"
      },
      "schedulers": {
        "step_lr": "Tasa de aprendizaje por pasos",
        "multi_step_lr": "Tasa de aprendizaje multi-pasos",
        "exponential_lr": "Tasa de aprendizaje exponencial",
        "cosine_annealing_lr": "Recocido coseno",
        "cosine_annealing_warm_restarts": "Recocido coseno con reinicios cálidos",
        "reduce_lr_on_plateau": "Reducir LR en meseta",
        "cyclic_lr": "Tasa de aprendizaje cíclica",
        "one_cycle_lr": "Una tasa de aprendizaje cíclica",
        "polynomial_lr": "Tasa de aprendizaje polinomial",
        "linear_lr": "Tasa de aprendizaje lineal",
        "lambda_lr": "Tasa de aprendizaje lambda",
        "multiplicative_lr": "Tasa de aprendizaje multiplicativa"
      },
      "losses": {
        "cross_entropy": "Entropía cruzada",
        "nll_loss": "Pérdida de log-verosimilitud negativa",
        "bce_loss": "Entropía cruzada binaria",
        "bce_with_logits": "BCE con logits",
        "multi_margin": "Pérdida de margen multi-clase",
        "multi_label_margin": "Pérdida de margen multi-etiqueta",
        "multi_label_soft_margin": "Margen suave multi-etiqueta",
        "mse_loss": "Error cuadrático medio",
        "l1_loss": "Pérdida L1 (MAE)",
        "smooth_l1": "L1 suave",
        "huber_loss": "Pérdida Huber",
        "kl_div": "Divergencia KL",
        "margin_ranking": "Clasificación de margen",
        "hinge_embedding": "Incrustación bisagra",
        "triplet_margin": "Margen triple",
        "cosine_embedding": "Incrustación coseno",
        "ctc_loss": "Pérdida CTC",
        "poisson_nll": "NLL Poisson",
        "gaussian_nll": "NLL Gaussiana"
      }
    }
  },
  "groups": {
    "model_settings": "Configuración del modelo",
    "task_configuration": "Configuración de tarea",
    "training_parameters": "Parámetros de entrenamiento",
    "optimizer_settings": "Configuración del optimizador",
    "scheduler_settings": "Configuración del scheduler",
    "loss_configuration": "Configuración de pérdida",
    "data_loading": "Carga de datos",
    "normalization": "Normalización",
    "checkpointing": "Checkpointing",
    "weight_initialization": "Inicialización de pesos"
  },
  "actions": {
    "save_config": "Guardar configuración",
    "export": "Exportar",
    "save_training_config": "Guardar configuración de entrenamiento",
    "save_system_settings": "Guardar configuración global"
  },
  "footer": {
    "tagline": "Configurado",
    "generated": "",
    "ready": "Listo"
  },
  "field": {
    "training_model_type": "Tipo de modelo",
    "training_model_name": "Nombre del modelo",
    "training_pretrained": "Preentrenado",
    "training_task": "Tarea",
    "training_batch_size": "Tamaño de lote",
    "training_epochs": "Épocas",
    "training_learning_rate": "Tasa de aprendizaje",
    "training_weight_decay": "Decaimiento de pesos",
    "training_input_size": "Tamaño de entrada",
    "training_val_ratio": "Ratio de validación",
    "training_optimizer_type": "Tipo de optimizador",
    "training_scheduler_type": "Tipo de scheduler",
    "training_loss_type": "Tipo de pérdida",
    "training_dataloader": "DataLoader",
    "training_dataloader_num_workers": "Num trabajadores",
    "training_dataloader_pin_memory": "Fijar memoria",
    "training_dataloader_persistent_workers": "Trabajadores persistentes",
    "training_dataloader_prefetch_factor": "Factor de prefetch",
    "training_normalize": "Normalizar",
    "training_normalize_mean": "Media",
    "training_normalize_std": "Desviación estándar",
    "training_checkpoint": "Checkpoint",
    "training_checkpoint_save_best_only": "Guardar solo mejor",
    "training_checkpoint_save_frequency": "Frecuencia de guardado",
    "training_checkpoint_max_checkpoints": "Máx checkpoints",
    "training_checkpoint_checkpoint_dir": "Dir de checkpoint",
    "training_checkpoint_best_model_filename": "Nombre archivo mejor modelo",
    "training_checkpoint_training_history_filename": "Nombre archivo historial entrenamiento",
    "training_weight_init": "Init pesos",
    "training_weight_init_type": "Tipo de init",
    "training_weight_init_params": "Parámetros de init",
    "training_optimizer_params_adamw_lr": "Tasa de aprendizaje",
    "training_optimizer_params_adamw_betas": "Betas",
    "training_optimizer_params_adamw_eps": "Eps",
    "training_optimizer_params_adamw_weight_decay": "Decaimiento de pesos",
    "training_optimizer_params_adamw_amsgrad": "Amsgrad",
    "training_scheduler_params_step_lr_step_size": "Tamaño de paso",
    "training_scheduler_params_step_lr_gamma": "Gamma",
    "training_scheduler_params_step_lr_last_epoch": "Última época",
    "training_loss_params_bce_with_logits_weight": "Peso",
    "training_loss_params_bce_with_logits_size_average": "Promedio de tamaño",
    "training_loss_params_bce_with_logits_reduce": "Reducir",
    "training_loss_params_bce_with_logits_reduction": "Reducción",
    "training_loss_params_bce_with_logits_pos_weight": "Peso positivo"
  },
  "ui": {
    "generate_heatmap": "Generar mapa de calor",
    "no_heatmap_generated": "Aún no se ha generado ningún mapa de calor.",
    "no_data_available": "No hay datos disponibles.",
    "page_not_implemented": "Página no implementada",
    "error": "Error",
    "schema_not_loaded": "Esquema aún no cargado. Por favor espera...",
    "config_not_loaded": "Config aún no cargada. Por favor espera...",
    "augmentation_phase": "Aumento {phase}",
    "add": "Agregar",
    "remove": "Remover",
    "transform": "transformar",
    "no_project_loaded": "Ningún proyecto cargado",
    "load_project_first": "Por favor carga un proyecto primero desde la pestaña Proyectos.",
    "go_to_projects": "Ir a Proyectos",
    "dataset_overview": "Resumen del conjunto de datos",
    "balance_analysis": "Análisis de balance",
    "label_distribution": "Distribución de etiquetas (Top 20)",
    "recommendations": "Recomendaciones",
    "failed_to_load_dataset": "Error al cargar información del conjunto de datos.",
    "current_project": "PROYECTO ACTUAL",
    "load": "Cargar",
    "start_training": "Iniciar entrenamiento",
    "stop_training": "Detener entrenamiento",
    "stop_training_disabled": "No hay entrenamiento activo para este proyecto.",
    "training_in_progress": "Entrenamiento en progreso",
    "memory": "Memoria",
    "loading": "Cargando...",
    "training_status": "Estado del entrenamiento",
    "idle": "Inactivo",
    "prediction": "Predicción",
    "predictions": "Predicciones",
    "no_predictions_above_threshold": "No hay predicciones por encima del umbral",
    "image": "Imagen",
    "checkpoint": "Checkpoint",
    "auto": "auto",
    "value": "valor",
    "one_number_per_line": "Un número por línea",
    "empty_object": "Objeto vacío",
    "language_warning": "El cambio de idioma refresca el sistema",
    "language_select_title": "Seleccionar idioma",
    "not_available": "N/D",
    "unknown": "Desconocido",
    "configuration_empty": "No hay secciones de configuración disponibles",
    "configuration_schema_missing": "El esquema de configuración no está cargado aún."
  },
  "augmentation_ui": {
    "page_title": "Aumento de datos",
    "page_description": "Configura transformaciones de imagen para mejorar la generalización y robustez del modelo.",
    "train_title": "Aumentos de entrenamiento",
    "train_description": "Aplicados durante el entrenamiento para aumentar la diversidad visual manteniendo las etiquetas intactas.",
    "val_title": "Aumentos de validación",
    "val_description": "Aplicados durante la validación para mantener evaluaciones determinísticas.",
    "toggle_help": "Alterna un aumento para habilitarlo o deshabilitarlo para esta fase.",
    "no_options": "No hay opciones de aumento disponibles.",
    "custom_warning": "Las siguientes transformaciones están preservadas pero no se pueden editar aquí:",
    "unknown_transform": "Transformación desconocida",
    "random_resized_crop": "Recorte redimensionado aleatorio",
    "random_resized_crop_description": "Recorta y redimensiona la imagen aleatoriamente al tamaño objetivo respetando rangos de escala y relación de aspecto.",
    "random_horizontal_flip": "Volteo horizontal aleatorio",
    "random_horizontal_flip_description": "Voltea la imagen horizontalmente con la probabilidad configurada para capturar variaciones izquierda y derecha.",
    "random_vertical_flip": "Volteo vertical aleatorio",
    "random_vertical_flip_description": "Voltea la imagen verticalmente para exponer el modelo a cambios de perspectiva arriba y abajo.",
    "random_rotation": "Rotación aleatoria",
    "random_rotation_description": "Aplica una rotación aleatoria dentro del rango de grados definido para reducir el sesgo de orientación.",
    "color_jitter": "Variación de color",
    "color_jitter_description": "Varía aleatoriamente el brillo, contraste, saturación y tono para mejorar la robustez del color.",
    "random_grayscale": "Escala de grises aleatoria",
    "random_grayscale_description": "Convierte imágenes a escala de grises con la probabilidad configurada para mejorar el reconocimiento de luminancia.",
    "random_erasing": "Borrado aleatorio",
    "random_erasing_description": "Mascara regiones rectangulares aleatoriamente para fomentar la robustez espacial y el razonamiento de completitud de objetos.",
    "random_perspective": "Perspectiva aleatoria",
    "random_perspective_description": "Aplica una transformación de perspectiva aleatoria usando la escala de distorsión y probabilidad configuradas.",
    "center_crop": "Recorte centrado",
    "center_crop_description": "Recorta la región centrada al tamaño objetivo para entradas de validación consistentes.",
    "random_resized_crop.size_label": "Tamaño de salida",
    "random_resized_crop.size_description": "Longitud de borde en píxeles después de que el recorte se redimensiona.",
    "random_resized_crop.scale_min_label": "Escala mínima",
    "random_resized_crop.scale_min_description": "Límite inferior para la escala de área relativa a la imagen original (0-1).",
    "random_resized_crop.scale_max_label": "Escala máxima",
    "random_resized_crop.scale_max_description": "Límite superior para la escala de área relativa a la imagen original.",
    "random_resized_crop.ratio_min_label": "Relación de aspecto mínima",
    "random_resized_crop.ratio_min_description": "Límite inferior para la relación de aspecto muestreada antes del redimensionado.",
    "random_resized_crop.ratio_max_label": "Relación de aspecto máxima",
    "random_resized_crop.ratio_max_description": "Límite superior para la relación de aspecto muestreada antes del redimensionado.",
    "random_horizontal_flip.p_label": "Probabilidad de volteo",
    "random_horizontal_flip.p_description": "Probabilidad de que una imagen sea reflejada horizontalmente.",
    "random_vertical_flip.p_label": "Probabilidad de volteo",
    "random_vertical_flip.p_description": "Probabilidad de que una imagen sea volteada verticalmente.",
    "random_rotation.min_label": "Grados mínimos",
    "random_rotation.min_description": "Límite inferior de rotación en grados (valores negativos rotan en sentido horario).",
    "random_rotation.max_label": "Grados máximos",
    "random_rotation.max_description": "Límite superior de rotación en grados (valores positivos rotan en sentido antihorario).",
    "color_jitter.brightness_label": "Variación de brillo",
    "color_jitter.brightness_description": "Desviación máxima de brillo agregada a cada canal.",
    "color_jitter.contrast_label": "Variación de contraste",
    "color_jitter.contrast_description": "Escalado máximo de contraste aplicado a la imagen.",
    "color_jitter.saturation_label": "Variación de saturación",
    "color_jitter.saturation_description": "Cambio máximo de saturación aplicado en espacio HSV.",
    "color_jitter.hue_label": "Variación de tono",
    "color_jitter.hue_description": "Rango máximo de cambio de tono (0-0.5).",
    "random_grayscale.p_label": "Probabilidad de escala de grises",
    "random_grayscale.p_description": "Probabilidad de que una imagen sea convertida a escala de grises.",
    "random_erasing.p_label": "Probabilidad de borrado",
    "random_erasing.p_description": "Probabilidad de que una región aleatoria sea borrada por imagen.",
    "random_erasing.scale_min_label": "Escala mínima",
    "random_erasing.scale_min_description": "Límite inferior para la escala de área borrada relativa a toda la imagen.",
    "random_erasing.scale_max_label": "Escala máxima",
    "random_erasing.scale_max_description": "Límite superior para la escala de área borrada relativa a toda la imagen.",
    "random_erasing.ratio_min_label": "Relación de aspecto mínima",
    "random_erasing.ratio_min_description": "Límite inferior para la relación de aspecto del parche borrado.",
    "random_erasing.ratio_max_label": "Relación de aspecto máxima",
    "random_erasing.ratio_max_description": "Límite superior para la relación de aspecto del parche borrado.",
    "random_erasing.value_label": "Valor de relleno",
    "random_erasing.value_description": "Valor de píxel usado para rellenar la región borrada (0-1).",
    "random_erasing.inplace_label": "En el lugar",
    "random_erasing.inplace_description": "Aplicar borrado directamente en el tensor de entrada sin asignar una copia.",
    "random_perspective.distortion_scale_label": "Escala de distorsión",
    "random_perspective.distortion_scale_description": "Controla la fuerza de la distorsión de perspectiva (0-1).",
    "random_perspective.p_label": "Probabilidad de perspectiva",
    "random_perspective.p_description": "Probabilidad de que se aplique una deformación de perspectiva aleatoria.",
    "center_crop.size_label": "Tamaño de recorte",
    "center_crop.size_description": "Longitud de borde objetivo en píxeles para el recorte centrado.",
    "preview_section_title": "Vista previa",
    "preview_description": "Aplica la pipeline actual a una imagen aleatoria del conjunto de datos.",
    "preview_button": "Ver vista previa",
    "preview_idle": "Haz clic en Ver vista previa para ver la imagen aumentada.",
    "preview_loading": "Generando vista previa...",
    "preview_no_project": "Carga un proyecto para previsualizar aumentos.",
    "preview_empty_pipeline": "Configura al menos una transformación para previsualizar.",
    "preview_generic_error": "Error al generar vista previa.",
    "preview_original_label": "Original",
    "preview_augmented_label": "Aumentada",
    "preview_image_path_label": "Ruta de imagen"
  },
  "about_ui": {
    "page_title": "Acerca de Hootsight",
    "page_description": "Entiende el propósito, arquitectura central y principios de desarrollo que dan forma a Hootsight.",
    "card_title": "Herramienta de aumento de imagen para entrenamiento offline",
    "intro": "Hootsight es una herramienta de clasificación de imágenes offline-first que combina entrenamiento PyTorch con una interfaz web configurada.",
    "content_markdown": "## Acerca de Hootsight\n\nHola, soy Tanathy! La desarrolladora solitaria manteniendo Hootsight funcionando. Lo construí porque quería una herramienta de clasificación de imágenes confiable, offline-first que pudiera confiar en mi propio hardware, y pensé que otros merecían esa libertad también.\n\n### Filosofía\n- Tus datos nunca salen de tu máquina a menos que los muevas. No hay trabajos de sincronización en segundo plano ni llamadas sorpresa a la nube.\n- Me niego a enviar telemetría o hooks de rastreo. Los diagnósticos se quedan locales para que puedas elegir qué compartir.\n- Toda configuración vive en JSON. Versiona, diferencia, lanza en Git—cualquier cosa que mantenga tu flujo de trabajo honesto.\n- El instalador configura un entorno virtual aislado, manteniendo tu Python global limpio.\n- Los pesos preentrenados viven bajo `cache/` para que puedas respaldarlos, auditarlos o quemarlos en segundos.\n- Los roadmaps siguen la vida real. Las versiones salen cuando tengo el ancho de banda, no porque un tablero de sprint diga adelante.\n- Las herramientas son neutrales; cómo las uses importa. Espero que todos, yo incluida, las usemos con cuidado.\n\n>Descargo de responsabilidad: Cada conjunto de datos que ingieras, cada etiqueta que predigas y cada modelo que exportes es tu responsabilidad. Mantente atento a consentimiento, legalidad y a las personas afectadas por tu trabajo.\n\n### Estado de desarrollo\n- Hootsight está firmemente en alfa. Espera actualizaciones, experimentos y el ocasional borde áspero.\n- ResNet tiene validación end-to-end completa. ResNeXt, EfficientNet y las arquitecturas restantes están en pruebas a largo plazo cuando el tiempo lo permite.\n- ¿Encontraste un bug? Por favor regístralo en [GitHub Issues](https://github.com/Tanathy/HootSight/issues). Los reportes claros me ayudan a arreglar cosas más rápido.\n\n### Fundamentos técnicos\n- **Backend**: Servicios FastAPI coordinan descubrimiento de conjuntos de datos, orquestación de entrenamiento y endpoints de estado.\n- **Núcleo ML**: PyTorch maneja bucles de entrenamiento, inferencia y pipelines de aumento.\n- **Frontend**: Una interfaz web ligera HTML/JS/CSS potenciada por una librería de ayuda básica (`qte.js`) en lugar de un framework pesado.\n- **Configuración**: Todo es configurado—no hay valores predeterminados ocultos en código.\n- **Operación offline**: La app funciona sin acceso a internet. Las verificaciones opcionales de actualización llaman a GitHub solo cuando lo pides.\n- **Gestión de memoria**: Utilidades personalizadas ajustan tamaños de lote sobre la marcha y monitorean el uso de GPU/CPU para evitar errores de memoria insuficiente.\n- **Manejo de datos**: Proyectos, conjuntos de datos, checkpoints y logs se quedan bajo tu control del sistema de archivos. Sin sincronización automática, sin mirrors remotos.\n\n### Privacidad y cumplimiento\n- Diseñado con expectativas de GDPR en mente: no hay datos personales que salgan de tu entorno por defecto.\n- Tú decides qué importar, y mantienes control total sobre edición, exportación o eliminación.\n- Configuración, logs y checkpoints permanecen en disco a menos que los compartas deliberadamente.\n- Las verificaciones de actualización son opt-in y transmiten solo metadatos de solicitud; el contenido del proyecto nunca va junto.\n- Si estás manejando categorías sensibles (biométricas, médicas, cualquier cosa regulada), mapea esas obligaciones contra tus propias políticas antes de entrenar.\n- No hay SDKs de analytics, reportadores de crashes o trackers de terceros incluidos en la app.\n\n### Apoyo\nEstoy feliz de compartir este proyecto para que lo uses libremente. Si quisieras apoyar mi trabajo, puedes comprarme un café en [ko-fi.com/tanathy](https://ko-fi.com/tanathy).\n\n### Licencia y créditos\nHootsight se envía con la fuente [Roboto](https://fonts.google.com/specimen/Roboto/license), distribuida bajo la Licencia de Fuente Abierta SIL, Versión 1.1."
  },
  "training_ui": {
    "page_title": "Configuración de entrenamiento",
    "page_description": "Configura arquitectura de modelo, parámetros de entrenamiento y ajustes de optimización.",
    "optimizer_params_title": "Parámetros del optimizador",
    "scheduler_params_title": "Parámetros del scheduler",
    "loss_params_title": "Parámetros de pérdida",
    "select_type_first": "Selecciona un tipo para ver parámetros.",
    "no_extra_params": "No hay parámetros adicionales para esta selección."
  },
  "dataset_ui": {
    "page_title": "Conjunto de datos",
    "page_description": "Explora y analiza la estructura de tu conjunto de datos, etiquetas y distribución de datos.",
    "summary": {
      "project": "Proyecto",
      "dataset_type": "Tipo de conjunto de datos",
      "total_images": "Total de imágenes",
      "total_labels": "Total de etiquetas",
      "balance_status": "Estado de balance",
      "balance_score": "Puntuación de balance",
      "images_per_label_ideal": "Imágenes por etiqueta (Ideal)",
      "min_images": "Mín imágenes",
      "max_images": "Máx imágenes",
      "max_min_ratio": "Ratio Máx/Mín"
    },
    "table": {
      "label": "Etiqueta",
      "count": "Conteo",
      "percentage": "Porcentaje"
    }
  },
  "projects_ui": {
    "page_title": "Proyectos",
    "page_description": "Gestiona y cambia entre diferentes proyectos y conjuntos de datos de machine learning.",
    "card": {
      "images": "Imágenes",
      "labels": "Etiquetas",
      "balance_score": "Puntuación de balance",
      "balance_status": "Balance",
      "dataset_type": "Tipo de conjunto de datos",
      "status": {
        "balanced": "Balanceado",
        "imbalanced": "Desbalanceado",
        "critical": "Crítico",
        "warning": "Advertencia",
        "good": "Bueno",
        "poor": "Pobre",
        "excellent": "Excelente",
        "fair": "Justo",
        "ok": "OK",
        "unstable": "Inestable"
      }
    }
  },
  "status_ui": {
    "page_title": "Estado",
    "page_description": "Monitorea progreso de entrenamiento, estado del sistema y métricas de rendimiento en tiempo real."
  },
  "heatmap_ui": {
    "page_title": "Mapa de calor",
    "page_description": "Genera y visualiza mapas de atención del modelo para entender áreas de enfoque de predicción."
  },
  "memory_ui": {
    "page_title": "Memoria",
    "page_description": "Monitorea uso de memoria del sistema y optimiza tamaños de lote para entrenamiento eficiente."
  },
  "environment": {
    "venv_creating": "Creando entorno virtual Python en {path}...",
    "venv_created": "Entorno virtual listo.",
    "venv_create_failed": "Creación de entorno virtual fallida: {error}",
    "venv_exists": "Entorno virtual ya presente.",
    "pip_upgrading": "Actualizando pip dentro del entorno virtual...",
    "pip_upgraded": "Actualización de pip completada.",
    "pip_upgrade_failed": "Actualización de pip fallida: {error}",
    "cuda_debug_nvcc": "Salida de nvcc --version:\n{output}",
    "cuda_debug_nvcc_error": "Error al consultar nvcc: {error}",
    "cuda_debug_nvidia_smi": "Salida de nvidia-smi:\n{output}",
    "cuda_debug_nvidia_smi_error": "Error al consultar nvidia-smi: {error}",
    "cuda_debug_detected": "Versión CUDA detectada: {version}",
    "pytorch_install": "Instalando PyTorch para CUDA {cuda} en {platform}...",
    "pytorch_installed": "Instalación de PyTorch terminada.",
    "pytorch_install_failed": "Instalación de PyTorch fallida: {error}",
    "xformers_already_installed": "xFormers ya está instalado y actualizado.",
    "xformers_installing": "Instalando xFormers (CUDA {cuda_version})...",
    "xformers_installed": "Instalación de xFormers terminada.",
    "xformers_install_failed": "Instalación de xFormers fallida: {error}",
    "pytorch_skip": "Saltando instalación de PyTorch (CUDA detectado={cuda}, plataforma={platform}).",
    "config_loading": "Cargando configuración del entorno...",
    "config_loaded": "Configuración del entorno cargada.",
    "using_compatible_xformers": "Usando índice CUDA {cuda_version} para xFormers.",
    "env_packages_all_installed": "Paquetes del entorno ya están instalados.",
    "env_packages_progress_desc": "Instalando paquetes del entorno",
    "env_package_installing": "Instalando paquete del entorno {package}...",
    "env_package_installed": "Paquete del entorno instalado: {package}",
    "env_package_install_failed": "Error al instalar paquete del entorno {package}: {error}",
    "env_vars_configured": "Preparadas {count} variable(s) de entorno para el proceso de entrenamiento.",
    "env_vars_config_failed": "Error al configurar variables de entorno: {error}",
    "entry_not_found": "Script de entrada no encontrado en {path}.",
    "venv_python_not_found": "Ejecutable Python del entorno virtual faltante: {path}.",
    "venv_python_test_failed": "Python del entorno virtual falló en prueba --version: {error}",
    "re_exec_starting": "Lanzando entrenamiento vía {venv_python} -> {entry_py} (raíz {root}).",
    "re_exec_timeout": "Re-ejecución agotó el tiempo.",
    "re_exec_failed": "Error al lanzar proceso de entrenamiento: {error}",
    "re_exec_unexpected_error": "Error inesperado al lanzar proceso de entrenamiento: {error}"
  },
  "updates_ui": {
    "page_title": "Actualizaciones del sistema",
    "page_description": "Mantén tu instalación alineada con el repositorio upstream sin sobrescribir anulaciones de configuración específicas del proyecto.",
    "card_title": "Gestor de actualizaciones",
    "intro": "Compara archivos locales con el repositorio de referencia y sincroniza correcciones faltantes dejando config.json intacto.",
    "check_button": "Verificar actualizaciones",
    "apply_button": "Aplicar actualizaciones",
    "apply_disabled_hint": "Ejecuta una verificación para habilitar actualizaciones.",
    "status_idle": "No se han ejecutado verificaciones de actualización aún.",
    "status_checking": "Verificando actualizaciones...",
    "status_ready": "Resumen de actualización preparado.",
    "status_up_to_date": "Todo está actualizado.",
    "status_failed": "Verificación de actualización fallida.",
    "status_applying": "Actualizando archivos...",
    "status_applied": "Actualizaciones aplicadas exitosamente.",
    "status_apply_failed": "Algunas actualizaciones fallaron.",
    "table_header_file": "Archivo",
    "table_header_status": "Estado",
    "table_header_local": "Local",
    "table_header_remote": "Remoto",
    "table_row_missing": "Faltante localmente",
    "table_row_outdated": "Desajuste de checksum",
    "table_footnote": "Hashes truncados para legibilidad.",
    "no_updates": "Todos los archivos rastreados están actualizados.",
    "hash_missing": "—",
    "orphaned_title": "Archivos locales no rastreados",
    "orphaned_none": "No se detectaron archivos locales extra."
  },
  "schema": {
    "description": "Esquema JSON de Hootsight config.json: define tipos, rangos y la estructura jerárquica de todas las opciones configurables",
    "general_description": "Configuración general de la aplicación",
    "general_language_description": "Código de idioma de la interfaz",
    "api_description": "Configuración del servidor API",
    "api_host_description": "Host del servidor API",
    "api_port_description": "Puerto del servidor API",
    "ui_description": "Ajustes de la interfaz de usuario",
    "ui_title_description": "Título de la ventana de la aplicación",
    "ui_width_description": "Ancho de la ventana en píxeles",
    "ui_height_description": "Altura de la ventana en píxeles",
    "ui_resizable_description": "Indica si la ventana se puede redimensionar",
    "system_description": "Configuraciones a nivel de sistema",
    "system_max_threads_description": "Número máximo de hilos",
    "system_fallback_batch_size_description": "Tamaño de lote de respaldo cuando falla el cálculo automático",
    "system_memory_cleanup_interval_description": "Intervalo de limpieza de memoria en segundos",
    "system_thread_pool_timeout_description": "Tiempo de espera del pool de hilos en segundos",
    "system_startup_wait_seconds_description": "Tiempo de espera durante el arranque en segundos",
    "memory_description": "Parámetros de gestión de memoria",
    "memory_target_memory_usage_description": "Relación objetivo de uso de memoria (0.0-1.0)",
    "memory_safety_margin_description": "Margen de seguridad para los cálculos de memoria (0.0-1.0)",
    "memory_augmentation_threads_description": "Número de hilos para la augmentación de datos",
    "training_description": "Configuración del entrenamiento",
    "training_model_type_description": "Tipo de modelo a utilizar",
    "training_model_name_description": "Nombre específico del modelo",
    "training_pretrained_description": "Inicializa la red con pesos preentrenados en ImageNet cuando estén disponibles; desactívalo para entrenar desde cero",
    "training_task_description": "Tipo de tarea de aprendizaje automático",
    "training_batch_size_description": "Tamaño de lote para el entrenamiento",
    "training_epochs_description": "Número de épocas de entrenamiento",
    "training_learning_rate_description": "Tasa de aprendizaje",
    "training_weight_decay_description": "Decaimiento de pesos (regularización L2)",
    "training_input_size_description": "Longitud de borde en píxeles del tensor de entrada cuadrado (debe mantenerse coherente con el reescalado de tus aumentos)",
    "training_normalize_description": "Parámetros de normalización de imágenes",
    "training_normalize_mean_description": "Valores medios para los canales RGB",
    "training_normalize_std_description": "Desviaciones estándar para los canales RGB",
    "training_val_ratio_description": "Proporción de división para validación (0.0-1.0)",
    "training_dataloader_description": "Configuración del DataLoader",
    "training_dataloader_num_workers_description": "Número de procesos trabajadores",
    "training_dataloader_pin_memory_description": "Indica si se fija memoria para acelerar la transferencia a GPU",
    "training_dataloader_persistent_workers_description": "Indica si los trabajadores permanecen vivos entre épocas",
    "training_dataloader_prefetch_factor_description": "Número de lotes a precargar por trabajador",
    "training_augmentation_description": "Configuración de augmentación de datos",
    "training_augmentation_train_description": "Aumentos aplicados durante el entrenamiento",
    "training_augmentation_val_description": "Aumentos aplicados durante la validación",
    "training_optimizer_type_description": "Tipo de optimizador",
    "training_optimizer_params_description": "Parámetros del optimizador",
    "training_scheduler_type_description": "Tipo de scheduler de tasa de aprendizaje",
    "training_scheduler_params_description": "Parámetros del scheduler",
    "training_loss_type_description": "Tipo de función de pérdida",
    "training_loss_params_description": "Parámetros de la función de pérdida",
    "training_weight_init_description": "Configuración de inicialización de pesos",
    "training_checkpoint_description": "Configuración de checkpoints",
    "training_early_stopping_description": "Configuración de parada temprana",
    "training_gradient_description": "Opciones de gradiente",
    "training_runtime_description": "Ajustes de optimización en tiempo de ejecución",
    "training_runtime_mixed_precision_description": "Habilitar entrenamiento automático en precisión mixta",
    "training_runtime_channels_last_description": "Usar formato de memoria channels-last para mejor aprovechamiento de la GPU",
    "training_runtime_allow_tf32_description": "Permitir TF32 para acelerar operaciones matriciales en GPUs Ampere o superiores",
    "training_runtime_cudnn_benchmark_description": "Activar la prueba de cuDNN para obtener algoritmos de convolución optimizados",
    "dataset_description": "Configuración del conjunto de datos",
    "dataset_image_extensions_description": "Extensiones de imagen admitidas",
    "optimizers_description": "Sobrescritura de valores predeterminados de optimizadores",
    "optimizers_defaults_description": "Parámetros predeterminados para optimizadores",
    "schedulers_description": "Sobrescritura de valores predeterminados de schedulers",
    "schedulers_defaults_description": "Parámetros predeterminados para schedulers",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Función lambda como texto, por ejemplo 'lambda epoch: 0.95 ** epoch'",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Función lambda como texto, por ejemplo 'lambda epoch: 0.95'",
    "losses_description": "Sobrescritura de valores predeterminados de pérdidas",
    "losses_defaults_description": "Parámetros predeterminados para pérdidas",
    "models_description": "Configuraciones de modelos",
    "models_resnet_description": "Ajustes de la familia de modelos ResNet",
    "models_resnet_variants_description": "Configuración de variantes ResNet",
    "models_resnext_description": "Ajustes de la familia ResNeXt",
    "models_resnext_variants_description": "Configuración de variantes ResNeXt",
    "models_mobilenet_description": "Ajustes de la familia MobileNet",
    "models_mobilenet_variants_description": "Configuración de variantes MobileNet",
    "models_shufflenet_description": "Ajustes de la familia ShuffleNet",
    "models_shufflenet_variants_description": "Configuración de variantes ShuffleNet",
    "models_squeezenet_description": "Ajustes de la familia SqueezeNet",
    "models_squeezenet_variants_description": "Configuración de variantes SqueezeNet",
    "models_efficientnet_description": "Ajustes de la familia EfficientNet",
    "models_efficientnet_variants_description": "Configuración de variantes EfficientNet",
    "models_supported_types_description": "Tipos de modelo compatibles",
    "general_language_enum_descriptor": {
      "en": "Idioma inglés: controla toda la localización de la interfaz, incluidos menús, mensajes de error, ayudas emergentes y textos de validación. Afecta a todos los textos mostrados en la interfaz web y en las respuestas del API. Determina qué paquete de idioma se carga al iniciar la aplicación. Actualmente es la única opción soportada, por lo que actúa como predeterminada en todas las instalaciones."
    },
    "system_max_threads_enum_descriptor": {
      "auto": "Conteo automático de hilos: calcula dinámicamente el tamaño óptimo del pool de hilos según los núcleos de CPU disponibles (normalmente núcleos - 1). Afecta la carga paralela de datos, el preprocesamiento de imágenes, el batching de inferencia y las tareas en segundo plano. Controla la asignación de hilos para los DataLoader de PyTorch, las pipelines de augmentación y el manejo concurrente de peticiones HTTP. Se ajusta automáticamente a la capacidad de hardware y al uso de memoria disponible."
    },
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Conteo automático de hilos de augmentación: calcula la cantidad óptima de hilos para augmentación de imágenes en paralelo según los núcleos de CPU y la RAM disponible. Afecta el rendimiento de la pipeline de transformaciones (rotaciones, escalados, variación de color y normalización). Controla la memoria asignada a buffers de augmentación y almacenamiento intermedio. Balancea el uso de CPU frente a la presión de memoria para evitar sobrecargar el sistema durante preprocesamiento intensivo."
    },
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet (Residual Network): red neuronal convolucional profunda con conexiones residuales que permiten entrenar arquitecturas muy profundas (18-152 capas). Influye en el flujo de gradientes, la estabilidad de entrenamiento, la profundidad de representación y la capacidad del modelo. Utiliza bloques residuales con normalización por lotes y activación ReLU. Controla la complejidad arquitectónica desde 11M de parámetros (ResNet-18) hasta 60M (ResNet-152). Impacta el uso de memoria, el tiempo de entrenamiento, la velocidad de inferencia y la precisión final en clasificación de imágenes.",
      "resnext": "ResNeXt (Transformaciones residuales agregadas): evolución de ResNet que usa cardinalidad (convoluciones agrupadas) para aumentar la capacidad sin incrementar significativamente los parámetros. Mejora la diversidad de aprendizaje de características, la expresividad y la eficiencia computacional. Controla rutas de transformación paralelas dentro de cada bloque residual. Impacta la memoria de GPU, la duración del entrenamiento y ofrece mayor precisión que ResNet estándar con costes similares.",
      "mobilenet": "MobileNet: CNN ligera que emplea convoluciones separables en profundidad para reducir tamaño y coste computacional. Afecta la latencia de inferencia, el consumo energético, el tamaño de almacenamiento y la viabilidad de despliegue en móviles. Controla el equilibrio entre precisión y eficiencia mediante multiplicador de ancho y resolución. Incide en la batería, el procesamiento en tiempo real y la compatibilidad con dispositivos edge.",
      "shufflenet": "ShuffleNet: CNN altamente eficiente que utiliza operaciones de barajado de canales y convoluciones agrupadas punto a punto. Optimiza el ancho de banda de memoria, el coste de inferencia, el tamaño del modelo y la velocidad de procesamiento. Controla la comunicación entre canales agrupados para mantener el flujo de información. Diseñada para procesadores ARM y dispositivos de bajo consumo. Impacta los requisitos de rendimiento en tiempo real y despliegues con recursos limitados.",
      "squeezenet": "SqueezeNet: CNN ultracompacta que emplea módulos Fire (etapas squeeze + expand) para lograr precisión similar a AlexNet con 50 veces menos parámetros. Reduce los requisitos de almacenamiento, tiempo de descarga, eficiencia de caché y ancho de banda de despliegue. Controla la cantidad de parámetros mediante reducción agresiva de dimensiones seguida de expansión. Minimiza el tamaño en disco manteniendo precisión razonable para tareas básicas de clasificación.",
      "efficientnet": "EfficientNet: CNN con escalado compuesto que ajusta de forma conjunta profundidad, ancho y resolución usando búsqueda de arquitectura. Mejora la eficiencia computacional, la escalabilidad de precisión, los requisitos de recursos y la optimización de inferencia. Controla la complejidad mediante un coeficiente compuesto que equilibra simultáneamente las tres dimensiones. Ofrece mejores compromisos precisión/eficiencia que los métodos tradicionales de escalado."
    },
    "training_task_enum_descriptor": {
      "classification": "Clasificación de etiqueta única: asigna exactamente una etiqueta mutuamente excluyente por imagen. Afecta la arquitectura de la capa final (activación softmax), la función de pérdida (entropía cruzada categórica), la dimensionalidad de salida y la interpretación de confianza. Controla los límites de decisión, la distribución de probabilidades y los patrones de convergencia. Requiere datasets balanceados y clases bien separadas para un rendimiento óptimo.",
      "multi_label": "Clasificación multietiqueta: asigna cero, una o múltiples etiquetas no excluyentes a cada imagen. Afecta la activación de salida (sigmoide por clase), la composición de la pérdida (entropía cruzada binaria por etiqueta), la selección de umbrales y las métricas (F1, mAP). Controla rutas de predicción independientes, manejo de correlaciones entre etiquetas y estrategias de ponderación para clases desbalanceadas. Gestiona escenarios complejos donde las imágenes contienen varios conceptos semánticos.",
      "detection": "Detección de objetos: localiza y clasifica múltiples instancias dentro de una imagen usando cajas delimitadoras. Afecta la complejidad arquitectónica (redes tipo pirámide, generación de anchors), la composición de la pérdida (clasificación + regresión), los requisitos de datos (cajas anotadas), el postprocesado (supresión no máxima) y la carga computacional. Controla la profundidad de extracción espacial, el reconocimiento multiescala, los mecanismos de propuesta y el cálculo de IoU.",
      "segmentation": "Segmentación semántica: realiza clasificación por píxel asignando etiquetas semánticas a cada punto de la imagen. Exige memoria elevada (mapas de características a resolución completa), arquitecturas encoder-decoder con conexiones de salto, diseño de pérdidas (entropía cruzada por píxel, focal para desbalance) y manejo de complejidad (desequilibrio a nivel de píxel). Controla las estrategias de upsampling, la calidad del refinamiento de bordes, la precisión espacial y el razonamiento contextual."
    },
    "training_epochs_enum_descriptor": {
      "auto": "Determinación automática de épocas: monitoriza la pérdida y la precisión de validación para encontrar la duración óptima usando criterios de parada temprana. Afecta el tiempo total de entrenamiento, la calidad de convergencia, el consumo de recursos y la prevención de sobreajuste. Rastrea mejoras durante el período de paciencia y detiene automáticamente cuando no hay progreso significativo. Equilibra exhaustividad y eficiencia."
    },
    "training_optimizer_type_enum_descriptor": {
      "sgd": "Descenso de gradiente estocástico: paso de gradiente determinista con momentum opcional y lookahead de Nesterov. Expone learning_rate, momentum, dampening y weight_decay como controles críticos. Funciona mejor cuando puedes planificar un scheduler y deseas un control estrecho de la generalización. Ofrece resultados sólidos en grandes datasets de visión con schedulers coseno o por pasos, pero ajusta el momentum (0.9 suele ser buen inicio) y mantén tasas entre 0.01 y 0.1 según el lote.",
      "adam": "Optimizador Adam: método adaptativo de primer orden que almacena promedios móviles de gradientes (beta1) y de gradientes al cuadrado (beta2). Los valores por defecto 0.9/0.999 y eps=1e-8 sirven para la mayoría de tareas. Maneja gradientes ruidosos o dispersos sin ajustar manualmente la tasa, lo que lo convierte en una base fiable para clasificación y transfer learning. Si ves convergencia lenta con weight decay acoplado, considera AdamW.",
      "adamw": "AdamW: separa el decaimiento de pesos de las actualizaciones adaptativas de Adam para que la regularización L2 actúe correctamente. Mantiene betas y epsilon de Adam exponiendo weight_decay como un regularizador real. Ideal para vision transformers, ajuste fino de ResNet o cualquier modelo donde importe la estabilidad con generalización predecible. Empieza con weight_decay≈0.01 y ajusta learning_rate entre 3e-5 y 3e-4 en escenarios de transferencia.",
      "adamax": "AdaMax: variante de Adam que usa la norma infinito para el segundo momento. Comparte hiperparámetros pero ofrece mayor resiliencia a picos esporádicos en gradientes. Útil cuando Adam se vuelve inestable por magnitudes extremas, especialmente en GANs o RL. Mantén beta2 cerca de 0.999 y trata learning_rate como en Adam estándar; espera convergencia ligeramente más lenta pero menos saltos catastróficos.",
      "nadam": "Adam acelerado por Nesterov: añade momentum de Nesterov a la adaptación de Adam. Comparte betas y epsilon, pero realiza un cálculo lookahead que puede ceñir la convergencia en objetivos suaves. Supone algo más de cómputo por paso. Recomendado si Adam converge pero se estanca pronto; baja learning_rate respecto a Adam puro (p.ej. 1e-4 en vez de 3e-4) para evitar sobrepasarse.",
      "radam": "Rectified Adam (RAdam): versión con warmup automático derivado de la rectificación de varianza. Elimina la necesidad de un warmup manual reduciendo la magnitud de los pasos hasta que la varianza se estabiliza. Sus hiperparámetros coinciden con los de Adam. Úsalo cuando necesites adaptatividad pero tu entrenamiento sea sensible a las primeras iteraciones. Funciona bien en datasets pequeños donde un warmup manual sobreajustaría.",
      "rmsprop": "RMSprop: mantiene un promedio exponencial de gradientes al cuadrado (alpha) para normalizar actualizaciones. Por defecto alpha=0.99 y eps=1e-8. Popular en redes recurrentes y RL; sigue siendo útil cuando los gradientes oscilan y Adam parece demasiado agresivo. Combínalo con un scheduler decreciente; inicia cerca de 1e-3 con momentum desactivado o bajo (≤0.1).",
      "rprop": "Resilient Backpropagation (RProp): optimizador basado en el signo que ajusta el tamaño de paso por parámetro usando solo los cambios de signo del gradiente. Ignora el tamaño de lote al asumir actualizaciones full-batch, por lo que rara vez conviene para CNN con mini-batches. Empléalo en contextos deterministas (datasets pequeños con pasadas completas) donde quieras convergencia tipo segundo orden sin almacenar la Hessiana. eta_plus=1.2 y eta_minus=0.5 gobiernan la adaptación.",
      "adagrad": "AdaGrad: acumula gradientes al cuadrado reduciendo la tasa para pesos actualizados con frecuencia. Casi sin mantenimiento en características dispersas, pero la suma acumulada lleva la tasa efectiva hacia cero en entrenamientos largos. Úsalo para embeddings o problemas NLP clásicos, no para CNN profundas que entrenan cientos de épocas. Learning_rate inicial típico 1e-2 con epsilon ~1e-10 para evitar divisiones por cero.",
      "adadelta": "AdaDelta: corrige el decaimiento de tasa de AdaGrad rastreando una ventana móvil de gradientes y actualizaciones. Requiere poco ajuste manual más allá de rho (0.9) y eps (1e-6). Funciona con objetivos ruidosos donde Adam puede ser agresivo, aunque su precisión final suele quedar por debajo de AdamW. Prefiérelo cuando debas evitar schedulers manuales y aún necesites comportamiento adaptativo.",
      "sparse_adam": "Sparse Adam: versión de Adam que solo actualiza índices con gradiente, reduciendo memoria y cómputo para tablas de embeddings. Usa los mismos hiperparámetros que Adam pero asume gradientes casi nulos. Esencial para NLP con vocabularios gigantes. Evítalo en modelos convolucionales densos; la contabilidad sparsa solo desperdicia tiempo.",
      "lbfgs": "L-BFGS: método quasi-Newton de memoria limitada que aproxima la Hessiana inversa con gradientes pasados. Requiere gradientes full-batch y una búsqueda de línea por paso, por lo que debes implementar un closure que recalcule pérdida y gradientes. Excelente para afinar modelos pequeños o resolver problemas convexos de alta precisión. No es viable con mini-lotes grandes porque cada paso es caro y la memoria crece con history_size (controlado por max_iter e history_size).",
      "asgd": "SGD promediado (ASGD): mantiene un promedio en curso de los parámetros para amortiguar oscilaciones causadas por gradientes ruidosos. Ajustas la learning_rate base de SGD, pero el promedio se activa tras averaging_start para suavizar la convergencia. Úsalo cuando SGD puro se tambalee al final y quieras evitar cambiar a Adam. Funciona mejor con tasas constantes o decay lento y momentum desactivado."
    },
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Scheduler de tasa por pasos: multiplica la tasa por gamma cada step_size épocas. Funciona cuando conoces de antemano dónde se ralentiza el progreso (p.ej. 30/60/90 en ImageNet). Elige gamma entre 0.1 y 0.3 y alinea step_size con tu presupuesto total de épocas. Sin esa referencia puede sentirse brusco, así que vigila las métricas de validación.",
      "multi_step_lr": "Scheduler de tasa multi-pasos: generaliza el esquema por pasos aceptando una lista de épocas hito. Te permite programar varias reducciones en puntos arbitrarios, ideal para replicar calendarios de papers o experimentos previos. Mantén gamma uniforme salvo razón contraria y asegura que los hitos sean enteros estrictamente crecientes.",
      "exponential_lr": "Scheduler exponencial: aplica lr_t = lr_0 * gamma^t, logrando un decaimiento suave a cambio de ajustar gamma con cuidado. Útil en entrenamientos largos donde prefieres deslizamiento gradual en vez de saltos. Gamma típico entre 0.97 y 0.995 por época. Combínalo con warmup si la pendiente inicial es demasiado pronunciada.",
      "cosine_annealing_lr": "Scheduler cosenoidal: reduce la tasa siguiendo una curva coseno sobre T_max épocas y opcionalmente reinicia en eta_min. Proporciona aterrizajes suaves que aumentan la precisión final en modelos de visión. Ajusta T_max al número de épocas del ciclo y eta_min a un valor pequeño como lr_0/100. Ideal para afinar automáticamente sin hitos manuales.",
      "cosine_annealing_warm_restarts": "Coseno con reinicios cálidos: repite ciclos cosenoidales reiniciando la tasa inicial tras cada ciclo. Excelente para escapar mínimos poco profundos en sesiones largas. T_0 marca la duración inicial y T_mult escala los ciclos siguientes. Mantén eta_min bajo pero no nulo para evitar congelar el optimizador.",
      "reduce_lr_on_plateau": "Reducir tasa en meseta: observa una métrica (normalmente pérdida de validación) y baja la tasa por un factor cuando la mejora se detiene durante la paciencia. Clave cuando no puedes prever el momento de la meseta. Configura cooldown para evitar disparos consecutivos y usa threshold para filtrar ruido. Gamma entre 0.1 y 0.5 suele equilibrar bien.",
      "cyclic_lr": "Scheduler cíclico: alterna la tasa entre base_lr y max_lr en ventanas cortas, opcionalmente reduciendo amplitud según el modo. Útil para convergencia rápida en objetivos difíciles o para explorar rangos de tasa. Fija step_size_up/down al número de iteraciones por medio ciclo; mantén max_lr alrededor de 3-10× base_lr. Combínalo con ciclo de momentum si activas cycle_momentum.",
      "one_cycle_lr": "Política One Cycle: barrido único que eleva la tasa hasta max_lr y luego la reduce a una fracción mientras invierte el momentum. Ofrece convergencia rápida cuando conoces los pasos totales. Proporciona total_steps o (epochs × steps_per_epoch); define pct_start para marcar el tramo de warmup (0.3 habitual). Funciona mejor con SGD o AdamW y no espera schedulers adicionales.",
      "polynomial_lr": "Scheduler polinomial: reduce la tasa a cero siguiendo (1 - t/T)^power. Elige total_iters como número de pasos del optimizador y power para la curvatura (1 lineal, 2 cuadrática). Útil en segmentación y detección cuando quieres un descenso determinista hasta el final.",
      "linear_lr": "Scheduler lineal: interpola linealmente entre start_factor y end_factor durante total_iters pasos. Ideal para warmup (start_factor < 1) o fases de enfriamiento. Mantén total_iters acorde con las iteraciones que quieres cubrir y combínalo con otro scheduler para el resto del entrenamiento.",
      "lambda_lr": "Scheduler lambda: hook directo que multiplica la tasa base por tu función lambda(epoch) personalizada. Brinda control total para calendarios de investigación o aprendizaje curricular. Proporciona una expresión Python que devuelva un flotante; recuerda que se evalúa como cadena en el proceso de entrenamiento. Valida la función con cuidado: errores de sintaxis o salidas negativas arruinarán la ejecución.",
      "multiplicative_lr": "Scheduler multiplicativo: similar a lambda_lr, pero espera un callable que devuelva un multiplicador en cada paso, usado a menudo para escalado por época. Pasa una lambda que dependa del conteo de pasos del optimizador si necesitas control por iteración. Mantén multiplicadores positivos y acotados; valores >1 pueden disparar la tasa y desestabilizar el entrenamiento."
    },
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Pérdida de entropía cruzada: combina softmax y log-verosimilitud negativa en una llamada. Es la elección principal para clasificación de etiqueta única. Acepta logits crudos, gestiona desbalance mediante weight o label_smoothing y ofrece probabilidades calibradas. Mantén reduction='mean' para gradientes estables y monitoriza label_smoothing para no borrar clases minoritarias.",
      "nll_loss": "Pérdida de log-verosimilitud negativa: misma matemática que entropía cruzada pero espera que invoques log_softmax. Útil cuando el modelo ya entrega log-probabilidades (p.ej. escalado de temperatura o precisión mixta manual). Asegúrate de que las entradas sean log-probabilidades; dar logits crudos producirá basura sin aviso.",
      "bce_loss": "Pérdida de entropía cruzada binaria: opera sobre probabilidades en [0,1], por lo que debes acompañarla de un sigmoide explícito. Apropiada para clasificación binaria cuando controlas la activación aparte. Evita el underflow numérico con logits extremos recortando entradas o usa BCEWithLogitsLoss si aparecen NaNs.",
      "bce_with_logits": "Entropía cruzada binaria con logits: versión numéricamente estable que aplica sigmoide internamente. Opción predeterminada para clasificación multietiqueta y tareas binarias. Admite pos_weight para desbalance sin trucos manuales. Genera pérdidas sin límite si olvidas restringir los objetivos a {0,1}.",
      "multi_margin": "Pérdida de margen multiclase: objetivo basado en margen (tipo hinge) que obliga a que la puntuación correcta supere a las demás por al menos el margen. Ofrece normas L1 o L2 mediante el parámetro p. Úsala cuando quieras comportamiento de gran margen en lugar de probabilístico, pero puede converger más lento sin un control cuidadoso de la tasa.",
      "multi_label_margin": "Pérdida de margen multietiqueta: extiende la pérdida de margen a problemas multietiqueta ordenando clases positivas por delante de negativas. Requiere codificar objetivos como listas de índices, lo que dificulta usar tensores densos. Reservada para investigación que requiera ranking de margen en espacio multietiqueta.",
      "multi_label_soft_margin": "Pérdida de margen suave multietiqueta: aplica una formulación de margen suave sobre activaciones sigmoide, produciendo gradientes más suaves que las pérdidas de margen duro. Maneja mejor etiquetas solapadas y desbalance que la BCE estándar. Las etiquetas deben seguir en {0,1}; ajusta umbrales en inferencia para aprovechar el paisaje más suave.",
      "mse_loss": "Error cuadrático medio: penalización L2 clásica. Castiga errores grandes de forma cuadrática, amplificando el impacto de outliers. Ideal en autoencoders y regresión con poco ruido, pero recorta objetivos extremos o cambia a Huber si ves explosión de gradientes.",
      "l1_loss": "Pérdida L1 (error absoluto medio): penalización lineal sobre el error absoluto, robusta ante outliers a costa de converger más lento cerca de cero. Úsala cuando necesites comportamiento similar a la mediana o tu métrica sea MAE. Los gradientes tienen magnitud constante, así que combina con schedulers suaves para evitar fluctuaciones.",
      "smooth_l1": "Pérdida Smooth L1: pérdida tipo Huber con región beta que se comporta como L2 cerca de cero y L1 fuera. Elección estándar para regresión de cajas delimitadoras (beta≈1). Ajusta beta si tu escala difiere; valores menores estrechan la región cuadrática y dan penalizaciones más fuertes a errores medios.",
      "huber_loss": "Pérdida Huber: similar a Smooth L1 pero parametrizada por delta en lugar de beta. Permite controlar explícitamente el punto de cambio entre penalizaciones cuadráticas y lineales. Excelente para regresión con outliers ocasionales; fija delta cercano a la desviación estándar esperada del ruido.",
      "kl_div": "Divergencia Kullback-Leibler: mide la divergencia entre la distribución predicha y la objetivo. Requiere log-probabilidades como entrada y probabilidades crudas como objetivo por defecto (o viceversa con log_target). Esencial en distilación de conocimiento y modelos variacionales. Comprueba el modo de reducción; 'batchmean' preserva la teoría de KL al sumar sobre clases y promediar en el lote.",
      "margin_ranking": "Pérdida de ranking por margen: opera sobre pares de puntuaciones (x1, x2) con orden real y ∈ {−1, 1}. Entrena al modelo para que x1 supere a x2 por al menos el margen cuando y=1. Combínala con muestreo cuidadoso de pares positivos/negativos o tripletas; pares aleatorios rara vez aportan señal útil.",
      "hinge_embedding": "Pérdida hinge embedding: para aprendizaje de similitud donde las etiquetas indican si los pares deben estar cerca (+1) o lejos (−1). Penaliza distancias que violen el margen especificado. Úsala cuando solo tengas supervisión binaria similar/diferente y quieras embeddings agrupados en consecuencia.",
      "triplet_margin": "Pérdida de margen tripleta: consume embeddings ancla, positivo y negativo e impone un margen entre las distancias positiva y negativa. Necesita minería de tripletas duras o semifirmes para ser eficaz; tripletas aleatorias suelen desperdiciar cómputo. El margen por defecto es 1.0, ajústalo según la escala de tus embeddings (menor si están normalizados).",
      "cosine_embedding": "Pérdida de embedding coseno: optimiza directamente la similitud coseno, enfatizando la distancia angular sobre la magnitud. Ideal con vectores normalizados o cuando la dirección lleva la semántica (por ejemplo reconocimiento facial). Asegúrate de normalizar embeddings para evitar reintroducir la magnitud.",
      "ctc_loss": "Pérdida CTC (Connectionist Temporal Classification): alinea entradas de longitud variable con secuencias objetivo sin anotaciones a nivel de frame. Requiere log-probabilidades con forma (T, N, C) y secuencias objetivo sin huecos insertados (la pérdida los maneja). Configura el índice de blanco y ordena las longitudes objetivo; discrepancias generan errores de ejecución.",
      "poisson_nll": "Pérdida log-verosimilitud negativa de Poisson: para modelar datos de conteo con objetivos enteros no negativos. Acepta log_input para imponer predicciones positivas o logits completos con recorte para permanecer sobre cero. Usa full=true si tu modelo predice tasas crudas. No alimentes objetivos negativos; se rompe la suposición de la distribución.",
      "gaussian_nll": "Pérdida log-verosimilitud negativa gaussiana: entrena al modelo para emitir media y varianza para objetivos continuos. Espera que el modelo devuelva tensores (media, varianza). Soporta covarianza completa mediante cholesky_factor; de lo contrario la varianza debe permanecer positiva. Muy útil para regresión con incertidumbre; añade un pequeño epsilon a la varianza para evitar log(0)."
    },
    "training_loss_reduction_enum_descriptor": {
      "mean": "Reducción media: calcula la pérdida promedio dividiendo la pérdida total entre el tamaño del lote. Normaliza la magnitud del gradiente, hace que el entrenamiento sea independiente del tamaño del lote y estabiliza la sensibilidad a la tasa. Mantiene gradientes proporcionales al error de cada muestra en lugar de al lote completo.",
      "sum": "Reducción suma: calcula la pérdida total sumando las pérdidas individuales sin normalizar. Hace que la magnitud del gradiente dependa del tamaño del lote, exige ajustar la tasa de aprendizaje en proporción y modifica la dinámica de entrenamiento. Útil cuando deseas que el gradiente escale con el número de muestras procesadas.",
      "none": "Sin reducción: devuelve la pérdida individual por muestra sin agregación. Permite ponderaciones personalizadas, análisis por muestra, combinaciones manuales y estrategias avanzadas de entrenamiento. Es imprescindible cuando necesitas manipular la pérdida por muestra."
    },
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Monitorización de pérdida de validación: detiene el entrenamiento cuando la pérdida deja de mejorar. Previene sobreajuste, optimiza la duración, mejora la generalización y ahorra recursos. El entrenamiento se corta al detectar la meseta, normalmente indicando que el modelo ya capturó los patrones generalizables.",
      "val_accuracy": "Monitorización de precisión de validación: detiene el entrenamiento cuando la precisión deja de subir. Enfocado en optimizar el rendimiento de clasificación, detectar sobreajuste y mejorar la eficiencia. Adecuado para tareas balanceadas donde la precisión es la métrica principal y correlaciona con la generalización."
    },
    "optimizers_defaults_lbfgs_line_search_fn_oneOf[1]_enum_descriptor": {
      "strong_wolfe": "Búsqueda lineal Strong Wolfe: algoritmo avanzado para L-BFGS que garantiza las condiciones de disminución suficiente (Armijo) y curvatura. Asegura longitudes de paso apropiadas que cumplen criterios de optimalidad manteniendo eficiencia computacional. Es esencial para las garantías teóricas de L-BFGS y proporciona pasos robustos en métodos quasi-Newton."
    },
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Modo mínimo: monitoriza métricas donde valores menores son mejores (p. ej. pérdida de validación). Dispara la reducción cuando la métrica deja de disminuir por debajo del umbral durante la paciencia indicada. Evita estancamiento al bajar la tasa cuando la pérdida se aplana.",
      "max": "Modo máximo: monitoriza métricas donde valores mayores son mejores (p. ej. precisión). Reduce la tasa cuando la métrica deja de subir por encima del umbral durante la paciencia definida. Facilita un ajuste fino adicional al detectar mesetas de precisión."
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Umbral relativo: define la mejora como porcentaje del mejor valor actual. Ajusta la sensibilidad a medida que el modelo mejora, volviendo el umbral más estricto conforme sube el rendimiento. Evita reducir la tasa prematuramente en modelos de alto desempeño.",
      "abs": "Umbral absoluto: establece una mejora fija independiente del nivel actual de la métrica. Mantiene un requisito constante durante todo el entrenamiento. Útil cuando necesitas estándares uniformes sin importar el rendimiento alcanzado."
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Modo triangular: crea ciclos básicos con aumentos y descensos lineales entre base_lr y max_lr manteniendo amplitud constante. Proporciona un equilibrio constante entre exploración y explotación sin decaimiento adicional.",
      "triangular2": "Modo triangular2: similar al triangular pero reduce a la mitad la amplitud tras cada ciclo completo. Ofrece ciclos cada vez más conservadores combinando exploración inicial agresiva con refinamiento progresivo.",
      "exp_range": "Modo rango exponencial: escala la amplitud del ciclo de forma exponencial usando gamma para ajustar el rango dinámicamente. Permite patrones avanzados donde la amplitud crece o decrece según el número de ciclo."
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Escala por ciclo: aplica la función de escalado según la cantidad de ciclos completados. Cambia la amplitud solo en los límites de cada ciclo, útil cuando deseas ajustes discretos entre ciclos.",
      "iterations": "Escala por iteraciones: aplica la función según el número total de iteraciones desde el inicio. Permite transiciones suaves y continuas durante todo el entrenamiento, ideal para cambios graduales."
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Estrategia cosenoidal: usa una función coseno para transiciones suaves, con cambios graduales en los extremos y más pronunciados en el centro. Reduce las oscilaciones y favorece una convergencia suave.",
      "linear": "Estrategia lineal: usa interpolación lineal para un cambio a ritmo constante durante el ciclo. Proporciona evolución predecible y uniforme cuando no necesitas la suavidad cosenoidal."
    },
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "Norma L1 (distancia Manhattan): usa diferencias absolutas para calcular el margen en la pérdida multi-margen. Penaliza de forma lineal y es menos sensible a outliers que L2, ideal con datos ruidosos o extremos.",
      "2": "Norma L2 (distancia euclidiana): usa diferencias al cuadrado para calcular el margen. Penaliza fuertemente errores grandes e incluye gradientes suaves, adecuado para datos limpios donde los errores grandes deben castigarse con dureza."
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "Sin reducción: devuelve la divergencia KL individual por muestra. Permite ponderación personalizada, filtrado o análisis detallado. Es esencial para estrategias avanzadas que requieren manipular la pérdida por muestra.",
      "mean": "Reducción media: promedia la divergencia KL sobre todos los elementos del lote, incluidas dimensiones espaciales. Normaliza la magnitud del gradiente respecto al número total de elementos, manteniendo gradientes consistentes sin importar la resolución.",
      "sum": "Reducción suma: agrega la divergencia KL de todos los elementos sin normalizar. La magnitud del gradiente escala con el número de elementos, por lo que exige ajustar la tasa cuando cambian el tamaño del lote o la resolución.",
      "batchmean": "Media por lote: promedia únicamente sobre la dimensión de lote, preservando la contribución espacial completa. Es la reducción recomendada para KL porque mantiene las propiedades teóricas y dinamiza un entrenamiento estable."
    },
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW para ResNet: versión de Adam con decaimiento de peso desacoplado ajustada para arquitecturas ResNet. Mejora la eficacia de la regularización y la estabilidad al separar adaptación y penalización L2. Mantiene el weight_decay independiente de las actualizaciones para evitar interferencias. Es la opción recomendada por su buena generalización y entrenamiento estable en distintas profundidades y datasets.",
      "adam": "Adam para ResNet: estimador adaptativo clásico que ajusta automáticamente la tasa por parámetro. Afecta la velocidad de convergencia y la sensibilidad a hiperparámetros. Controla momentum y gradientes al cuadrado para equilibrar exploración y explotación. Es una opción de propósito general con buen desempeño y poco ajuste.",
      "sgd": "SGD con momentum para ResNet: descenso estocástico tradicional configurado específicamente. Requiere calendarios cuidadosos de tasa para lograr el máximo rendimiento. Controla las actualizaciones mediante momentum mientras mantiene comportamiento determinista. Ofrece resultados excelentes con el ajuste adecuado, aunque demanda mayor atención que los optimizadores adaptativos."
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Scheduler por pasos para ResNet: decaimiento escalonado con épocas hito predeterminadas optimizadas para fases de entrenamiento. Reduce la tasa en momentos específicos alineados con los patrones de convergencia de ResNet. Ideal cuando conoces los momentos óptimos de decaimiento a partir de experimentos previos o literatura.",
      "cosine_annealing_lr": "Scheduler cosenoidal para ResNet: transiciones suaves que suelen mejorar la precisión final al reducir la tasa gradualmente. Minimiza oscilaciones y permite ajustes finos en las últimas fases del entrenamiento. A menudo supera a los esquemas por pasos en precisión final.",
      "reduce_lr_on_plateau": "Scheduler adaptativo para ResNet: reduce automáticamente la tasa cuando el progreso se estanca según las métricas de validación. Ajusta el calendario al comportamiento real del entrenamiento en lugar de hitos fijos. Es óptimo cuando no conoces el momento ideal o las dinámicas varían entre datasets."
    },
    "paths_description": "Configuración de rutas",
    "paths_projects_dir_description": "Directorio que contiene las carpetas de proyectos",
    "paths_ui_dir_description": "Directorio que contiene los recursos de la interfaz",
    "paths_config_dir_description": "Directorio que contiene los archivos de configuración",
    "paths_localizations_dir_description": "Directorio que contiene los archivos de localización",
    "paths_packages_file_description": "Ruta al archivo packages.jsonc",
    "paths_mappings_file_description": "Ruta al archivo de mapeos",
    "paths_cache_dir_description": "Directorio para archivos de caché"
  },
  "status_graph": {
    "epoch_accuracy": "Precisión por época",
    "epoch_loss": "Pérdida por época",
    "step_loss": "Pérdida por paso",
    "learning_rate": "Tasa de aprendizaje",
    "loss": "Pérdida",
    "no_data": "Esperando actualizaciones",
    "no_training": "No hay entrenamientos activos.",
    "active_count": "Entrenamientos activos: {count}",
    "label_training_id": "ID de entrenamiento",
    "label_status": "Estado",
    "label_phase": "Fase",
    "label_epoch": "Época",
    "label_step": "Paso",
    "badge_training": "Entrenamiento: {project}",
    "footer_training": "Entrenamiento {project} — Época {epoch} • Paso {step}"
  },
  "updates": {
    "log": {
      "check_started": "Comprobando checksums upstream...",
      "check_complete": "Verificación de actualizaciones completada. Archivos pendientes: {count}",
      "check_failed": "La verificación de actualizaciones falló: {error}",
      "remote_config_failed": "No se pudo descargar la config.json remota: {error}",
      "remote_checksum_failed": "No se pudo descargar el manifiesto de checksums remoto: {error}",
      "remote_payload_invalid": "La respuesta remota de {url} no era un mapeo válido.",
      "local_checksum_missing": "Falta checksum.json local; se asume manifiesto vacío.",
      "local_checksum_invalid": "No se pudo analizar checksum.json local: {error}",
      "path_escape": "Se bloqueó la ruta insegura {path}",
      "apply_started": "Aplicando actualizaciones...",
      "apply_failed": "No se pudieron aplicar las actualizaciones: {error}",
      "apply_nothing": "No se requieren actualizaciones.",
      "apply_file_success": "Archivo actualizado: {path}",
      "apply_file_failed": "No se pudo actualizar {path}: {error}",
      "apply_partial": "Se aplicaron {updated} actualizaciones con {failed} fallos.",
      "apply_complete": "Se aplicaron {count} actualizaciones."
    },
    "api": {
      "check_success": "Verificación de actualizaciones completada. {count} archivo(s) pendientes.",
      "check_no_updates": "Todo está al día.",
      "check_failed": "La verificación de actualizaciones falló: {error}",
      "apply_success": "Actualizaciones aplicadas correctamente. {updated} archivo(s) actualizados.",
      "apply_partial": "Actualizaciones aplicadas con {updated} éxito(s) y {failed} fallo(s).",
      "apply_failed": "No se pudieron aplicar las actualizaciones: {error}",
      "apply_nothing": "No fue necesario aplicar actualizaciones."
    },
    "status": {
      "missing": "Falta localmente",
      "outdated": "Checksum desfasado"
    }
  },
  "status": {
    "project_load_failed": "Carga de proyecto fallida",
    "project_loading": "Cargando proyecto {projectName}...",
    "project_loaded_custom": "Proyecto {projectName} cargado con config personalizado",
    "project_loaded_defaults": "Proyecto {projectName} cargado con valores predeterminados globales",
    "project_load_error": "Error al cargar proyecto {projectName}: {error}",
    "no_project_loaded": "Ningún proyecto cargado",
    "validation_errors": "Arregla errores de validación antes de guardar",
    "saving_training_config": "Guardando config de entrenamiento...",
    "training_config_saved": "Config de entrenamiento guardada",
    "save_failed": "Guardado fallido",
    "loading_schema": "Cargando esquema & config...",
    "init_failed": "Init fallido",
    "memory_load_failed": "Carga de memoria fallida",
    "checking_updates": "Verificando actualizaciones...",
    "updates_ready": "Actualizaciones disponibles",
    "updates_none": "No hay actualizaciones disponibles",
    "updates_check_failed": "Verificación de actualización fallida",
    "updates_applying": "Aplicando actualizaciones...",
    "updates_applied": "Actualizaciones aplicadas exitosamente",
    "updates_apply_failed": "Aplicación de actualización fallida",
    "generating_heatmap": "Generando mapa de calor...",
    "heatmap_generated": "Mapa de calor generado",
    "heatmap_generation_failed": "Generación de mapa de calor fallida",
    "saving_system_settings": "Guardando configuraciones del sistema...",
    "system_settings_saved": "Configuraciones del sistema guardadas",
    "starting_training": "Iniciando entrenamiento...",
    "training_started": "Entrenamiento iniciado",
    "training_stopping": "Deteniendo entrenamiento...",
    "training_stop_requested": "Detención de entrenamiento solicitada",
    "training_stop_failed": "Detención de entrenamiento fallida",
    "training_start_failed": "Inicio de entrenamiento fallido",
    "switching_language": "Cambiando idioma...",
    "language_switched": "Idioma cambiado exitosamente",
    "language_switch_failed": "Cambio de idioma fallido",
    "augmentation_preview_ready": "Vista previa de aumento generada"
  }
}
