{
  "language_name": "English",
  "app": {
    "page_not_found": {
      "title": "Page Not Found",
      "description": "The requested page does not exist"
    }
  },
  "updates": {
    "status": {
      "missing": "Missing",
      "outdated": "Outdated",
      "orphaned": "Orphaned"
    },
    "api": {
      "check_failed": "Failed to check for updates: {error}",
      "check_success": "Found {count} updates available",
      "check_no_updates": "No updates available",
      "apply_failed": "Failed to apply updates: {error}",
      "apply_nothing": "No updates to apply",
      "apply_partial": "Partial update: {updated} updated, {failed} failed",
      "apply_success": "Update complete: {updated} file(s) updated"
    }
  },
  "projects": {
    "api": {
      "create_missing": "Project name is required",
      "create_length": "Project name must be between {min} and {max} characters",
      "create_invalid": "Invalid project name",
      "create_exists": "Project '{name}' already exists",
      "create_error": "Failed to create project: {error}",
      "create_success": "Project '{name}' created successfully",
      "delete_invalid": "Invalid project name",
      "delete_not_found": "Project '{name}' not found",
      "delete_success": "Project '{name}' deleted successfully",
      "delete_error": "Failed to delete project: {error}",
      "rename_invalid": "Invalid project name",
      "rename_not_found": "Project '{name}' not found",
      "rename_missing": "New project name is required",
      "rename_success": "Project renamed to '{name}' successfully",
      "rename_error": "Failed to rename project: {error}"
    }
  },
  "dataset_editor": {
    "api": {
      "project_not_found": "Project '{project}' not found",
      "image_not_found": "No images found in project '{project}'",
      "invalid_page_size": "Invalid page size. Allowed: {allowed}",
      "invalid_size": "Invalid size. Allowed: {allowed}",
      "invalid_image_path": "Invalid image path",
      "image_missing": "Image file not found"
    }
  },
  "docs": {
    "api": {
      "missing_root": "Documentation root not configured",
      "not_found": "Document '{path}' not found",
      "decode_error": "Failed to decode '{path}'",
      "read_failed": "Failed to read document: {error}"
    }
  },
  "augmentation": {
    "preview_invalid_phase": "Invalid augmentation phase: {phase}",
    "preview_no_images": "No images found in project '{project}'",
    "preview_failed": "Failed to generate preview: {error}"
  },
  "augmentations": {
    "random_crop": "Random Crop",
    "random_resized_crop": "Random Resized Crop",
    "center_crop": "Center Crop",
    "random_horizontal_flip": "Random Horizontal Flip",
    "random_vertical_flip": "Random Vertical Flip",
    "random_rotation": "Random Rotation",
    "color_jitter": "Color Jitter",
    "random_grayscale": "Random Grayscale",
    "random_erasing": "Random Erasing",
    "normalize": "Normalize",
    "random_invert": "Random Invert",
    "random_posterize": "Random Posterize",
    "random_solarize": "Random Solarize",
    "random_adjust_sharpness": "Random Adjust Sharpness",
    "random_autocontrast": "Random Auto Contrast",
    "random_equalize": "Random Equalize",
    "random_perspective": "Random Perspective"
  },
  "common": {
    "project_label": "Project:",
    "cancel": "Cancel"
  },
  "params": {
    "lr": "Learning Rate",
    "betas": "Betas",
    "eps": "Epsilon",
    "weight_decay": "Weight Decay",
    "amsgrad": "AMSGrad",
    "momentum": "Momentum",
    "dampening": "Dampening",
    "nesterov": "Nesterov",
    "alpha": "Alpha",
    "centered": "Centered",
    "etas": "Etas",
    "step_sizes": "Step Sizes",
    "lr_decay": "LR Decay",
    "initial_accumulator_value": "Initial Accumulator",
    "rho": "Rho",
    "max_iter": "Max Iterations",
    "max_eval": "Max Evaluations",
    "tolerance_grad": "Gradient Tolerance",
    "tolerance_change": "Change Tolerance",
    "history_size": "History Size",
    "line_search_fn": "Line Search Function",
    "lambd": "Lambda",
    "t0": "T0",
    "momentum_decay": "Momentum Decay",
    "step_size": "Step Size",
    "gamma": "Gamma",
    "milestones": "Milestones",
    "T_max": "T Max",
    "eta_min": "Eta Min",
    "T_0": "T0",
    "T_mult": "T Multiplier",
    "mode": "Mode",
    "factor": "Factor",
    "patience": "Patience",
    "threshold": "Threshold",
    "threshold_mode": "Threshold Mode",
    "cooldown": "Cooldown",
    "min_lr": "Min LR",
    "base_lr": "Base LR",
    "max_lr": "Max LR",
    "step_size_up": "Step Size Up",
    "step_size_down": "Step Size Down",
    "scale_fn": "Scale Function",
    "scale_mode": "Scale Mode",
    "cycle_momentum": "Cycle Momentum",
    "base_momentum": "Base Momentum",
    "max_momentum": "Max Momentum",
    "total_steps": "Total Steps",
    "epochs": "Epochs",
    "steps_per_epoch": "Steps Per Epoch",
    "pct_start": "Percent Start",
    "anneal_strategy": "Anneal Strategy",
    "div_factor": "Div Factor",
    "final_div_factor": "Final Div Factor",
    "three_phase": "Three Phase",
    "total_iters": "Total Iterations",
    "power": "Power",
    "start_factor": "Start Factor",
    "end_factor": "End Factor",
    "lr_lambda": "LR Lambda",
    "reduction": "Reduction",
    "p": "P",
    "margin": "Margin",
    "swap": "Swap",
    "size_average": "Size Average",
    "reduce": "Reduce",
    "delta": "Delta",
    "beta": "Beta",
    "blank": "Blank",
    "zero_infinity": "Zero Infinity",
    "log_input": "Log Input",
    "full": "Full",
    "gain": "Gain",
    "a": "A",
    "b": "B",
    "mean": "Mean",
    "std": "Std",
    "val": "Value",
    "sparsity": "Sparsity",
    "groups": "Groups",
    "nonlinearity": "Nonlinearity",
    "size": "Size",
    "padding": "Padding",
    "pad_if_needed": "Pad If Needed",
    "fill": "Fill",
    "padding_mode": "Padding Mode",
    "scale": "Scale",
    "ratio": "Ratio",
    "interpolation": "Interpolation",
    "degrees": "Degrees",
    "expand": "Expand",
    "center": "Center",
    "translate": "Translate",
    "shear": "Shear",
    "distortion_scale": "Distortion Scale",
    "brightness": "Brightness",
    "contrast": "Contrast",
    "saturation": "Saturation",
    "hue": "Hue",
    "value": "Value",
    "inplace": "Inplace",
    "bits": "Bits",
    "sharpness_factor": "Sharpness Factor",
    "transforms": "Transforms"
  },
  "nav": {
    "projects": "Projects",
    "training_setup": "Training Setup",
    "dataset": "Dataset",
    "performance": "Performance",
    "heatmap": "Heatmap",
    "updates": "Updates"
  },
  "ui": {
    "language_select_title": "Language"
  },
  "context_menu": {
    "view_image": "View Image",
    "select": "Select",
    "deselect": "Deselect",
    "select_all": "Select All",
    "clear_selection": "Clear Selection",
    "copy_filename": "Copy Filename",
    "copy_path": "Copy Path",
    "delete": "Delete",
    "delete_selected": "Delete Selected ({count})",
    "open_folder": "Open in Explorer",
    "new_subfolder": "New Subfolder"
  },
  "training_controller": {
    "stop": "Stop",
    "clear": "Clear",
    "completed": "Completed",
    "stopped": "Stopped",
    "error": "Error",
    "already_running": "Training is already running"
  },
  "projects_page": {
    "title": "Projects",
    "description": "Manage your image recognition projects",
    "empty": {
      "title": "No Projects Found",
      "description": "Create a new project to get started"
    },
    "error": {
      "title": "Error Loading Projects"
    },
    "card": {
      "unknown_type": "Unknown",
      "no_stats": "No statistics available",
      "loading": "Loading",
      "error": "Error"
    },
    "stats": {
      "total_images": "Total Images",
      "balance_score": "Balance Score",
      "balance_status": "Balance Status"
    },
    "buttons": {
      "refresh": "Refresh",
      "load": "Load",
      "loaded": "Loaded",
      "start_training": "Start Training",
      "stop_training": "Stop Training",
      "new_project": "New Project",
      "rename_project": "Rename",
      "delete_project": "Delete"
    },
    "training": {
      "stop_error": "Failed to stop training",
      "start_error": "Failed to start training",
      "no_project": "No project selected"
    },
    "new_project": {
      "title": "Create New Project",
      "prompt": "Enter project name:",
      "error": "Failed to create project"
    },
    "delete_project": {
      "title": "Delete Project",
      "confirm": "Are you sure you want to delete project '{name}'? This action cannot be undone.",
      "no_selection": "No project selected",
      "error": "Failed to delete project"
    },
    "rename_project": {
      "title": "Rename Project",
      "prompt": "Enter new project name:",
      "no_selection": "No project selected",
      "error": "Failed to rename project"
    },
    "dataset_types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    }
  },
  "training_page": {
    "tabs": {
      "presets": "Presets",
      "model": "Model",
      "hyperparameters": "Hyperparameters",
      "dataloader": "Data Loader",
      "augmentation": "Augmentation",
      "optimizer": "Optimizer",
      "scheduler": "Scheduler",
      "loss": "Loss",
      "checkpoint": "Checkpoint",
      "early_stopping": "Early Stopping",
      "gradient": "Gradient",
      "runtime": "Runtime"
    },
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to configure training settings.",
      "button": "Go to Projects"
    },
    "load_defaults_button": "Load Defaults",
    "load_project_button": "Load Project",
    "save_button": "Save",
    "saving": "Saving...",
    "saved": "Saved",
    "save_error": "Save Error",
    "defaults_loaded": "Defaults Loaded",
    "project_loaded": "Project Loaded",
    "load_error": "Load Error",
    "no_settings": "No settings available",
    "augmentation": {
      "train_title": "Training Augmentations",
      "train_description": "Augmentations applied during training to improve model generalization",
      "val_title": "Validation Augmentations",
      "val_description": "Minimal augmentations for validation data (typically just resize and normalize)",
      "preview_placeholder": "Drop an image or click Random to preview augmentations",
      "btn_random": "Random",
      "btn_train": "Train",
      "btn_val": "Val",
      "no_params": "No parameters"
    },
    "presets": {
      "title": "Training Presets",
      "description": "Quick configuration templates for common training scenarios",
      "search_placeholder": "Search presets by name, description, task...",
      "only_compatible": "Only Compatible",
      "search_results": "presets found",
      "loading": "Loading presets...",
      "no_presets": "No presets available",
      "apply_error": "Failed to apply preset",
      "applied": "Preset applied successfully",
      "incompatible_dataset": "Incompatible with current dataset",
      "dataset_size": "images",
      "recommended": "Recommended",
      "apply_button": "Apply"
    }
  },
  "dataset_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to view the dataset.",
      "button": "Go to Projects"
    },
    "type_label": "Dataset Type",
    "types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    },
    "sync_button": "Sync Dataset",
    "build_button": "Build Dataset",
    "progress_discovery": "Discovering images",
    "progress_starting": "Starting...",
    "progress_complete": "Complete",
    "new_folder": "New Folder",
    "rename_folder": "Rename Folder",
    "delete_folder": "Delete Folder",
    "per_page": "Per page",
    "search_filename": "Filename",
    "search_annotation": "Annotation",
    "search_placeholder": "Search images...",
    "show_duplicates": "Show Duplicates",
    "duplicates_none": "No duplicate images found",
    "duplicates_error": "Failed to check for duplicates",
    "folders": "Folders",
    "drop_images": "Drop images here to upload",
    "root_folder": "Root",
    "delete_image": "Delete Image",
    "duplicate_count": "{count} duplicates",
    "tag_placeholder": "Add tags...",
    "annotation_placeholder": "Add annotation...",
    "confirm_delete": "Are you sure you want to delete this image?",
    "uploading": "Uploading...",
    "upload_error": "Upload failed",
    "enter_folder_name": "Enter folder name:",
    "folder_create_error": "Failed to create folder",
    "enter_new_name": "Enter new name:",
    "folder_rename_error": "Failed to rename folder",
    "delete_folder_confirm_with_images": "This folder contains {count} images. Are you sure you want to delete it?",
    "delete_folder_confirm": "Are you sure you want to delete this folder?",
    "folder_delete_error": "Failed to delete folder",
    "build_error": "Build failed",
    "status_count": "{count} images",
    "clear_selection": "Clear Selection",
    "select_all": "Select All",
    "bulk_add_tags_placeholder": "Tags to add...",
    "bulk_add": "Add",
    "bulk_remove_tags_placeholder": "Tags to remove...",
    "bulk_remove": "Remove",
    "bulk_delete": "Delete Selected",
    "selected_count": "{count} selected",
    "bulk_error": "Bulk operation failed",
    "bulk_delete_confirm": "Are you sure you want to delete {count} images?"
  },
  "performance_page": {
    "tabs": {
      "training": "Training",
      "system": "System"
    },
    "training": {
      "title": "Training Progress",
      "no_active": "No active training session",
      "no_data": "No Training Data",
      "description": "Start training to see progress graphs",
      "running": "Training in progress",
      "completed": "Training completed",
      "stopped": "Training stopped",
      "train": "Train",
      "validation": "Validation",
      "train_step_loss": "Training Step Loss",
      "val_step_loss": "Validation Step Loss",
      "step_accuracy": "Step Accuracy",
      "learning_rate": "Learning Rate",
      "epoch_loss": "Epoch Loss",
      "epoch_accuracy": "Epoch Accuracy",
      "current": "Current",
      "step": "Step",
      "train_step_accuracy": "Train Step Accuracy",
      "val_step_accuracy": "Val Step Accuracy",
      "current_lr": "Current LR",
      "train_loss": "Train Loss",
      "val_loss": "Val Loss",
      "train_accuracy": "Train Accuracy",
      "val_accuracy": "Val Accuracy"
    },
    "system": {
      "title": "System Monitor",
      "monitoring_active": "Monitoring active",
      "monitoring_paused": "Monitoring paused",
      "cpu_usage": "CPU Usage",
      "system_memory": "System Memory",
      "gpu_usage": "GPU {index} Usage",
      "gpu_memory": "GPU {index} Memory",
      "speed": "Speed",
      "cores_threads": "Cores/Threads",
      "temperature": "Temperature",
      "max_memory": "Max Memory",
      "available": "Available",
      "power": "Power",
      "gpu_clock": "GPU Clock",
      "mem_clock": "Memory Clock",
      "fan": "Fan",
      "dedicated_memory": "Dedicated Memory",
      "used_memory": "Used Memory"
    }
  },
  "heatmap_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to generate heatmaps.",
      "button": "Go to Projects"
    },
    "drop_zone": {
      "text": "Drop image here",
      "hint": "or click to select"
    },
    "alpha_label": "Overlay Alpha",
    "results": {
      "title": "Results",
      "placeholder": "Drop an image to see predictions and heatmap",
      "no_predictions": "No predictions available"
    },
    "live_model_switch": "Use live model",
    "random_button": "Random",
    "refresh_button": "Refresh",
    "checkpoint_label": "Checkpoint:"
  },
  "updates_ui": {
    "page_title": "Updates",
    "page_description": "Check for and apply system updates",
    "intro": "Check for updates to keep your system current with the latest features and fixes.",
    "check_button": "Check for Updates",
    "apply_button": "Apply Updates",
    "status_idle": "Ready to check for updates",
    "status_checking": "Checking for updates...",
    "status_ready": "Updates available",
    "status_up_to_date": "System is up to date",
    "status_applying": "Applying updates...",
    "status_applied": "Updates applied successfully",
    "status_failed": "Update check failed",
    "no_updates": "No updates available",
    "files_to_update": "files to update",
    "orphaned_count": "orphaned files",
    "table_header_file": "File",
    "table_header_status": "Status"
  },
  "schema": {
    "description": "Configuration schema for Hootsight image recognition system",
    "ui_group": {
      "general": "General",
      "api": "API Settings",
      "ui_settings": "User Interface",
      "paths": "Paths",
      "system": "System",
      "memory": "Memory Management",
      "dataset": "Dataset",
      "dataset_editor": "Dataset Editor",
      "training_model": "Model",
      "training_hyperparameters": "Hyperparameters",
      "training_dataloader": "Data Loader",
      "training_augmentation": "Augmentation",
      "training_optimizer": "Optimizer",
      "training_scheduler": "Scheduler",
      "training_loss": "Loss Function",
      "training_checkpoint": "Checkpointing",
      "training_early_stopping": "Early Stopping",
      "training_gradient": "Gradient",
      "training_runtime": "Runtime",
      "activations_defaults": "Activation Defaults",
      "augmentations_defaults": "Augmentation Defaults",
      "optimizers": "Optimizers",
      "schedulers": "Schedulers",
      "losses": "Losses",
      "activations": "Activations",
      "augmentations": "Augmentations",
      "regularization": "Regularization",
      "normalization": "Normalization",
      "pooling": "Pooling",
      "models": "Models"
    },
    "general_description": "General application settings",
    "general_language_description": "User interface language",
    "general_language_enum_descriptor": {
      "en": "English"
    },
    "api_description": "API server configuration",
    "api_host_description": "API server host address",
    "api_port_description": "API server port number",
    "ui_description": "User interface settings",
    "ui_title_description": "Application window title",
    "ui_width_description": "Window width in pixels",
    "ui_height_description": "Window height in pixels",
    "ui_resizable_description": "Allow window resizing",
    "paths_description": "File and directory paths",
    "paths_projects_dir_description": "Directory containing projects",
    "paths_ui_dir_description": "Directory containing UI files",
    "paths_config_dir_description": "Directory containing configuration files",
    "paths_localizations_dir_description": "Directory containing localization files",
    "paths_packages_file_description": "Path to packages configuration file",
    "paths_cache_dir_description": "Directory for cached files",
    "paths_docs_dir_description": "Directory for documentation",
    "system_description": "System configuration settings",
    "system_max_threads_description": "Maximum number of worker threads",
    "system_max_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "system_fallback_batch_size_description": "Fallback batch size when auto-detection fails",
    "system_startup_wait_seconds_description": "Seconds to wait before starting UI",
    "system_http_timeout_description": "HTTP request timeout in seconds",
    "system_update_repository_url_description": "URL for update repository",
    "system_update_skip_paths_description": "Paths to skip during updates",
    "dataset_editor_description": "Dataset editor configuration",
    "dataset_editor_page_sizes_description": "Available page size options",
    "dataset_editor_default_page_size_description": "Default images per page",
    "dataset_editor_size_presets_description": "Image size presets by model type",
    "dataset_editor_default_size_description": "Default image size",
    "dataset_editor_build_workers_description": "Number of workers for dataset building",
    "dataset_editor_build_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_discovery_workers_description": "Number of workers for image discovery",
    "dataset_editor_discovery_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_project_name_validation_description": "Project name validation rules",
    "dataset_editor_project_name_validation_min_length_description": "Minimum project name length",
    "dataset_editor_project_name_validation_max_length_description": "Maximum project name length",
    "dataset_editor_project_name_validation_pattern_description": "Allowed project name pattern (regex)",
    "memory_description": "Memory management configuration",
    "memory_target_memory_usage_description": "Target GPU memory usage ratio",
    "memory_safety_margin_description": "Safety margin for memory allocation",
    "memory_augmentation_threads_description": "Threads for augmentation processing",
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "memory_reserved_memory_ratio_description": "Reserved memory ratio",
    "memory_per_sample_multiplier_description": "Memory multiplier per sample",
    "memory_min_batch_size_description": "Minimum batch size",
    "memory_max_batch_size_description": "Maximum batch size",
    "memory_thresholds_description": "Memory usage thresholds",
    "memory_thresholds_high_usage_description": "High memory usage threshold",
    "memory_thresholds_moderate_usage_description": "Moderate memory usage threshold",
    "memory_thresholds_low_usage_description": "Low memory usage threshold",
    "memory_batch_size_limits_description": "Batch size warning limits",
    "memory_batch_size_limits_small_description": "Small batch size warning threshold",
    "memory_batch_size_limits_large_description": "Large batch size warning threshold",
    "training_description": "Training configuration",
    "training_model_type_title": "Model Type",
    "training_model_type_description": "Select the neural network architecture family. Each architecture offers unique characteristics in terms of accuracy, inference speed, memory usage, and model file size. Your choice should be based on your deployment target (server, mobile, edge device) and accuracy requirements.",
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet (Residual Networks) - The foundational deep learning architecture that introduced skip connections, solving the vanishing gradient problem and enabling training of very deep networks (up to 152 layers). ResNet is the industry benchmark with proven reliability across countless applications. Offers excellent balance between accuracy and computational cost. ResNet50 is the most popular variant, achieving ~76% ImageNet top-1 accuracy. Ideal for: general image classification, transfer learning base, medical imaging, quality inspection, facial recognition. Choose ResNet18/34 for faster inference, ResNet101/152 when maximum accuracy is critical.",
      "resnext": "ResNeXt (Residual Networks with Aggregated Transformations) - An evolution of ResNet that introduces cardinality (number of parallel pathways) as a new dimension alongside depth and width. Uses grouped convolutions to learn richer feature representations. Achieves 1-2% higher accuracy than equivalent ResNet models with similar computational cost. The 32x4d configuration means 32 parallel pathways with 4-dimensional transforms. Ideal for: fine-grained visual recognition (bird species, car models), complex multi-attribute classification, fashion/style analysis, texture recognition, cases where ResNet accuracy plateaus. Best when you need that extra accuracy boost without major architectural changes.",
      "mobilenet": "MobileNet (Mobile Networks) - Specifically designed for mobile and embedded devices using depthwise separable convolutions that dramatically reduce computation. MobileNetV3 incorporates neural architecture search (NAS) and squeeze-and-excitation blocks. Achieves 75-80% of ResNet50 accuracy while being 3-5x faster with 10x smaller model size. Typical inference: <10ms on modern phones. Ideal for: iOS/Android apps, Raspberry Pi, Jetson Nano, real-time video processing, AR/VR applications, drone vision, IoT sensors, smart cameras, retail analytics. Choose V3-Small for ultra-fast inference, V3-Large for better accuracy while remaining mobile-friendly.",
      "shufflenet": "ShuffleNet - Designed for maximum efficiency on CPU and low-power devices using channel shuffle operations and pointwise group convolutions. Achieves excellent speed-accuracy trade-off by efficiently mixing information between channel groups. Even faster than MobileNet on CPU-only devices. Model sizes range from ultra-light (x0.5, ~1MB) to more capable (x2.0, ~7MB). Ideal for: strict latency requirements (<5ms), robotics vision, wearable devices, always-on smart cameras, traffic monitoring, industrial inspection on resource-constrained hardware, battery-powered devices. Not recommended for tasks requiring maximum accuracy or fine-grained detail recognition.",
      "squeezenet": "SqueezeNet - The most compact architecture, achieving AlexNet-level accuracy with 50x fewer parameters using innovative Fire modules (squeeze and expand layers). Model size as small as ~5MB (or ~0.5MB with deep compression). Designed for extreme model compression while maintaining reasonable accuracy. Ideal for: microcontrollers with <10MB storage, FPGA deployment, over-the-air model updates on bandwidth-limited devices, embedded systems with strict memory constraints, always-on vision applications where power consumption is critical. SqueezeNet 1.1 is 2.4x faster than 1.0. Not recommended for complex tasks or large datasets - limited capacity will cause underfitting.",
      "efficientnet": "EfficientNet - State-of-the-art architecture using compound scaling that uniformly scales network depth, width, and resolution with fixed ratios derived from neural architecture search. Achieves best-in-class accuracy while being 8-10x more efficient than previous architectures. EfficientNet-B0 matches ResNet50 accuracy with 5x fewer parameters. B4-B7 variants achieve up to 84%+ ImageNet accuracy. EfficientNetV2 offers faster training with progressive learning. Ideal for: maximum accuracy requirements, medical image analysis (X-ray, MRI, pathology), fine-grained recognition (species identification, defect detection), satellite/aerial imagery, scientific imaging, art authentication, forensic analysis. The gold standard when accuracy matters most."
    },
    "training_model_name_title": "Model Name",
    "training_model_name_description": "Specific model variant",
    "training_pretrained_title": "Pretrained",
    "training_pretrained_description": "Use pretrained weights from ImageNet",
    "training_task_title": "Task",
    "training_task_description": "Define how the model should interpret and predict from images. This fundamentally determines the output format and loss function used during training.",
    "training_task_enum_descriptor": {
      "classification": "Single-label Classification - Each image belongs to exactly ONE class from a predefined set. The model outputs a probability distribution over all classes, selecting the highest as the prediction. Uses softmax activation and cross-entropy loss. Ideal for: mutually exclusive categories (cat vs dog, product types, digit recognition 0-9, sentiment from faces, pass/fail inspection). Expected output: single class label with confidence score. Accuracy typically 85-99% depending on task complexity and data quality.",
      "multi_label": "Multi-label Classification - Each image can have MULTIPLE tags/labels simultaneously. The model treats each label as an independent binary decision. Uses sigmoid activation and binary cross-entropy loss. Ideal for: image tagging (outdoor + sunny + people), content moderation (violence + nudity flags), product attributes (red + leather + large), medical symptoms (multiple conditions present). Expected output: set of applicable labels with individual confidence scores. Each label threshold can be tuned independently.",
      "detection": "Object Detection - Locate and classify MULTIPLE objects within an image with bounding boxes. Model predicts object locations (x, y, width, height), class labels, and confidence scores for each detected instance. Ideal for: counting objects, surveillance, autonomous vehicles, retail shelf analysis, document element detection. Expected output: list of detected objects with bounding boxes and class labels. Note: Requires specialized architectures (YOLO, Faster R-CNN) - standard classifiers won't work.",
      "segmentation": "Semantic Segmentation - Classify EVERY PIXEL in the image into predefined categories. Creates a dense prediction map where each pixel is assigned a class label. Ideal for: medical image analysis (tumor boundaries), autonomous driving (road/sidewalk/vehicle regions), satellite imagery (land use mapping), industrial inspection (defect area measurement). Expected output: pixel-wise class map same size as input. Note: Requires specialized architectures (U-Net, DeepLab) - computationally intensive."
    },
    "training_batch_size_title": "Batch Size",
    "training_batch_size_description": "Training batch size",
    "training_epochs_title": "Epochs",
    "training_epochs_description": "Number of training epochs",
    "training_epochs_enum_descriptor": {
      "auto": "Automatic (based on dataset size)"
    },
    "training_learning_rate_title": "Learning Rate",
    "training_learning_rate_description": "Initial learning rate",
    "training_weight_decay_title": "Weight Decay",
    "training_weight_decay_description": "Weight decay (L2 regularization)",
    "training_input_size_title": "Input Size",
    "training_input_size_description": "Input image size",
    "training_normalize_title": "Normalize",
    "training_normalize_description": "Normalization parameters",
    "training_normalize_mean_title": "Mean",
    "training_normalize_mean_description": "Mean values for each channel (RGB)",
    "training_normalize_std_title": "Std",
    "training_normalize_std_description": "Standard deviation for each channel (RGB)",
    "training_val_ratio_title": "Validation Ratio",
    "training_val_ratio_description": "Validation split ratio",
    "training_dataloader_title": "Dataloader",
    "training_dataloader_description": "Data loader configuration",
    "training_dataloader_num_workers_title": "Num Workers",
    "training_dataloader_num_workers_description": "Number of data loading workers",
    "training_dataloader_pin_memory_title": "Pin Memory",
    "training_dataloader_pin_memory_description": "Pin memory for faster GPU transfer",
    "training_dataloader_persistent_workers_title": "Persistent Workers",
    "training_dataloader_persistent_workers_description": "Keep workers alive between epochs",
    "training_dataloader_prefetch_factor_title": "Prefetch Factor",
    "training_dataloader_prefetch_factor_description": "Number of batches to prefetch per worker",
    "training_augmentation_title": "Augmentation",
    "training_augmentation_description": "Data augmentation configuration",
    "training_augmentation_train_title": "Training Augmentations",
    "training_augmentation_train_description": "Augmentations for training data",
    "training_augmentation_val_title": "Validation Augmentations",
    "training_augmentation_val_description": "Augmentations for validation data",
    "training_optimizer_type_title": "Optimizer Type",
    "training_optimizer_type_description": "The optimization algorithm determines how model weights are updated during training. Each optimizer has different characteristics for convergence speed, stability, and memory usage. The right choice depends on your model size, dataset, and training goals.",
    "training_optimizer_type_enum_descriptor": {
      "sgd": "SGD (Stochastic Gradient Descent) - The classic, fundamental optimizer. Updates weights by moving in the direction of negative gradient scaled by learning rate. With momentum, accumulates velocity for faster convergence and smoother updates. Simple, memory-efficient, and well-understood. Best for: large-scale training, when you need reproducibility, fine-tuning pretrained models, ConvNets with proper LR scheduling. Expected results: reliable convergence, often achieves best final accuracy with proper tuning. Requires careful learning rate selection and typically needs LR scheduling.",
      "adam": "Adam (Adaptive Moment Estimation) - Combines momentum (first moment) with RMSprop's adaptive learning rates (second moment). Maintains per-parameter learning rates that adapt based on gradient history. Fast convergence with minimal hyperparameter tuning. Best for: quick prototyping, transformers, NLP tasks, sparse gradients, when you want good results without extensive tuning. Expected results: fast initial convergence, good default performance. May generalize slightly worse than well-tuned SGD on some vision tasks.",
      "adamw": "AdamW (Adam with Decoupled Weight Decay) - Fixes Adam's weight decay implementation by decoupling it from gradient updates. This is the CORRECT way to apply L2 regularization with adaptive optimizers. Recommended over standard Adam for most use cases. Best for: any task where you'd use Adam, especially with weight decay/regularization, transformers, modern architectures. Expected results: better generalization than Adam, more effective regularization, often the best default choice for deep learning.",
      "adamax": "Adamax - Adam variant using infinity norm (max) instead of L2 norm for second moment. More stable with large or sparse gradients. Less sensitive to learning rate choice. Best for: embeddings with large vocabulary, NLP tasks, when Adam shows unstable behavior, sparse or noisy gradients. Expected results: more stable training than Adam in edge cases, similar convergence speed.",
      "nadam": "NAdam (Nesterov-accelerated Adam) - Incorporates Nesterov momentum into Adam, providing look-ahead gradient computation. Combines fast adaptive learning with improved momentum. Best for: tasks benefiting from Nesterov momentum, RNNs, sequence models, when you want faster convergence than Adam. Expected results: slightly faster convergence than Adam, better handling of gradient noise.",
      "radam": "RAdam (Rectified Adam) - Addresses Adam's variance problem in early training by introducing a rectification term. Provides automatic warmup without needing explicit LR warmup schedules. More stable early training. Best for: training from scratch without warmup, when Adam shows erratic early behavior, automated pipelines where manual warmup tuning isn't feasible. Expected results: stable training from step 1, eliminates need for warmup, similar final performance to Adam.",
      "rmsprop": "RMSprop (Root Mean Square Propagation) - Divides learning rate by running average of recent gradient magnitudes. Adapts learning rate per-parameter. Precursor to Adam without momentum component. Best for: RNNs and recurrent architectures, non-stationary objectives, online learning. Expected results: good for recurrent networks, less effective than Adam for most CNN tasks.",
      "rprop": "Rprop (Resilient Propagation) - Uses only the sign of gradients, not magnitude. Each parameter has its own adaptive step size that increases when gradient sign is consistent, decreases on sign changes. Best for: full-batch training only, small datasets that fit in memory, when gradient magnitudes are unreliable. Expected results: fast convergence for full-batch, NOT suitable for mini-batch training (will fail).",
      "adagrad": "Adagrad (Adaptive Gradient) - Accumulates all past squared gradients, giving frequently updated parameters smaller learning rates. Good for sparse features. Best for: sparse data (NLP, recommendations), features with very different frequencies, when some parameters need much smaller updates. Expected results: handles sparse gradients well. Drawback: learning rate continuously decreases, may stop learning too early on dense tasks.",
      "adadelta": "Adadelta - Addresses Adagrad's diminishing learning rate by using windowed accumulated gradients instead of all past gradients. No need to set initial learning rate. Best for: when you don't want to tune learning rate, continuation of Adagrad without learning rate decay. Expected results: more sustained learning than Adagrad, but generally outperformed by Adam family.",
      "sparse_adam": "Sparse Adam - Adam variant optimized for sparse gradient tensors. Only updates parameters that received non-zero gradients. Memory efficient for large embedding tables. Best for: large embedding layers, sparse NLP tasks, recommendation systems with huge item catalogs. Expected results: same as Adam but with better memory efficiency for sparse updates.",
      "lbfgs": "L-BFGS (Limited-memory BFGS) - Second-order optimizer approximating the Hessian matrix using limited memory. Powerful but requires full-batch gradients. Best for: small datasets, neural style transfer, physics-informed networks, when you can compute full-batch gradients. Expected results: very fast convergence when applicable. NOT suitable for mini-batch training or large datasets.",
      "asgd": "ASGD (Averaged SGD) - Maintains running average of parameters during training, using averaged weights for final model. Theoretical convergence guarantees. Best for: convex or nearly-convex problems, when you want theoretical guarantees, simple models. Expected results: good convergence for simpler models, less effective for deep networks compared to Adam/SGD with momentum."
    },
    "training_optimizer_lr_title": "Optimizer LR",
    "training_optimizer_lr_description": "Optimizer learning rate",
    "training_optimizer_weight_decay_title": "Optimizer Weight Decay",
    "training_optimizer_weight_decay_description": "Optimizer weight decay",
    "training_optimizer_params_title": "Optimizer Parameters",
    "training_optimizer_params_description": "Optimizer-specific parameters",
    "training_scheduler_type_title": "Scheduler Type",
    "training_scheduler_type_description": "Learning rate scheduling controls how the learning rate changes during training. Proper scheduling can dramatically improve convergence, final accuracy, and training stability. Different schedulers suit different training scenarios and model architectures.",
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR - Decreases learning rate by a fixed factor (gamma) every N epochs. Simple and predictable. Best for: standard training schedules, when you know approximately when to reduce LR, baseline experiments. Expected results: steady training with periodic jumps in loss when LR drops, then continued improvement. Typically multiply LR by 0.1 every 30-50 epochs.",
      "multi_step_lr": "Multi-Step LR - Decreases learning rate at specified epoch milestones. More control than Step LR. Best for: when you know exact epochs where LR should drop, reproducing published training schedules, fine-grained control. Expected results: similar to Step LR but with custom timing. Common pattern: drop at 60%, 80% of total epochs.",
      "exponential_lr": "Exponential LR - Multiplies learning rate by gamma every epoch for continuous smooth decay. Best for: gradual fine-tuning, when step-wise drops are too abrupt, long training runs. Expected results: smooth, continuous LR reduction. Requires careful gamma selection (typically 0.95-0.99) to avoid decaying too fast.",
      "cosine_annealing_lr": "Cosine Annealing - Learning rate follows a cosine curve from initial to minimum value. Smooth decay that slows near the end. Very popular for modern training. Best for: fixed-length training, achieving best final accuracy, most CNN training, competition models. Expected results: excellent final accuracy, smooth training. The gentle decay near minimum allows fine convergence.",
      "cosine_annealing_warm_restarts": "Cosine with Warm Restarts - Cosine annealing that periodically resets to initial LR. Each restart explores different loss surface regions. Best for: escaping local minima, ensemble-like single model training, when single cosine plateaus. Expected results: can find better optima than single decay, each restart may improve or explore differently. Good for longer training.",
      "reduce_lr_on_plateau": "Reduce on Plateau - Monitors a metric (usually val_loss) and reduces LR when it stops improving. Adaptive and requires no epoch planning. Best for: when you don't know optimal schedule, variable-length training, when validation loss behavior is unpredictable. Expected results: automatic adaptation to training dynamics. May train longer but finds good LR automatically.",
      "cyclic_lr": "Cyclic LR - Learning rate oscillates between minimum and maximum bounds. Can use triangular, triangular2, or exp_range policies. Best for: escaping local minima, when you want to explore multiple LR ranges, super-convergence experiments. Expected results: periodic loss oscillation, potentially better generalization. Requires careful bound selection.",
      "one_cycle_lr": "One Cycle - Starts low, ramps up to max LR, then anneals down. Enables super-convergence with very high learning rates. Best for: fast training with high LR, achieving good accuracy quickly, when training time is limited. Expected results: fast convergence, often matches or beats longer schedules in less time. Requires proper max LR selection (LR range test recommended).",
      "polynomial_lr": "Polynomial LR - Decays learning rate using polynomial function with specified power. Power=1 is linear, higher powers decay faster initially. Best for: fine-grained control over decay curve shape, matching specific published schedules. Expected results: customizable decay shape. Power=2 (quadratic) gives faster early decay, slower late decay.",
      "linear_lr": "Linear LR - Simple linear interpolation between start and end learning rates. Can implement warmup (increasing) or decay (decreasing). Best for: warmup phases, simple interpretable schedules, combining with other schedulers. Expected results: predictable, steady LR change. Often used for first few epochs as warmup.",
      "lambda_lr": "Lambda LR - User-defined function determines LR at each epoch. Maximum flexibility. Best for: custom schedules, research experiments, when no built-in scheduler fits your needs. Expected results: whatever your function defines. Requires programming custom lambda function.",
      "multiplicative_lr": "Multiplicative LR - Multiplies current LR by a factor returned by user function each epoch. Cumulative effect. Best for: custom decay patterns, when you want schedule to depend on current LR. Expected results: depends on your multiplicative function. Useful for implementing custom adaptive schedules."
    },
    "training_scheduler_step_size_title": "Step Size",
    "training_scheduler_step_size_description": "Scheduler step size (epochs)",
    "training_scheduler_gamma_title": "Gamma",
    "training_scheduler_gamma_description": "Learning rate decay factor",
    "training_scheduler_params_title": "Scheduler Parameters",
    "training_scheduler_params_description": "Scheduler-specific parameters",
    "training_loss_type_title": "Loss Type",
    "training_loss_type_description": "Loss functions measure how well model predictions match ground truth. The choice of loss function fundamentally shapes what the model learns. Different losses have different gradients, sensitivities, and behaviors. Wrong choice can cause training failure, poor generalization, or biased predictions.",
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Cross Entropy - Standard loss for multi-class classification. Combines log-softmax and NLL loss. Heavily penalizes confident wrong predictions. Best for: image classification, most multi-class problems, mutually exclusive categories. Expected results: well-calibrated probabilities, fast convergence on balanced data. NEGATIVE EFFECTS: Struggles with class imbalance (rare classes get ignored), highly sensitive to label noise (mislabeled samples cause large gradients), can become overconfident on out-of-distribution data.",
      "nll_loss": "NLL Loss (Negative Log Likelihood) - Expects log-probabilities as input (apply log_softmax first). Mathematically equivalent to cross_entropy when combined with log_softmax. Best for: when you need explicit log-softmax for other purposes, custom probability handling. Expected results: same as cross_entropy. NEGATIVE EFFECTS: Same as cross_entropy. Additionally, forgetting log_softmax causes silent failure with wrong gradients.",
      "bce_loss": "BCE Loss (Binary Cross Entropy) - For binary classification or multi-label problems. Expects probabilities (apply sigmoid first). Each output is independent binary decision. Best for: binary classification, multi-label classification where items can have multiple labels. Expected results: independent probability per class, good for multi-label. NEGATIVE EFFECTS: Numerically unstable with extreme predictions (use bce_with_logits instead), sensitive to class imbalance in multi-label settings, requires careful threshold selection for predictions.",
      "bce_with_logits": "BCE with Logits - Numerically stable BCE that combines sigmoid and BCE in one operation. Preferred over separate sigmoid + BCE. Best for: all cases where you'd use BCE loss, binary and multi-label classification. Expected results: stable training, same results as BCE but more robust. NEGATIVE EFFECTS: Same conceptual issues as BCE (class imbalance, threshold selection), but numerically stable. Still struggles with extreme positive/negative imbalance.",
      "multi_margin": "Multi-Margin Loss - Hinge-based loss for multi-class SVM-style classification. Creates margin between correct class and others. Best for: SVM-style classifiers, when you want maximum margin separation, robust classification. Expected results: larger margins between classes, potentially more robust to outliers than cross_entropy. NEGATIVE EFFECTS: Harder to tune margin parameter, doesn't produce calibrated probabilities (outputs aren't interpretable as confidence), may converge slower than cross_entropy.",
      "multi_label_margin": "Multi-Label Margin Loss - Margin-based loss for multi-label classification. Ensures positive labels score higher than negative labels by a margin. Best for: multi-label ranking where relative ordering matters, multi-label SVMs. Expected results: positive labels consistently score above negatives. NEGATIVE EFFECTS: Doesn't provide calibrated probabilities, sensitive to margin hyperparameter, can struggle when positive and negative labels have very different frequencies.",
      "multi_label_soft_margin": "Multi-Label Soft Margin - Sigmoid cross-entropy for multi-label, treating each label as independent binary. Essentially BCE across all labels. Best for: multi-label classification, standard choice for multi-label problems. Expected results: similar to BCE, handles multiple labels well. NEGATIVE EFFECTS: Each label trained independently ignores label correlations, imbalanced labels can dominate training, may predict incoherent label combinations.",
      "mse_loss": "MSE Loss (Mean Squared Error) - Squared difference between predictions and targets. Standard regression loss. Penalizes large errors heavily (quadratic). Best for: regression tasks, predicting continuous values, when errors are Gaussian distributed. Expected results: predictions centered on mean, sensitive to outliers. NEGATIVE EFFECTS: Very sensitive to outliers (single outlier can dominate loss), tends to predict mean values in ambiguous cases, can cause gradient explosion with large errors early in training.",
      "l1_loss": "L1 Loss (Mean Absolute Error) - Absolute difference between predictions and targets. Linear penalty, more robust to outliers than MSE. Best for: regression with outliers, when you want median predictions, robust estimation. Expected results: predictions closer to median, less affected by outliers. NEGATIVE EFFECTS: Non-smooth gradient at zero (discontinuity), can cause unstable updates near correct predictions, slower convergence than MSE on clean data.",
      "smooth_l1": "Smooth L1 Loss - Combines L1 and L2: quadratic when error is small, linear when large. Best of both worlds. Best for: object detection (bounding box regression), robust regression, when you want MSE benefits without outlier sensitivity. Expected results: stable training, robust to outliers, smooth gradients. NEGATIVE EFFECTS: Requires tuning beta parameter for transition point, still somewhat sensitive to outliers (just less than MSE), adds hyperparameter complexity.",
      "huber_loss": "Huber Loss - Similar to Smooth L1. Quadratic for small errors, linear for large. Robust regression standard. Best for: regression with potential outliers, financial predictions, any regression where outliers are possible. Expected results: robust predictions, smooth convergence. NEGATIVE EFFECTS: Delta parameter requires tuning for your error scale, wrong delta makes it equivalent to pure L1 or MSE negating benefits.",
      "kl_div": "KL Divergence (Kullback-Leibler) - Measures how one probability distribution diverges from another. Used for distribution matching. Best for: knowledge distillation, VAE regularization, matching output to target distribution. Expected results: output distribution approaches target distribution. NEGATIVE EFFECTS: Asymmetric (KL(P||Q) != KL(Q||P)), infinite when target has zeros where prediction doesn't, requires careful log-probability handling, mode-seeking behavior can miss multimodal targets.",
      "margin_ranking": "Margin Ranking Loss - Learns to rank pairs: x1 should be ranked higher than x2 when y=1. Margin-based pairwise ranking. Best for: learning to rank, recommendation systems, when relative ordering matters more than absolute values. Expected results: correctly ordered pairs with margin separation. NEGATIVE EFFECTS: Requires pair generation (quadratic complexity), doesn't directly optimize list-wise ranking metrics, margin parameter needs tuning, sensitive to pair sampling strategy.",
      "hinge_embedding": "Hinge Embedding Loss - For learning embeddings where similar items should be close, dissimilar items far apart. Uses hinge loss formulation. Best for: embedding learning, similarity learning, metric learning with pairs. Expected results: meaningful embedding space with margin separation. NEGATIVE EFFECTS: Hard to tune margin, can collapse embeddings if margin too large, requires careful positive/negative pair mining.",
      "triplet_margin": "Triplet Margin Loss - Takes anchor, positive, and negative samples. Learns embeddings where anchor is closer to positive than negative by margin. Best for: face recognition, image retrieval, fine-grained similarity. Expected results: strong embedding space for similarity search. NEGATIVE EFFECTS: Requires expensive triplet mining, many triplets become uninformative (too easy), training can be slow, margin and mining strategy critical for success, may need hard negative mining.",
      "cosine_embedding": "Cosine Embedding Loss - Learns embeddings using cosine similarity. Similar pairs should have high cosine similarity, dissimilar pairs low. Best for: text embeddings, when magnitude shouldn't matter, semantic similarity tasks. Expected results: direction-based similarity in embedding space. NEGATIVE EFFECTS: Ignores magnitude information (sometimes important), sensitive to initialization, can struggle with fine-grained distinctions.",
      "ctc_loss": "CTC Loss (Connectionist Temporal Classification) - For sequence-to-sequence where alignment is unknown. Sums over all valid alignments. Best for: speech recognition, OCR, handwriting recognition, any sequence task without explicit alignment. Expected results: learns alignment implicitly, handles variable length. NEGATIVE EFFECTS: Assumes output sequence shorter than input, computationally expensive, can struggle with long sequences, requires blank token handling, peaky output distributions can cause recognition errors.",
      "poisson_nll": "Poisson NLL Loss - Negative log likelihood assuming Poisson-distributed targets. For non-negative integer count data. Best for: event counting, rate estimation, any non-negative count prediction. Expected results: appropriate for count data, handles zero-inflation better than MSE. NEGATIVE EFFECTS: Assumes variance equals mean (real data often overdispersed), requires positive predictions (use exp transform), not suitable for continuous targets.",
      "gaussian_nll": "Gaussian NLL Loss - NLL assuming Gaussian targets. Model predicts both mean and variance, enabling uncertainty estimation. Best for: regression with uncertainty, confidence intervals, heteroscedastic regression. Expected results: calibrated uncertainty estimates alongside predictions. NEGATIVE EFFECTS: Can predict high variance to avoid penalty (variance collapse), requires careful initialization of variance head, doubles output size, training can be unstable if variance predictions poorly initialized."
    },
    "training_loss_reduction_title": "Reduction",
    "training_loss_reduction_description": "Loss reduction method",
    "training_loss_reduction_enum_descriptor": {
      "mean": "Mean - Average over batch",
      "sum": "Sum - Sum over batch",
      "none": "None - No reduction"
    },
    "training_loss_params_title": "Loss Parameters",
    "training_loss_params_description": "Loss function parameters",
    "training_weight_init_title": "Weight Initialization",
    "training_weight_init_description": "Weight initialization settings",
    "training_weight_init_type_title": "Type",
    "training_weight_init_params_title": "Init Parameters",
    "training_weight_init_params_description": "Weight initialization parameters",
    "weight_init_type_enum_descriptor": {
      "kaiming_normal": "Kaiming Normal - He initialization (normal)",
      "kaiming_uniform": "Kaiming Uniform - He initialization (uniform)",
      "xavier_normal": "Xavier Normal - Glorot initialization (normal)",
      "xavier_uniform": "Xavier Uniform - Glorot initialization (uniform)",
      "normal": "Normal - Gaussian initialization",
      "uniform": "Uniform - Uniform distribution",
      "trunc_normal": "Truncated Normal - Bounded Gaussian",
      "orthogonal": "Orthogonal - Orthogonal matrix",
      "sparse": "Sparse - Sparse initialization",
      "constant": "Constant - Fixed value",
      "zeros": "Zeros - All zeros",
      "ones": "Ones - All ones",
      "eye": "Eye - Identity matrix",
      "dirac": "Dirac - Delta function"
    },
    "training_checkpoint_title": "Checkpoint",
    "training_checkpoint_description": "Model checkpointing configuration",
    "training_checkpoint_save_best_only_title": "Save Best Only",
    "training_checkpoint_save_frequency_title": "Save Frequency",
    "training_checkpoint_max_checkpoints_title": "Max Checkpoints",
    "training_checkpoint_checkpoint_dir_title": "Checkpoint Directory",
    "training_checkpoint_best_model_filename_title": "Best Model Filename",
    "training_early_stopping_title": "Early Stopping",
    "training_early_stopping_description": "Early stopping configuration",
    "training_early_stopping_enabled_title": "Enabled",
    "training_early_stopping_patience_title": "Patience",
    "training_early_stopping_min_delta_title": "Min Delta",
    "training_early_stopping_monitor_title": "Monitor",
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Validation Loss",
      "val_accuracy": "Validation Accuracy"
    },
    "training_gradient_title": "Gradient",
    "training_gradient_description": "Gradient configuration",
    "training_gradient_clip_norm_title": "Clip Norm",
    "training_gradient_clip_value_title": "Clip Value",
    "training_gradient_accumulation_steps_title": "Accumulation Steps",
    "training_runtime_title": "Runtime",
    "training_runtime_description": "Runtime optimization settings",
    "training_runtime_mixed_precision_title": "Mixed Precision",
    "training_runtime_mixed_precision_description": "Enable mixed precision training (FP16)",
    "training_runtime_channels_last_title": "Channels Last",
    "training_runtime_channels_last_description": "Use channels-last memory format",
    "training_runtime_allow_tf32_title": "Allow TF32",
    "training_runtime_allow_tf32_description": "Allow TF32 on Ampere GPUs",
    "training_runtime_cudnn_benchmark_title": "cuDNN Benchmark",
    "training_runtime_cudnn_benchmark_description": "Enable cuDNN auto-tuning",
    "dataset_description": "Dataset configuration",
    "dataset_image_extensions_description": "Supported image file extensions",
    "dataset_discovery_description": "Dataset discovery settings",
    "dataset_discovery_annotation_formats_description": "Annotation format detection",
    "dataset_discovery_txt_annotations_description": "Text annotation detection",
    "dataset_discovery_txt_annotations_min_coverage_percent_description": "Minimum coverage for text annotations",
    "dataset_discovery_balance_analysis_description": "Dataset balance analysis",
    "dataset_discovery_balance_analysis_min_images_per_class_description": "Minimum images per class",
    "dataset_discovery_balance_analysis_critical_shortage_threshold_description": "Critical shortage threshold",
    "dataset_discovery_balance_analysis_over_representation_ratio_description": "Over-representation ratio",
    "dataset_discovery_balance_analysis_under_representation_ratio_description": "Under-representation ratio",
    "dataset_discovery_balance_analysis_severe_over_representation_ratio_description": "Severe over-representation ratio",
    "dataset_discovery_balance_analysis_hierarchical_balance_threshold_description": "Hierarchical balance threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_description": "Dataset size warning thresholds",
    "dataset_discovery_balance_analysis_dataset_size_warnings_tiny_dataset_description": "Tiny dataset warning threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_small_dataset_description": "Small dataset warning threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_description": "Balance score classification thresholds",
    "dataset_discovery_balance_analysis_balance_score_thresholds_legendary_description": "Legendary balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_excellent_description": "Excellent balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_very_good_description": "Very good balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_good_description": "Good balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_balanced_description": "Balanced threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_slightly_unbalanced_description": "Slightly unbalanced threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_fair_description": "Fair balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_poor_description": "Poor balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_very_poor_description": "Very poor balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_critical_description": "Critical balance threshold",
    "optimizers_description": "Optimizer configurations",
    "optimizers_defaults_description": "Default optimizer parameters",
    "optimizers_defaults_lbfgs_line_search_fn_enum_descriptor": {
      "strong_wolfe": "Strong Wolfe conditions"
    },
    "schedulers_description": "Learning rate scheduler configurations",
    "schedulers_defaults_description": "Default scheduler parameters",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Lambda function for LR calculation",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Lambda function for multiplicative LR",
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Minimize metric",
      "max": "Maximize metric"
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Relative threshold",
      "abs": "Absolute threshold"
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Triangular - Linear oscillation",
      "triangular2": "Triangular2 - Halving amplitude",
      "exp_range": "Exp Range - Exponential decay"
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Per cycle",
      "iterations": "Per iteration"
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Cosine annealing",
      "linear": "Linear annealing"
    },
    "losses_description": "Loss function configurations",
    "losses_defaults_description": "Default loss function parameters",
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "L1 norm",
      "2": "L2 norm"
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "No reduction",
      "mean": "Mean reduction",
      "sum": "Sum reduction",
      "batchmean": "Batch mean reduction"
    },
    "models_description": "Model architecture configurations",
    "models_resnet_description": "ResNet model family",
    "models_resnet_variants_description": "Available ResNet variants",
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW optimizer",
      "adam": "Adam optimizer",
      "sgd": "SGD optimizer"
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR scheduler",
      "cosine_annealing_lr": "Cosine annealing scheduler",
      "reduce_lr_on_plateau": "Reduce on plateau scheduler"
    },
    "models_resnext_description": "ResNeXt model family",
    "models_resnext_variants_description": "Available ResNeXt variants",
    "models_mobilenet_description": "MobileNet model family",
    "models_mobilenet_variants_description": "Available MobileNet variants",
    "models_shufflenet_description": "ShuffleNet model family",
    "models_shufflenet_variants_description": "Available ShuffleNet variants",
    "models_squeezenet_description": "SqueezeNet model family",
    "models_squeezenet_variants_description": "Available SqueezeNet variants",
    "models_efficientnet_description": "EfficientNet model family",
    "models_efficientnet_variants_description": "Available EfficientNet variants",
    "models_supported_types_description": "List of supported model architectures",
    "activations_description": "Activation function configurations",
    "activations_defaults_description": "Default activation parameters",
    "activations_properties_description": "Activation function properties",
    "activations_defaults_leaky_relu_description": "Leaky ReLU activation",
    "activations_defaults_leaky_relu_negative_slope_description": "Slope for negative values",
    "activations_defaults_prelu_description": "PReLU activation",
    "activations_defaults_prelu_num_parameters_description": "Number of learnable parameters",
    "activations_defaults_prelu_init_description": "Initial slope value",
    "activations_defaults_elu_description": "ELU activation",
    "activations_defaults_elu_alpha_description": "Alpha value for negative inputs",
    "activations_defaults_celu_description": "CELU activation",
    "activations_defaults_celu_alpha_description": "Alpha value",
    "activations_defaults_hardtanh_description": "Hardtanh activation",
    "activations_defaults_hardtanh_min_val_description": "Minimum output value",
    "activations_defaults_hardtanh_max_val_description": "Maximum output value",
    "activations_defaults_hardshrink_description": "Hard shrink activation",
    "activations_defaults_hardshrink_lambd_description": "Lambda threshold",
    "activations_defaults_softshrink_description": "Soft shrink activation",
    "activations_defaults_softshrink_lambd_description": "Lambda threshold",
    "activations_defaults_threshold_description": "Threshold activation",
    "activations_defaults_threshold_threshold_description": "Threshold value",
    "activations_defaults_threshold_value_description": "Replacement value below threshold",
    "activations_defaults_softplus_description": "Softplus activation",
    "activations_defaults_softplus_beta_description": "Beta scaling factor",
    "activations_defaults_softplus_threshold_description": "Linear transition threshold",
    "augmentations_description": "Data augmentation configurations",
    "augmentations_defaults_description": "Default augmentation parameters",
    "augmentations_properties_description": "Augmentation properties",
    "augmentations_defaults_random_crop_description": "Random crop augmentation",
    "augmentations_defaults_random_crop_size_description": "Output crop size",
    "augmentations_defaults_random_crop_padding_description": "Padding before crop",
    "augmentations_defaults_random_crop_pad_if_needed_description": "Pad if smaller than crop size",
    "augmentations_defaults_random_crop_fill_description": "Fill value for padding",
    "augmentations_defaults_random_crop_padding_mode_description": "Padding mode",
    "augmentations_defaults_random_resized_crop_description": "Random resized crop augmentation",
    "augmentations_defaults_random_resized_crop_size_description": "Output size",
    "augmentations_defaults_random_resized_crop_scale_description": "Scale range (min, max)",
    "augmentations_defaults_random_resized_crop_ratio_description": "Aspect ratio range",
    "augmentations_defaults_random_resized_crop_interpolation_description": "Interpolation method",
    "augmentations_defaults_center_crop_description": "Center crop augmentation",
    "augmentations_defaults_center_crop_size_description": "Output crop size",
    "augmentations_defaults_random_horizontal_flip_description": "Random horizontal flip",
    "augmentations_defaults_random_horizontal_flip_p_description": "Flip probability",
    "augmentations_defaults_random_vertical_flip_description": "Random vertical flip",
    "augmentations_defaults_random_vertical_flip_p_description": "Flip probability",
    "augmentations_defaults_random_rotation_description": "Random rotation augmentation",
    "augmentations_defaults_random_rotation_degrees_description": "Rotation range in degrees",
    "augmentations_defaults_random_rotation_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_rotation_expand_description": "Expand to fit rotated image",
    "augmentations_defaults_random_rotation_fill_description": "Fill value for empty areas",
    "augmentations_defaults_color_jitter_description": "Color jitter augmentation",
    "augmentations_defaults_color_jitter_brightness_description": "Brightness variation",
    "augmentations_defaults_color_jitter_contrast_description": "Contrast variation",
    "augmentations_defaults_color_jitter_saturation_description": "Saturation variation",
    "augmentations_defaults_color_jitter_hue_description": "Hue variation",
    "augmentations_defaults_random_grayscale_description": "Random grayscale conversion",
    "augmentations_defaults_random_grayscale_p_description": "Conversion probability",
    "augmentations_defaults_random_erasing_description": "Random erasing augmentation",
    "augmentations_defaults_random_erasing_p_description": "Erasing probability",
    "augmentations_defaults_random_erasing_scale_description": "Erased area scale range",
    "augmentations_defaults_random_erasing_ratio_description": "Erased area aspect ratio",
    "augmentations_defaults_random_erasing_value_description": "Fill value for erased area",
    "augmentations_defaults_random_erasing_inplace_description": "Erase in-place",
    "augmentations_defaults_normalize_description": "Normalize augmentation",
    "augmentations_defaults_normalize_mean_description": "Mean values (RGB)",
    "augmentations_defaults_normalize_std_description": "Standard deviation (RGB)",
    "augmentations_defaults_random_invert_description": "Random color inversion",
    "augmentations_defaults_random_invert_p_description": "Inversion probability",
    "augmentations_defaults_random_posterize_description": "Random posterize",
    "augmentations_defaults_random_posterize_bits_description": "Bits per channel",
    "augmentations_defaults_random_posterize_p_description": "Posterize probability",
    "augmentations_defaults_random_solarize_description": "Random solarize",
    "augmentations_defaults_random_solarize_threshold_description": "Solarize threshold",
    "augmentations_defaults_random_solarize_p_description": "Solarize probability",
    "augmentations_defaults_random_adjust_sharpness_description": "Random sharpness adjustment",
    "augmentations_defaults_random_adjust_sharpness_sharpness_factor_description": "Sharpness factor",
    "augmentations_defaults_random_adjust_sharpness_p_description": "Adjustment probability",
    "augmentations_defaults_random_autocontrast_description": "Random auto contrast",
    "augmentations_defaults_random_autocontrast_p_description": "Auto contrast probability",
    "augmentations_defaults_random_equalize_description": "Random histogram equalization",
    "augmentations_defaults_random_equalize_p_description": "Equalization probability",
    "augmentations_defaults_random_perspective_description": "Random perspective transform",
    "augmentations_defaults_random_perspective_distortion_scale_description": "Distortion scale",
    "augmentations_defaults_random_perspective_p_description": "Transform probability",
    "augmentations_defaults_random_perspective_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_perspective_fill_description": "Fill value for empty areas",
    "regularization_description": "Regularization configurations",
    "regularization_defaults_description": "Default regularization parameters",
    "regularization_properties_description": "Regularization properties",
    "normalization_description": "Normalization layer configurations",
    "normalization_defaults_description": "Default normalization parameters",
    "normalization_properties_description": "Normalization properties",
    "pooling_description": "Pooling layer configurations",
    "pooling_defaults_description": "Default pooling parameters",
    "pooling_properties_description": "Pooling properties"
  },
  "presets": {
    "binary_multiclass_classification": {
      "name": "Binary/Multi-class Classification",
      "description": "Standard single-label classification where each image belongs to exactly ONE class. Perfect for building models that categorize images into distinct, mutually exclusive categories. USE CASES: Cat vs dog detection, product category identification (electronics/clothing/food), digit recognition (0-9), sentiment classification from facial expressions, defective vs non-defective parts, spam vs legitimate images, gender classification, yes/no decisions, pass/fail quality control. SUPPORTED MODELS: ResNet (reliable baseline), EfficientNet (best accuracy), MobileNet/ShuffleNet (fast inference), SqueezeNet (minimal size), ResNeXt (complex patterns). OUTPUT: Single class label with softmax confidence score per image. The model learns decision boundaries between categories and outputs the most likely class."
    },
    "confidence_based_classification": {
      "name": "Confidence-based Classification",
      "description": "Single-label classification optimized for well-calibrated confidence scores. The confidence percentage should accurately reflect prediction certainty - 90% confident should be correct ~90% of the time. USE CASES: Medical diagnosis (tumor/benign with reliable confidence), quality control decisions (pass/fail with certainty levels), authentication systems (genuine/fake), safety-critical applications, autonomous vehicle decisions, financial fraud detection, insurance claim assessment, legal document verification. SUPPORTED MODELS: EfficientNet (best calibration), ResNet/ResNeXt (reliable), avoid lightweight models for critical decisions. OUTPUT: Class prediction with calibrated confidence that accurately reflects true probability of correctness. Essential when confidence thresholds drive real-world decisions."
    },
    "content_moderation_safety": {
      "name": "Content Moderation / Safety",
      "description": "Detect inappropriate, unsafe, or policy-violating content in images. Multi-label approach detects multiple violation types simultaneously. Critical for platforms hosting user-generated content. USE CASES: NSFW/adult content detection, violence/gore filtering, hate symbol recognition, spam image detection, child safety systems, brand safety for advertising, platform policy enforcement, workplace-appropriate filtering, age-restricted content flagging. SUPPORTED MODELS: ResNet/ResNeXt (reliable detection), EfficientNet (high accuracy), MobileNet for real-time scanning. Train on diverse violation examples. OUTPUT: Safety flags with confidence scores, e.g., {adult: 0.95, violence: 0.12, spam: 0.03}. Use conservative thresholds and combine with human review for edge cases. Continuous model updates needed as violators adapt."
    },
    "document_classification": {
      "name": "Document / Text Image Classification",
      "description": "Classify document images, forms, and text-containing visual content. Optimized for high-frequency details and structured layouts typical in documents. USE CASES: Document type classification (invoice, receipt, ID card, passport, form, certificate), handwriting vs printed text detection, language/script identification, document orientation detection, form field detection, check processing, mail sorting, legal document categorization, medical record classification. SUPPORTED MODELS: ResNet (good for structured content), EfficientNet (captures fine text details), MobileNet for high-volume processing. Standard 224px input usually sufficient. OUTPUT: Document category enabling automated routing, archival, and processing pipelines. Combine with OCR for full document understanding."
    },
    "efficientnet_balanced_scaling": {
      "name": "EfficientNet Family - Balanced Scaling",
      "description": "Optimized for EfficientNet architecture (B0-B7, V2-S/M/L) using compound scaling that uniformly scales depth, width, and resolution with fixed ratios. EfficientNets achieve state-of-the-art accuracy while being 8-10x more efficient than previous architectures. Best for: maximum accuracy tasks, medical image analysis (X-ray, MRI, pathology), fine-grained recognition (bird species, car models, plant diseases), scientific imaging, satellite/aerial imagery, quality inspection with subtle defects, art authentication, forgery detection, dermoscopy, retinal imaging, food quality assessment. EfficientNet-B0 matches ResNet50 accuracy with 5x fewer parameters. B4-B7 for when accuracy is critical regardless of compute. EfficientNetV2 variants offer faster training with progressive learning. The gold standard for accuracy-critical applications."
    },
    "fast_mobile_inference": {
      "name": "Fast Mobile Inference",
      "description": "Lightweight models optimized for maximum inference speed and minimal resource usage. Trades some accuracy for 3-10x faster predictions and smaller model files. USE CASES: Mobile apps (iOS/Android), embedded systems (Raspberry Pi, Jetson Nano), real-time video processing, edge AI devices, IoT sensors, smart cameras, drone vision, AR/VR applications, browser-based inference, batch processing millions of images, latency-critical applications (<10ms response). SUPPORTED MODELS: MobileNetV3 (best speed/accuracy), ShuffleNet (very fast), SqueezeNet (smallest file size ~5MB), avoid ResNet/EfficientNet for speed-critical deployment. OUTPUT: Same classification results but optimized for deployment - smaller model files, faster inference, lower memory usage. Ideal for production systems with hardware constraints."
    },
    "feature_extraction_embedding": {
      "name": "Feature Extraction / Embedding Learning",
      "description": "Train models to produce meaningful feature vectors (embeddings) where similar images map to nearby points in vector space. The learned representations enable similarity-based applications without explicit classification. USE CASES: Visual search engines, reverse image lookup, duplicate/near-duplicate detection, image clustering, content-based recommendation, face recognition systems, product matching, trademark similarity search, plagiarism detection, visual analogies. SUPPORTED MODELS: ResNet50 (industry standard embeddings), EfficientNet (compact effective embeddings), ResNeXt (rich features). Extract features from penultimate layer. OUTPUT: Fixed-size vector (512-2048 dimensions) per image. Compare using cosine similarity or Euclidean distance. Can be indexed for fast similarity search (FAISS, Annoy)."
    },
    "finegrained_recognition": {
      "name": "Fine-grained Visual Recognition",
      "description": "Distinguish between highly similar subcategories requiring attention to subtle visual differences. Uses larger input resolution and deeper networks to capture fine details. USE CASES: Bird species identification (200+ species), car make/model/year recognition, plant/flower species classification, dog/cat breed identification, aircraft type recognition, mushroom species (safety-critical), fish species for fishing apps, insect identification, fashion brand recognition, font identification. SUPPORTED MODELS: EfficientNet-B4+ (best for fine details), ResNeXt (captures subtle patterns), ResNet101/152 (proven performer). Larger input sizes (380-528px) recommended. Avoid lightweight models for fine-grained tasks. OUTPUT: Specific subcategory with confidence, e.g., Labrador Retriever: 94% not just dog. Requires high-quality training images with expert labels."
    },
    "highres_detail_preservation": {
      "name": "High Resolution / Detail Preservation",
      "description": "Training with larger input images (380-528px+) to preserve fine visual details critical for accurate classification. Requires more GPU memory but captures information lost at standard 224px resolution. USE CASES: Texture/fabric analysis, manufacturing defect detection (scratches, cracks), satellite/aerial imagery analysis, artwork authentication, surface inspection, circuit board inspection, gemstone grading, print quality assessment, forensic image analysis. SUPPORTED MODELS: EfficientNet-B4/B5/B6 (designed for higher resolutions), ResNeXt (detail-oriented). Avoid lightweight models - they lose detail benefits. Requires 8GB+ VRAM. OUTPUT: Classifications based on fine-grained details invisible at standard resolution. Essential when subtle visual differences determine the correct class."
    },
    "image_quality_assessment": {
      "name": "Image Quality Assessment",
      "description": "Evaluate technical and perceptual quality of images. Detects blur, noise, exposure issues, and assesses overall aesthetic appeal. USE CASES: Photo gallery quality scoring, automated photo culling for events, blur/motion detection, exposure assessment, print-worthiness evaluation, social media post optimization, camera/lens testing, image selection for publications, photography contest screening, real estate photo quality control. SUPPORTED MODELS: EfficientNet (best quality assessment), ResNet (technical quality), MobileNet for real-time quality filtering during capture. Medium resolution input sufficient. OUTPUT: Quality classification (excellent/good/fair/poor) or numeric score. Can assess technical merit (sharpness, noise, exposure) and/or aesthetic appeal separately."
    },
    "medical_scientific_analysis": {
      "name": "Medical / Scientific Image Analysis",
      "description": "High-precision classification for medical, scientific, and diagnostic imaging where accuracy and reliability are paramount. Conservative training settings with extensive validation. USE CASES: X-ray analysis (pneumonia, fractures, tuberculosis), CT/MRI scan classification, histopathology slide screening, retinal disease detection, skin lesion classification, cell type identification, microscopy analysis, satellite imagery classification, materials science imaging, pharmaceutical quality control. SUPPORTED MODELS: EfficientNet (best accuracy, well-calibrated), ResNeXt (complex pattern recognition), ResNet (proven medical imaging backbone). Always validate with domain experts. OUTPUT: Diagnostic classification with high reliability - intended as decision SUPPORT, not replacement for professional judgment. Requires extensive validation before clinical use."
    },
    "mobilenet_edge_deployment": {
      "name": "MobileNet Family - Edge Deployment",
      "description": "Optimized for MobileNet V2/V3 architectures using depthwise separable convolutions and inverted residuals for maximum efficiency. MobileNetV3 incorporates neural architecture search (NAS) and squeeze-and-excitation blocks. Best for: mobile app deployment, real-time inference on phones/tablets, edge AI devices, Raspberry Pi/Jetson deployment, smart cameras, surveillance systems, retail analytics, autonomous robots, drone vision, AR/VR applications, wearable devices, smart home devices, industrial IoT. MobileNetV3-Small for ultra-fast inference (<5ms), V3-Large for better accuracy while still being mobile-friendly. Achieves 3-5x faster inference than ResNet50 with ~75-80% of its accuracy. Perfect when latency matters more than peak accuracy."
    },
    "multi_object_detection": {
      "name": "Scene Classification / Object Presence",
      "description": "Scene-level classification and object presence detection without localization (no bounding boxes). Determines what objects/concepts are present in an image or classifies the overall scene type. USE CASES: Scene recognition (beach, mountain, city, indoor, outdoor), room type classification (kitchen, bedroom, office), general object presence (contains_car, has_person, shows_food), environment classification, context understanding for other AI systems, image organization by scene. SUPPORTED MODELS: ResNet (proven scene classifier), EfficientNet (good scene features), all models suitable. Unlike YOLO/RCNN, this is image-level not instance-level. OUTPUT: Scene category OR multi-label presence indicators. Simpler and faster than full object detection when you only need to know WHAT is present, not WHERE."
    },
    "multi_object_multi_attribute": {
      "name": "Complex Multi-attribute Analysis",
      "description": "Comprehensive multi-attribute analysis extracting many simultaneous properties from each image. Learns complex relationships between attributes. USE CASES: E-commerce product cataloging (size, color, material, style, brand, condition), person/character analysis (age, gender, clothing, accessories, pose, emotion), vehicle inspection (make, model, year, color, damage, modifications), real estate listing analysis (room type, features, condition, style, lighting), fashion analysis. SUPPORTED MODELS: ResNeXt (best for attribute relationships), EfficientNet (many attributes efficiently), ResNet (reliable multi-output). Deeper models handle more attributes. OUTPUT: Rich attribute dictionary with confidence scores across all defined characteristics. Single model predicts 10-50+ attributes simultaneously."
    },
    "multilabel_boolean": {
      "name": "Multi-label Classification (Boolean Tags)",
      "description": "Assign multiple binary (yes/no) tags to each image. Each attribute is an independent decision with 0.5 threshold. Simpler than probability-based when you just need presence/absence decisions. USE CASES: Image tagging systems (has_people, outdoor, night_scene), content moderation flags (violence, nudity, spam), product attributes (is_red, is_leather, has_logo), document properties (is_signed, has_stamp, is_color), medical image flags (has_tumor, abnormal, requires_review). SUPPORTED MODELS: All 6 architectures work well - choose based on deployment needs. ResNet/EfficientNet for accuracy, MobileNet/ShuffleNet/SqueezeNet for speed/size. OUTPUT: Boolean tags per image, e.g., {has_people: true, outdoor: true, night: false}. Clean yes/no decisions for each defined attribute."
    },
    "multilabel_probability": {
      "name": "Multi-label Classification (Probability Scores)",
      "description": "Assign multiple tags with probability scores (0.0-1.0) to each image. Each tag is independent - an image can have any combination of tags with varying confidence levels. USE CASES: Content recommendation systems needing relevance scores, aesthetic quality assessment across multiple dimensions, emotion detection (happy: 0.85, surprised: 0.32), social media hashtag suggestion with relevance ranking, music mood tagging, movie genre prediction, e-commerce product attribute scoring, news article image categorization. SUPPORTED MODELS: ResNet/ResNeXt (accurate probabilities), EfficientNet (best calibration), MobileNet/ShuffleNet (real-time tagging), SqueezeNet (edge deployment). OUTPUT: Dictionary of tags with probability scores, e.g., {outdoor: 0.94, sunny: 0.78, people: 0.45}. Allows custom threshold tuning per application."
    },
    "resnet_deep_learning": {
      "name": "ResNet Family - Deep Feature Learning",
      "description": "Optimized for the ResNet architecture family (ResNet18/34/50/101/152) which revolutionized deep learning with residual skip connections. ResNets enable training of very deep networks (up to 152 layers) without degradation. Best for: general image classification, transfer learning foundation, object recognition, scene classification, facial recognition, vehicle identification, plant/animal species classification, product categorization, satellite imagery analysis, X-ray/CT scan classification, quality control inspection. ResNets are the industry standard backbone due to excellent balance of accuracy, speed, and reliability. ResNet50 is often the best starting point. Smaller variants (18/34) for speed-critical applications, deeper (101/152) when accuracy is paramount. Extensively tested and well-documented architecture with abundant pretrained weights."
    },
    "resnext_aggregated_networks": {
      "name": "ResNeXt Family - Aggregated Residual Networks",
      "description": "Optimized for ResNeXt architecture (ResNeXt50/101 with 32x4d, 32x8d, 64x4d cardinalities) which extends ResNet with split-transform-merge paradigm using grouped convolutions. ResNeXt introduces cardinality as a new scaling dimension beyond depth and width. Best for: complex multi-class classification, fine-grained visual recognition, multi-attribute detection, fashion/clothing classification, vehicle make/model recognition, food recognition, artwork style classification, texture analysis, material classification, detailed product categorization. Achieves 1-2% higher accuracy than equivalent ResNets with similar compute. The 32x4d variant offers best accuracy/speed ratio, 32x8d and 64x4d for maximum accuracy. Ideal when ResNet accuracy plateaus and you need that extra performance boost without architectural changes."
    },
    "shufflenet_lightweight_speed": {
      "name": "ShuffleNet - Lightweight Speed Champion",
      "description": "Optimized for ShuffleNet V2 architecture which uses channel shuffle operations for efficient information exchange between channel groups. ShuffleNet achieves excellent speed-accuracy tradeoff through pointwise group convolutions and channel shuffle. Best for: real-time mobile applications, embedded systems with strict latency requirements, edge AI devices, IoT sensors, robotics vision, drone-based recognition, wearable devices, smart cameras, traffic monitoring, retail analytics, industrial inspection on resource-constrained hardware. ShuffleNet variants range from ultra-light (x0.5) to more capable (x2.0), all maintaining fast inference. Ideal when inference speed is critical and model size must be minimal. Not recommended for tasks requiring maximum accuracy or fine-grained detail recognition.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smallest ShuffleNet variant. High risk of overfitting - early stopping essential.",
        "large": "Large datasets (10k-30k images). Reduced regularization, larger ShuffleNet variants viable. Higher learning rates and bigger batches effective.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Larger ShuffleNet variants train effectively.",
        "massive": "Massive datasets (50k-100k images). Fast learning rates, large batches, minimal regularization. ShuffleNet x1.5 or x2.0 recommended.",
        "giant": "Giant datasets (100k+ images). Maximum ShuffleNet capacity (x2.0). Can train from scratch or fine-tune with aggressive settings."
      }
    },
    "small_dataset_fewshot": {
      "name": "Small Dataset / Few-shot Learning",
      "description": "Optimized for training with very limited data (50-500 images total or per class). Uses aggressive regularization, pretrained weights, and careful learning rate scheduling to prevent overfitting. USE CASES: Prototype/MVP development, rare object classification, custom corporate datasets, niche applications (rare diseases, specialized equipment), personal projects, academic research with limited samples, new product categories before full dataset collection. SUPPORTED MODELS: ResNet18/34 (stable with small data), EfficientNet-B0/B1 (good generalization), MobileNetV3-Small (if speed needed). Avoid very deep models (ResNet152, EfficientNet-B7) which overfit easily. OUTPUT: Reasonable classification despite limited data. Expect 70-85% accuracy - augment data and collect more samples to improve. Transfer learning is essential."
    },
    "squeezenet_ultra_compact": {
      "name": "SqueezeNet - Ultra Compact Networks",
      "description": "Optimized for SqueezeNet architecture which achieves AlexNet-level accuracy with 50x fewer parameters using Fire modules (squeeze and expand layers). SqueezeNet is designed for extreme model compression while maintaining reasonable accuracy. Best for: ultra-low memory devices, microcontrollers, FPGA deployment, model compression research, bandwidth-limited edge devices, embedded systems with <5MB storage, smart sensors, always-on vision applications, battery-powered devices where model size directly impacts power consumption. SqueezeNet 1.1 is 2.4x faster than 1.0 with same accuracy. Ideal when model size is the primary constraint. Not recommended for tasks requiring high accuracy or complex feature hierarchies.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization essential for this already-compact architecture. Early stopping critical to prevent overfitting.",
        "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained Fire modules. SqueezeNet learns efficiently from limited data.",
        "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters work well. SqueezeNet's compact size allows larger batch sizes.",
        "large": "Large datasets (10k-30k images). Reduced regularization. SqueezeNet 1.0 can be used for slightly more capacity.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization. SqueezeNet reaches its capacity limits here - consider larger architectures for better accuracy.",
        "massive": "Massive datasets (50k-100k images). SqueezeNet may underfit - this preset pushes the architecture to its limits. Consider EfficientNet for better results.",
        "giant": "Giant datasets (100k+ images). SqueezeNet is NOT recommended for datasets this large - limited capacity will cause underfitting. Use only if model size is absolutely critical."
      }
    },
    "style_aesthetic_classification": {
      "name": "Style / Aesthetic Classification",
      "description": "Classify images by visual style, artistic qualities, mood, or aesthetic properties. Focuses on global image characteristics (color palette, composition, texture) rather than specific objects. USE CASES: Photo quality scoring (professional vs amateur), art style recognition (impressionist, cubist, modern, abstract), interior design style (minimalist, rustic, industrial, bohemian), fashion style classification, image mood detection (cheerful, melancholic, dramatic), visual content curation, photography style (portrait, landscape, macro, street). SUPPORTED MODELS: EfficientNet (captures global features well), ResNeXt (style patterns), ResNet (reliable baseline). All models can learn style features effectively. OUTPUT: Style category or aesthetic quality score for content filtering, recommendation, curation systems, and creative applications."
    },
    "configs": {
      "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smaller models. High risk of overfitting - early stopping essential.",
      "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained weights. Balance between learning capacity and overfitting prevention.",
      "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters with good augmentation. Balanced training approach works well.",
      "large": "Large datasets (10k-30k images). Reduced regularization, larger models viable. Higher learning rates and bigger batches effective.",
      "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Deep networks train effectively with standard augmentation.",
      "massive": "Massive datasets (50k-100k images). Very deep networks fully supported. Fast learning rates, large batches, minimal regularization.",
      "giant": "Giant datasets (100k+ images). Maximum model capacity utilized. Can train from scratch or fine-tune with aggressive settings."
    }
  }
}