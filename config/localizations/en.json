{
  "language_name": "English",
  "app": {
    "page_not_found": {
      "title": "Page Not Found",
      "description": "The requested page does not exist"
    }
  },
  "updates": {
    "status": {
      "missing": "Missing",
      "outdated": "Outdated",
      "orphaned": "Orphaned"
    }
  },
  "common": {
    "project_label": "Project:",
    "cancel": "Cancel"
  },
  "nav": {
    "projects": "Projects",
    "training_setup": "Training Setup",
    "dataset": "Dataset",
    "performance": "Performance",
    "heatmap": "Heatmap",
    "updates": "Updates"
  },
  "ui": {
    "language_select_title": "Language"
  },
  "context_menu": {
    "view_image": "View Image",
    "select": "Select",
    "deselect": "Deselect",
    "select_all": "Select All",
    "clear_selection": "Clear Selection",
    "copy_filename": "Copy Filename",
    "copy_path": "Copy Path",
    "delete": "Delete",
    "delete_selected": "Delete Selected ({count})",
    "open_folder": "Open in Explorer",
    "new_subfolder": "New Subfolder"
  },
  "training_controller": {
    "stop": "Stop",
    "clear": "Clear",
    "completed": "Completed",
    "stopped": "Stopped",
    "error": "Error",
    "already_running": "Training is already running"
  },
  "projects_page": {
    "title": "Projects",
    "description": "Manage your image recognition projects",
    "empty": {
      "title": "No Projects Found",
      "description": "Create a new project to get started"
    },
    "error": {
      "title": "Error Loading Projects"
    },
    "card": {
      "unknown_type": "Unknown",
      "no_stats": "No statistics available",
      "loading": "Loading",
      "error": "Error"
    },
    "stats": {
      "total_images": "Total Images",
      "balance_score": "Balance Score",
      "balance_status": "Balance Status"
    },
    "buttons": {
      "refresh": "Refresh",
      "load": "Load",
      "loaded": "Loaded",
      "start_training": "Start Training",
      "stop_training": "Stop Training",
      "new_project": "New Project",
      "rename_project": "Rename",
      "delete_project": "Delete"
    },
    "training": {
      "stop_error": "Failed to stop training",
      "start_error": "Failed to start training",
      "no_project": "No project selected"
    },
    "new_project": {
      "title": "Create New Project",
      "prompt": "Enter project name:",
      "error": "Failed to create project"
    },
    "delete_project": {
      "title": "Delete Project",
      "confirm": "Are you sure you want to delete project '{name}'? This action cannot be undone.",
      "no_selection": "No project selected",
      "error": "Failed to delete project"
    },
    "rename_project": {
      "title": "Rename Project",
      "prompt": "Enter new project name:",
      "no_selection": "No project selected",
      "error": "Failed to rename project"
    },
    "dataset_types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    }
  },
  "training_page": {
    "tabs": {
      "presets": "Presets",
      "model": "Model",
      "hyperparameters": "Hyperparameters",
      "dataloader": "Data Loader",
      "augmentation": "Augmentation",
      "optimizer": "Optimizer",
      "scheduler": "Scheduler",
      "loss": "Loss",
      "checkpoint": "Checkpoint",
      "early_stopping": "Early Stopping",
      "gradient": "Gradient",
      "runtime": "Runtime"
    },
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to configure training settings.",
      "button": "Go to Projects"
    },
    "load_defaults_button": "Load Defaults",
    "load_project_button": "Load Project",
    "save_button": "Save",
    "saving": "Saving...",
    "saved": "Saved",
    "save_error": "Save Error",
    "defaults_loaded": "Defaults Loaded",
    "project_loaded": "Project Loaded",
    "load_error": "Load Error",
    "no_settings": "No settings available",
    "augmentation": {
      "train_title": "Training Augmentations",
      "train_description": "Augmentations applied during training to improve model generalization",
      "val_title": "Validation Augmentations",
      "val_description": "Minimal augmentations for validation data (typically just resize and normalize)",
      "preview_placeholder": "Drop an image or click Random to preview augmentations",
      "btn_random": "Random",
      "btn_train": "Train",
      "btn_val": "Val",
      "no_params": "No parameters"
    },
    "presets": {
      "title": "Training Presets",
      "description": "Quick configuration templates for common training scenarios",
      "search_placeholder": "Search presets by name, description, task...",
      "only_compatible": "Only Compatible",
      "search_results": "presets found",
      "loading": "Loading presets...",
      "no_presets": "No presets available",
      "apply_error": "Failed to apply preset",
      "applied": "Preset applied successfully",
      "incompatible_dataset": "Incompatible with current dataset",
      "dataset_size": "images",
      "recommended": "Recommended",
      "apply_button": "Apply"
    }
  },
  "dataset_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to view the dataset.",
      "button": "Go to Projects"
    },
    "type_label": "Dataset Type",
    "types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    },
    "sync_button": "Sync Dataset",
    "build_button": "Build Dataset",
    "progress_discovery": "Discovering images",
    "progress_starting": "Starting...",
    "progress_complete": "Complete",
    "new_folder": "New Folder",
    "rename_folder": "Rename Folder",
    "delete_folder": "Delete Folder",
    "per_page": "Per page",
    "search_filename": "Filename",
    "search_annotation": "Annotation",
    "search_placeholder": "Search images...",
    "show_duplicates": "Show Duplicates",
    "duplicates_none": "No duplicate images found",
    "duplicates_error": "Failed to check for duplicates",
    "folders": "Folders",
    "drop_images": "Drop images here to upload",
    "root_folder": "Root",
    "delete_image": "Delete Image",
    "duplicate_count": "{count} duplicates",
    "tag_placeholder": "Add tags...",
    "annotation_placeholder": "Add annotation...",
    "confirm_delete": "Are you sure you want to delete this image?",
    "uploading": "Uploading...",
    "upload_error": "Upload failed",
    "enter_folder_name": "Enter folder name:",
    "folder_create_error": "Failed to create folder",
    "enter_new_name": "Enter new name:",
    "folder_rename_error": "Failed to rename folder",
    "delete_folder_confirm_with_images": "This folder contains {count} images. Are you sure you want to delete it?",
    "delete_folder_confirm": "Are you sure you want to delete this folder?",
    "folder_delete_error": "Failed to delete folder",
    "build_error": "Build failed",
    "status_count": "{count} images",
    "clear_selection": "Clear Selection",
    "select_all": "Select All",
    "bulk_add_tags_placeholder": "Tags to add...",
    "bulk_add": "Add",
    "bulk_remove_tags_placeholder": "Tags to remove...",
    "bulk_remove": "Remove",
    "bulk_delete": "Delete Selected",
    "selected_count": "{count} selected",
    "bulk_error": "Bulk operation failed",
    "bulk_delete_confirm": "Are you sure you want to delete {count} images?"
  },
  "performance_page": {
    "tabs": {
      "training": "Training",
      "system": "System"
    },
    "training": {
      "title": "Training Progress",
      "no_active": "No active training session",
      "no_data": "No Training Data",
      "description": "Start training to see progress graphs",
      "running": "Training in progress",
      "completed": "Training completed",
      "stopped": "Training stopped",
      "train": "Train",
      "validation": "Validation",
      "train_step_loss": "Training Step Loss",
      "val_step_loss": "Validation Step Loss",
      "step_accuracy": "Step Accuracy",
      "learning_rate": "Learning Rate",
      "epoch_loss": "Epoch Loss",
      "epoch_accuracy": "Epoch Accuracy",
      "current": "Current",
      "step": "Step",
      "train_step_accuracy": "Train Step Accuracy",
      "val_step_accuracy": "Val Step Accuracy",
      "current_lr": "Current LR",
      "train_loss": "Train Loss",
      "val_loss": "Val Loss",
      "train_accuracy": "Train Accuracy",
      "val_accuracy": "Val Accuracy"
    },
    "system": {
      "title": "System Monitor",
      "monitoring_active": "Monitoring active",
      "monitoring_paused": "Monitoring paused",
      "cpu_usage": "CPU Usage",
      "system_memory": "System Memory",
      "gpu_usage": "GPU {index} Usage",
      "gpu_memory": "GPU {index} Memory",
      "speed": "Speed",
      "cores_threads": "Cores/Threads",
      "temperature": "Temperature",
      "max_memory": "Max Memory",
      "available": "Available",
      "power": "Power",
      "gpu_clock": "GPU Clock",
      "mem_clock": "Memory Clock",
      "fan": "Fan",
      "dedicated_memory": "Dedicated Memory",
      "used_memory": "Used Memory"
    }
  },
  "heatmap_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to generate heatmaps.",
      "button": "Go to Projects"
    },
    "drop_zone": {
      "text": "Drop image here",
      "hint": "or click to select"
    },
    "alpha_label": "Overlay Alpha",
    "results": {
      "title": "Results",
      "placeholder": "Drop an image to see predictions and heatmap",
      "no_predictions": "No predictions available"
    },
    "live_model_switch": "Use live model",
    "random_button": "Random",
    "refresh_button": "Refresh",
    "checkpoint_label": "Checkpoint:"
  },
  "updates_ui": {
    "page_title": "Updates",
    "page_description": "Check for and apply system updates",
    "intro": "Check for updates to keep your system current with the latest features and fixes.",
    "check_button": "Check for Updates",
    "apply_button": "Apply Updates",
    "status_idle": "Ready to check for updates",
    "status_checking": "Checking for updates...",
    "status_ready": "Updates available",
    "status_up_to_date": "System is up to date",
    "status_applying": "Applying updates...",
    "status_applied": "Updates applied successfully",
    "status_failed": "Update check failed",
    "no_updates": "No updates available",
    "files_to_update": "files to update",
    "orphaned_count": "orphaned files",
    "table_header_file": "File",
    "table_header_status": "Status"
  },
  "schema": {
    "description": "Configuration schema for Hootsight image recognition system",
    "ui_group": {
      "general": "General",
      "api": "API Settings",
      "ui_settings": "User Interface",
      "paths": "Paths",
      "system": "System",
      "memory": "Memory Management",
      "dataset": "Dataset",
      "dataset_editor": "Dataset Editor",
      "training_model": "Model",
      "training_hyperparameters": "Hyperparameters",
      "training_dataloader": "Data Loader",
      "training_augmentation": "Augmentation",
      "training_optimizer": "Optimizer",
      "training_scheduler": "Scheduler",
      "training_loss": "Loss Function",
      "training_checkpoint": "Checkpointing",
      "training_early_stopping": "Early Stopping",
      "training_gradient": "Gradient",
      "training_runtime": "Runtime",
      "activations_defaults": "Activation Defaults",
      "augmentations_defaults": "Augmentation Defaults",
      "optimizers": "Optimizers",
      "schedulers": "Schedulers",
      "losses": "Losses",
      "activations": "Activations",
      "augmentations": "Augmentations",
      "regularization": "Regularization",
      "normalization": "Normalization",
      "pooling": "Pooling",
      "models": "Models"
    },
    "general_description": "General application settings",
    "general_language_description": "User interface language",
    "general_language_enum_descriptor": {
      "en": "English"
    },
    "api_description": "API server configuration",
    "api_host_description": "API server host address",
    "api_port_description": "API server port number",
    "ui_description": "User interface settings",
    "ui_title_description": "Application window title",
    "ui_width_description": "Window width in pixels",
    "ui_height_description": "Window height in pixels",
    "ui_resizable_description": "Allow window resizing",
    "paths_description": "File and directory paths",
    "paths_projects_dir_description": "Directory containing projects",
    "paths_ui_dir_description": "Directory containing UI files",
    "paths_config_dir_description": "Directory containing configuration files",
    "paths_localizations_dir_description": "Directory containing localization files",
    "paths_packages_file_description": "Path to packages configuration file",
    "paths_cache_dir_description": "Directory for cached files",
    "paths_docs_dir_description": "Directory for documentation",
    "system_description": "System configuration settings",
    "system_max_threads_description": "Maximum number of worker threads",
    "system_max_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "system_fallback_batch_size_description": "Fallback batch size when auto-detection fails",
    "system_startup_wait_seconds_description": "Seconds to wait before starting UI",
    "system_http_timeout_description": "HTTP request timeout in seconds",
    "system_update_repository_url_description": "URL for update repository",
    "system_update_skip_paths_description": "Paths to skip during updates",
    "dataset_editor_description": "Dataset editor configuration",
    "dataset_editor_page_sizes_description": "Available page size options",
    "dataset_editor_default_page_size_description": "Default images per page",
    "dataset_editor_size_presets_description": "Image size presets by model type",
    "dataset_editor_default_size_description": "Default image size",
    "dataset_editor_build_workers_description": "Number of workers for dataset building",
    "dataset_editor_build_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_discovery_workers_description": "Number of workers for image discovery",
    "dataset_editor_discovery_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_project_name_validation_description": "Project name validation rules",
    "dataset_editor_project_name_validation_min_length_description": "Minimum project name length",
    "dataset_editor_project_name_validation_max_length_description": "Maximum project name length",
    "dataset_editor_project_name_validation_pattern_description": "Allowed project name pattern (regex)",
    "memory_description": "Memory management configuration",
    "memory_target_memory_usage_description": "Target GPU memory usage ratio",
    "memory_safety_margin_description": "Safety margin for memory allocation",
    "memory_augmentation_threads_description": "Threads for augmentation processing",
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "memory_reserved_memory_ratio_description": "Reserved memory ratio",
    "memory_per_sample_multiplier_description": "Memory multiplier per sample",
    "memory_min_batch_size_description": "Minimum batch size",
    "memory_max_batch_size_description": "Maximum batch size",
    "memory_thresholds_description": "Memory usage thresholds",
    "memory_thresholds_high_usage_description": "High memory usage threshold",
    "memory_thresholds_moderate_usage_description": "Moderate memory usage threshold",
    "memory_thresholds_low_usage_description": "Low memory usage threshold",
    "memory_batch_size_limits_description": "Batch size warning limits",
    "memory_batch_size_limits_small_description": "Small batch size warning threshold",
    "memory_batch_size_limits_large_description": "Large batch size warning threshold",
    "training_description": "Training configuration",
    "training_model_type_title": "Model Type",
    "training_model_type_description": "Model architecture family",
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet - Deep residual networks",
      "resnext": "ResNeXt - Aggregated residual transformations",
      "mobilenet": "MobileNet - Efficient mobile networks",
      "shufflenet": "ShuffleNet - Lightweight shuffle networks",
      "squeezenet": "SqueezeNet - Ultra-compact networks",
      "efficientnet": "EfficientNet - Compound scaled networks"
    },
    "training_model_name_title": "Model Name",
    "training_model_name_description": "Specific model variant",
    "training_pretrained_title": "Pretrained",
    "training_pretrained_description": "Use pretrained weights from ImageNet",
    "training_task_title": "Task",
    "training_task_description": "Training task type",
    "training_task_enum_descriptor": {
      "classification": "Single-label classification",
      "multi_label": "Multi-label classification",
      "detection": "Object detection",
      "segmentation": "Semantic segmentation"
    },
    "training_batch_size_title": "Batch Size",
    "training_batch_size_description": "Training batch size",
    "training_epochs_title": "Epochs",
    "training_epochs_description": "Number of training epochs",
    "training_epochs_enum_descriptor": {
      "auto": "Automatic (based on dataset size)"
    },
    "training_learning_rate_title": "Learning Rate",
    "training_learning_rate_description": "Initial learning rate",
    "training_weight_decay_title": "Weight Decay",
    "training_weight_decay_description": "Weight decay (L2 regularization)",
    "training_input_size_title": "Input Size",
    "training_input_size_description": "Input image size",
    "training_normalize_title": "Normalize",
    "training_normalize_description": "Normalization parameters",
    "training_normalize_mean_title": "Mean",
    "training_normalize_mean_description": "Mean values for each channel (RGB)",
    "training_normalize_std_title": "Std",
    "training_normalize_std_description": "Standard deviation for each channel (RGB)",
    "training_val_ratio_title": "Validation Ratio",
    "training_val_ratio_description": "Validation split ratio",
    "training_dataloader_title": "Dataloader",
    "training_dataloader_description": "Data loader configuration",
    "training_dataloader_num_workers_title": "Num Workers",
    "training_dataloader_num_workers_description": "Number of data loading workers",
    "training_dataloader_pin_memory_title": "Pin Memory",
    "training_dataloader_pin_memory_description": "Pin memory for faster GPU transfer",
    "training_dataloader_persistent_workers_title": "Persistent Workers",
    "training_dataloader_persistent_workers_description": "Keep workers alive between epochs",
    "training_dataloader_prefetch_factor_title": "Prefetch Factor",
    "training_dataloader_prefetch_factor_description": "Number of batches to prefetch per worker",
    "training_augmentation_title": "Augmentation",
    "training_augmentation_description": "Data augmentation configuration",
    "training_augmentation_train_title": "Training Augmentations",
    "training_augmentation_train_description": "Augmentations for training data",
    "training_augmentation_val_title": "Validation Augmentations",
    "training_augmentation_val_description": "Augmentations for validation data",
    "training_optimizer_type_title": "Optimizer Type",
    "training_optimizer_type_description": "Optimizer algorithm",
    "training_optimizer_type_enum_descriptor": {
      "sgd": "SGD - Stochastic Gradient Descent",
      "adam": "Adam - Adaptive Moment Estimation",
      "adamw": "AdamW - Adam with decoupled weight decay",
      "adamax": "Adamax - Adam with infinity norm",
      "nadam": "NAdam - Adam with Nesterov momentum",
      "radam": "RAdam - Rectified Adam",
      "rmsprop": "RMSprop - Root Mean Square Propagation",
      "rprop": "Rprop - Resilient Propagation",
      "adagrad": "Adagrad - Adaptive Gradient",
      "adadelta": "Adadelta - Adaptive Delta",
      "sparse_adam": "Sparse Adam - Adam for sparse tensors",
      "lbfgs": "L-BFGS - Limited-memory BFGS",
      "asgd": "ASGD - Averaged SGD"
    },
    "training_optimizer_lr_title": "Optimizer LR",
    "training_optimizer_lr_description": "Optimizer learning rate",
    "training_optimizer_weight_decay_title": "Optimizer Weight Decay",
    "training_optimizer_weight_decay_description": "Optimizer weight decay",
    "training_optimizer_params_title": "Optimizer Parameters",
    "training_optimizer_params_description": "Optimizer-specific parameters",
    "training_scheduler_type_title": "Scheduler Type",
    "training_scheduler_type_description": "Learning rate scheduler",
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR - Decay by factor every N epochs",
      "multi_step_lr": "Multi-Step LR - Decay at specific epochs",
      "exponential_lr": "Exponential LR - Exponential decay",
      "cosine_annealing_lr": "Cosine Annealing - Cosine decay",
      "cosine_annealing_warm_restarts": "Cosine with Warm Restarts",
      "reduce_lr_on_plateau": "Reduce on Plateau - Decay when metric plateaus",
      "cyclic_lr": "Cyclic LR - Oscillating learning rate",
      "one_cycle_lr": "One Cycle - Super-convergence policy",
      "polynomial_lr": "Polynomial LR - Polynomial decay",
      "linear_lr": "Linear LR - Linear warmup/decay",
      "lambda_lr": "Lambda LR - Custom decay function",
      "multiplicative_lr": "Multiplicative LR - Multiplicative decay"
    },
    "training_scheduler_step_size_title": "Step Size",
    "training_scheduler_step_size_description": "Scheduler step size (epochs)",
    "training_scheduler_gamma_title": "Gamma",
    "training_scheduler_gamma_description": "Learning rate decay factor",
    "training_scheduler_params_title": "Scheduler Parameters",
    "training_scheduler_params_description": "Scheduler-specific parameters",
    "training_loss_type_title": "Loss Type",
    "training_loss_type_description": "Loss function",
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Cross Entropy - Standard classification loss",
      "nll_loss": "NLL Loss - Negative log likelihood",
      "bce_loss": "BCE Loss - Binary cross entropy",
      "bce_with_logits": "BCE with Logits - BCE with sigmoid",
      "multi_margin": "Multi-Margin - Hinge-based multi-class",
      "multi_label_margin": "Multi-Label Margin - Multi-label hinge",
      "multi_label_soft_margin": "Multi-Label Soft Margin - Soft multi-label",
      "mse_loss": "MSE Loss - Mean squared error",
      "l1_loss": "L1 Loss - Mean absolute error",
      "smooth_l1": "Smooth L1 - Huber-like loss",
      "huber_loss": "Huber Loss - Robust regression",
      "kl_div": "KL Divergence - Distribution matching",
      "margin_ranking": "Margin Ranking - Pairwise ranking",
      "hinge_embedding": "Hinge Embedding - Embedding loss",
      "triplet_margin": "Triplet Margin - Metric learning",
      "cosine_embedding": "Cosine Embedding - Cosine similarity",
      "ctc_loss": "CTC Loss - Sequence-to-sequence",
      "poisson_nll": "Poisson NLL - Count data loss",
      "gaussian_nll": "Gaussian NLL - Regression with uncertainty"
    },
    "training_loss_reduction_title": "Reduction",
    "training_loss_reduction_description": "Loss reduction method",
    "training_loss_reduction_enum_descriptor": {
      "mean": "Mean - Average over batch",
      "sum": "Sum - Sum over batch",
      "none": "None - No reduction"
    },
    "training_loss_params_title": "Loss Parameters",
    "training_loss_params_description": "Loss function parameters",
    "training_weight_init_title": "Weight Initialization",
    "training_weight_init_description": "Weight initialization settings",
    "training_weight_init_params_title": "Init Parameters",
    "training_weight_init_params_description": "Weight initialization parameters",
    "weight_init_type_enum_descriptor": {
      "kaiming_normal": "Kaiming Normal - He initialization (normal)",
      "kaiming_uniform": "Kaiming Uniform - He initialization (uniform)",
      "xavier_normal": "Xavier Normal - Glorot initialization (normal)",
      "xavier_uniform": "Xavier Uniform - Glorot initialization (uniform)",
      "normal": "Normal - Gaussian initialization",
      "uniform": "Uniform - Uniform distribution",
      "trunc_normal": "Truncated Normal - Bounded Gaussian",
      "orthogonal": "Orthogonal - Orthogonal matrix",
      "sparse": "Sparse - Sparse initialization",
      "constant": "Constant - Fixed value",
      "zeros": "Zeros - All zeros",
      "ones": "Ones - All ones",
      "eye": "Eye - Identity matrix",
      "dirac": "Dirac - Delta function"
    },
    "training_checkpoint_title": "Checkpoint",
    "training_checkpoint_description": "Model checkpointing configuration",
    "training_early_stopping_title": "Early Stopping",
    "training_early_stopping_description": "Early stopping configuration",
    "training_early_stopping_monitor_title": "Monitor",
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Validation Loss",
      "val_accuracy": "Validation Accuracy"
    },
    "training_gradient_title": "Gradient",
    "training_gradient_description": "Gradient configuration",
    "training_runtime_title": "Runtime",
    "training_runtime_description": "Runtime optimization settings",
    "training_runtime_mixed_precision_title": "Mixed Precision",
    "training_runtime_mixed_precision_description": "Enable mixed precision training (FP16)",
    "training_runtime_channels_last_title": "Channels Last",
    "training_runtime_channels_last_description": "Use channels-last memory format",
    "training_runtime_allow_tf32_title": "Allow TF32",
    "training_runtime_allow_tf32_description": "Allow TF32 on Ampere GPUs",
    "training_runtime_cudnn_benchmark_title": "cuDNN Benchmark",
    "training_runtime_cudnn_benchmark_description": "Enable cuDNN auto-tuning",
    "dataset_description": "Dataset configuration",
    "dataset_image_extensions_description": "Supported image file extensions",
    "dataset_discovery_description": "Dataset discovery settings",
    "dataset_discovery_annotation_formats_description": "Annotation format detection",
    "dataset_discovery_txt_annotations_description": "Text annotation detection",
    "dataset_discovery_txt_annotations_min_coverage_percent_description": "Minimum coverage for text annotations",
    "dataset_discovery_balance_analysis_description": "Dataset balance analysis",
    "dataset_discovery_balance_analysis_min_images_per_class_description": "Minimum images per class",
    "dataset_discovery_balance_analysis_critical_shortage_threshold_description": "Critical shortage threshold",
    "dataset_discovery_balance_analysis_over_representation_ratio_description": "Over-representation ratio",
    "dataset_discovery_balance_analysis_under_representation_ratio_description": "Under-representation ratio",
    "dataset_discovery_balance_analysis_severe_over_representation_ratio_description": "Severe over-representation ratio",
    "dataset_discovery_balance_analysis_hierarchical_balance_threshold_description": "Hierarchical balance threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_description": "Dataset size warning thresholds",
    "dataset_discovery_balance_analysis_dataset_size_warnings_tiny_dataset_description": "Tiny dataset warning threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_small_dataset_description": "Small dataset warning threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_description": "Balance score classification thresholds",
    "dataset_discovery_balance_analysis_balance_score_thresholds_legendary_description": "Legendary balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_excellent_description": "Excellent balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_very_good_description": "Very good balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_good_description": "Good balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_balanced_description": "Balanced threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_slightly_unbalanced_description": "Slightly unbalanced threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_fair_description": "Fair balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_poor_description": "Poor balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_very_poor_description": "Very poor balance threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_critical_description": "Critical balance threshold",
    "optimizers_description": "Optimizer configurations",
    "optimizers_defaults_description": "Default optimizer parameters",
    "optimizers_defaults_lbfgs_line_search_fn_enum_descriptor": {
      "strong_wolfe": "Strong Wolfe conditions"
    },
    "schedulers_description": "Learning rate scheduler configurations",
    "schedulers_defaults_description": "Default scheduler parameters",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Lambda function for LR calculation",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Lambda function for multiplicative LR",
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Minimize metric",
      "max": "Maximize metric"
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Relative threshold",
      "abs": "Absolute threshold"
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Triangular - Linear oscillation",
      "triangular2": "Triangular2 - Halving amplitude",
      "exp_range": "Exp Range - Exponential decay"
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Per cycle",
      "iterations": "Per iteration"
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Cosine annealing",
      "linear": "Linear annealing"
    },
    "losses_description": "Loss function configurations",
    "losses_defaults_description": "Default loss function parameters",
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "L1 norm",
      "2": "L2 norm"
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "No reduction",
      "mean": "Mean reduction",
      "sum": "Sum reduction",
      "batchmean": "Batch mean reduction"
    },
    "models_description": "Model architecture configurations",
    "models_resnet_description": "ResNet model family",
    "models_resnet_variants_description": "Available ResNet variants",
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW optimizer",
      "adam": "Adam optimizer",
      "sgd": "SGD optimizer"
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR scheduler",
      "cosine_annealing_lr": "Cosine annealing scheduler",
      "reduce_lr_on_plateau": "Reduce on plateau scheduler"
    },
    "models_resnext_description": "ResNeXt model family",
    "models_resnext_variants_description": "Available ResNeXt variants",
    "models_mobilenet_description": "MobileNet model family",
    "models_mobilenet_variants_description": "Available MobileNet variants",
    "models_shufflenet_description": "ShuffleNet model family",
    "models_shufflenet_variants_description": "Available ShuffleNet variants",
    "models_squeezenet_description": "SqueezeNet model family",
    "models_squeezenet_variants_description": "Available SqueezeNet variants",
    "models_efficientnet_description": "EfficientNet model family",
    "models_efficientnet_variants_description": "Available EfficientNet variants",
    "models_supported_types_description": "List of supported model architectures",
    "activations_description": "Activation function configurations",
    "activations_defaults_description": "Default activation parameters",
    "activations_properties_description": "Activation function properties",
    "activations_defaults_leaky_relu_description": "Leaky ReLU activation",
    "activations_defaults_leaky_relu_negative_slope_description": "Slope for negative values",
    "activations_defaults_prelu_description": "PReLU activation",
    "activations_defaults_prelu_num_parameters_description": "Number of learnable parameters",
    "activations_defaults_prelu_init_description": "Initial slope value",
    "activations_defaults_elu_description": "ELU activation",
    "activations_defaults_elu_alpha_description": "Alpha value for negative inputs",
    "activations_defaults_celu_description": "CELU activation",
    "activations_defaults_celu_alpha_description": "Alpha value",
    "activations_defaults_hardtanh_description": "Hardtanh activation",
    "activations_defaults_hardtanh_min_val_description": "Minimum output value",
    "activations_defaults_hardtanh_max_val_description": "Maximum output value",
    "activations_defaults_hardshrink_description": "Hard shrink activation",
    "activations_defaults_hardshrink_lambd_description": "Lambda threshold",
    "activations_defaults_softshrink_description": "Soft shrink activation",
    "activations_defaults_softshrink_lambd_description": "Lambda threshold",
    "activations_defaults_threshold_description": "Threshold activation",
    "activations_defaults_threshold_threshold_description": "Threshold value",
    "activations_defaults_threshold_value_description": "Replacement value below threshold",
    "activations_defaults_softplus_description": "Softplus activation",
    "activations_defaults_softplus_beta_description": "Beta scaling factor",
    "activations_defaults_softplus_threshold_description": "Linear transition threshold",
    "augmentations_description": "Data augmentation configurations",
    "augmentations_defaults_description": "Default augmentation parameters",
    "augmentations_properties_description": "Augmentation properties",
    "augmentations_defaults_random_crop_description": "Random crop augmentation",
    "augmentations_defaults_random_crop_size_description": "Output crop size",
    "augmentations_defaults_random_crop_padding_description": "Padding before crop",
    "augmentations_defaults_random_crop_pad_if_needed_description": "Pad if smaller than crop size",
    "augmentations_defaults_random_crop_fill_description": "Fill value for padding",
    "augmentations_defaults_random_crop_padding_mode_description": "Padding mode",
    "augmentations_defaults_random_resized_crop_description": "Random resized crop augmentation",
    "augmentations_defaults_random_resized_crop_size_description": "Output size",
    "augmentations_defaults_random_resized_crop_scale_description": "Scale range (min, max)",
    "augmentations_defaults_random_resized_crop_ratio_description": "Aspect ratio range",
    "augmentations_defaults_random_resized_crop_interpolation_description": "Interpolation method",
    "augmentations_defaults_center_crop_description": "Center crop augmentation",
    "augmentations_defaults_center_crop_size_description": "Output crop size",
    "augmentations_defaults_random_horizontal_flip_description": "Random horizontal flip",
    "augmentations_defaults_random_horizontal_flip_p_description": "Flip probability",
    "augmentations_defaults_random_vertical_flip_description": "Random vertical flip",
    "augmentations_defaults_random_vertical_flip_p_description": "Flip probability",
    "augmentations_defaults_random_rotation_description": "Random rotation augmentation",
    "augmentations_defaults_random_rotation_degrees_description": "Rotation range in degrees",
    "augmentations_defaults_random_rotation_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_rotation_expand_description": "Expand to fit rotated image",
    "augmentations_defaults_random_rotation_fill_description": "Fill value for empty areas",
    "augmentations_defaults_color_jitter_description": "Color jitter augmentation",
    "augmentations_defaults_color_jitter_brightness_description": "Brightness variation",
    "augmentations_defaults_color_jitter_contrast_description": "Contrast variation",
    "augmentations_defaults_color_jitter_saturation_description": "Saturation variation",
    "augmentations_defaults_color_jitter_hue_description": "Hue variation",
    "augmentations_defaults_random_grayscale_description": "Random grayscale conversion",
    "augmentations_defaults_random_grayscale_p_description": "Conversion probability",
    "augmentations_defaults_random_erasing_description": "Random erasing augmentation",
    "augmentations_defaults_random_erasing_p_description": "Erasing probability",
    "augmentations_defaults_random_erasing_scale_description": "Erased area scale range",
    "augmentations_defaults_random_erasing_ratio_description": "Erased area aspect ratio",
    "augmentations_defaults_random_erasing_value_description": "Fill value for erased area",
    "augmentations_defaults_random_erasing_inplace_description": "Erase in-place",
    "augmentations_defaults_normalize_description": "Normalize augmentation",
    "augmentations_defaults_normalize_mean_description": "Mean values (RGB)",
    "augmentations_defaults_normalize_std_description": "Standard deviation (RGB)",
    "augmentations_defaults_random_invert_description": "Random color inversion",
    "augmentations_defaults_random_invert_p_description": "Inversion probability",
    "augmentations_defaults_random_posterize_description": "Random posterize",
    "augmentations_defaults_random_posterize_bits_description": "Bits per channel",
    "augmentations_defaults_random_posterize_p_description": "Posterize probability",
    "augmentations_defaults_random_solarize_description": "Random solarize",
    "augmentations_defaults_random_solarize_threshold_description": "Solarize threshold",
    "augmentations_defaults_random_solarize_p_description": "Solarize probability",
    "augmentations_defaults_random_adjust_sharpness_description": "Random sharpness adjustment",
    "augmentations_defaults_random_adjust_sharpness_sharpness_factor_description": "Sharpness factor",
    "augmentations_defaults_random_adjust_sharpness_p_description": "Adjustment probability",
    "augmentations_defaults_random_autocontrast_description": "Random auto contrast",
    "augmentations_defaults_random_autocontrast_p_description": "Auto contrast probability",
    "augmentations_defaults_random_equalize_description": "Random histogram equalization",
    "augmentations_defaults_random_equalize_p_description": "Equalization probability",
    "augmentations_defaults_random_perspective_description": "Random perspective transform",
    "augmentations_defaults_random_perspective_distortion_scale_description": "Distortion scale",
    "augmentations_defaults_random_perspective_p_description": "Transform probability",
    "augmentations_defaults_random_perspective_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_perspective_fill_description": "Fill value for empty areas",
    "regularization_description": "Regularization configurations",
    "regularization_defaults_description": "Default regularization parameters",
    "regularization_properties_description": "Regularization properties",
    "normalization_description": "Normalization layer configurations",
    "normalization_defaults_description": "Default normalization parameters",
    "normalization_properties_description": "Normalization properties",
    "pooling_description": "Pooling layer configurations",
    "pooling_defaults_description": "Default pooling parameters",
    "pooling_properties_description": "Pooling properties"
  },
  "presets": {
    "binary_multiclass_classification": {
      "name": "Binary/Multi-class Classification",
      "description": "Standard single-label classification where each image belongs to exactly ONE class. Perfect for building models that categorize images into distinct, mutually exclusive categories. USE CASES: Cat vs dog detection, product category identification (electronics/clothing/food), digit recognition (0-9), sentiment classification from facial expressions, defective vs non-defective parts, spam vs legitimate images, gender classification, yes/no decisions, pass/fail quality control. SUPPORTED MODELS: ResNet (reliable baseline), EfficientNet (best accuracy), MobileNet/ShuffleNet (fast inference), SqueezeNet (minimal size), ResNeXt (complex patterns). OUTPUT: Single class label with softmax confidence score per image. The model learns decision boundaries between categories and outputs the most likely class."
    },
    "confidence_based_classification": {
      "name": "Confidence-based Classification",
      "description": "Single-label classification optimized for well-calibrated confidence scores. The confidence percentage should accurately reflect prediction certainty - 90% confident should be correct ~90% of the time. USE CASES: Medical diagnosis (tumor/benign with reliable confidence), quality control decisions (pass/fail with certainty levels), authentication systems (genuine/fake), safety-critical applications, autonomous vehicle decisions, financial fraud detection, insurance claim assessment, legal document verification. SUPPORTED MODELS: EfficientNet (best calibration), ResNet/ResNeXt (reliable), avoid lightweight models for critical decisions. OUTPUT: Class prediction with calibrated confidence that accurately reflects true probability of correctness. Essential when confidence thresholds drive real-world decisions."
    },
    "content_moderation_safety": {
      "name": "Content Moderation / Safety",
      "description": "Detect inappropriate, unsafe, or policy-violating content in images. Multi-label approach detects multiple violation types simultaneously. Critical for platforms hosting user-generated content. USE CASES: NSFW/adult content detection, violence/gore filtering, hate symbol recognition, spam image detection, child safety systems, brand safety for advertising, platform policy enforcement, workplace-appropriate filtering, age-restricted content flagging. SUPPORTED MODELS: ResNet/ResNeXt (reliable detection), EfficientNet (high accuracy), MobileNet for real-time scanning. Train on diverse violation examples. OUTPUT: Safety flags with confidence scores, e.g., {adult: 0.95, violence: 0.12, spam: 0.03}. Use conservative thresholds and combine with human review for edge cases. Continuous model updates needed as violators adapt."
    },
    "document_classification": {
      "name": "Document / Text Image Classification",
      "description": "Classify document images, forms, and text-containing visual content. Optimized for high-frequency details and structured layouts typical in documents. USE CASES: Document type classification (invoice, receipt, ID card, passport, form, certificate), handwriting vs printed text detection, language/script identification, document orientation detection, form field detection, check processing, mail sorting, legal document categorization, medical record classification. SUPPORTED MODELS: ResNet (good for structured content), EfficientNet (captures fine text details), MobileNet for high-volume processing. Standard 224px input usually sufficient. OUTPUT: Document category enabling automated routing, archival, and processing pipelines. Combine with OCR for full document understanding."
    },
    "efficientnet_balanced_scaling": {
      "name": "EfficientNet Family - Balanced Scaling",
      "description": "Optimized for EfficientNet architecture (B0-B7, V2-S/M/L) using compound scaling that uniformly scales depth, width, and resolution with fixed ratios. EfficientNets achieve state-of-the-art accuracy while being 8-10x more efficient than previous architectures. Best for: maximum accuracy tasks, medical image analysis (X-ray, MRI, pathology), fine-grained recognition (bird species, car models, plant diseases), scientific imaging, satellite/aerial imagery, quality inspection with subtle defects, art authentication, forgery detection, dermoscopy, retinal imaging, food quality assessment. EfficientNet-B0 matches ResNet50 accuracy with 5x fewer parameters. B4-B7 for when accuracy is critical regardless of compute. EfficientNetV2 variants offer faster training with progressive learning. The gold standard for accuracy-critical applications."
    },
    "fast_mobile_inference": {
      "name": "Fast Mobile Inference",
      "description": "Lightweight models optimized for maximum inference speed and minimal resource usage. Trades some accuracy for 3-10x faster predictions and smaller model files. USE CASES: Mobile apps (iOS/Android), embedded systems (Raspberry Pi, Jetson Nano), real-time video processing, edge AI devices, IoT sensors, smart cameras, drone vision, AR/VR applications, browser-based inference, batch processing millions of images, latency-critical applications (<10ms response). SUPPORTED MODELS: MobileNetV3 (best speed/accuracy), ShuffleNet (very fast), SqueezeNet (smallest file size ~5MB), avoid ResNet/EfficientNet for speed-critical deployment. OUTPUT: Same classification results but optimized for deployment - smaller model files, faster inference, lower memory usage. Ideal for production systems with hardware constraints."
    },
    "feature_extraction_embedding": {
      "name": "Feature Extraction / Embedding Learning",
      "description": "Train models to produce meaningful feature vectors (embeddings) where similar images map to nearby points in vector space. The learned representations enable similarity-based applications without explicit classification. USE CASES: Visual search engines, reverse image lookup, duplicate/near-duplicate detection, image clustering, content-based recommendation, face recognition systems, product matching, trademark similarity search, plagiarism detection, visual analogies. SUPPORTED MODELS: ResNet50 (industry standard embeddings), EfficientNet (compact effective embeddings), ResNeXt (rich features). Extract features from penultimate layer. OUTPUT: Fixed-size vector (512-2048 dimensions) per image. Compare using cosine similarity or Euclidean distance. Can be indexed for fast similarity search (FAISS, Annoy)."
    },
    "finegrained_recognition": {
      "name": "Fine-grained Visual Recognition",
      "description": "Distinguish between highly similar subcategories requiring attention to subtle visual differences. Uses larger input resolution and deeper networks to capture fine details. USE CASES: Bird species identification (200+ species), car make/model/year recognition, plant/flower species classification, dog/cat breed identification, aircraft type recognition, mushroom species (safety-critical), fish species for fishing apps, insect identification, fashion brand recognition, font identification. SUPPORTED MODELS: EfficientNet-B4+ (best for fine details), ResNeXt (captures subtle patterns), ResNet101/152 (proven performer). Larger input sizes (380-528px) recommended. Avoid lightweight models for fine-grained tasks. OUTPUT: Specific subcategory with confidence, e.g., Labrador Retriever: 94% not just dog. Requires high-quality training images with expert labels."
    },
    "highres_detail_preservation": {
      "name": "High Resolution / Detail Preservation",
      "description": "Training with larger input images (380-528px+) to preserve fine visual details critical for accurate classification. Requires more GPU memory but captures information lost at standard 224px resolution. USE CASES: Texture/fabric analysis, manufacturing defect detection (scratches, cracks), satellite/aerial imagery analysis, artwork authentication, surface inspection, circuit board inspection, gemstone grading, print quality assessment, forensic image analysis. SUPPORTED MODELS: EfficientNet-B4/B5/B6 (designed for higher resolutions), ResNeXt (detail-oriented). Avoid lightweight models - they lose detail benefits. Requires 8GB+ VRAM. OUTPUT: Classifications based on fine-grained details invisible at standard resolution. Essential when subtle visual differences determine the correct class."
    },
    "image_quality_assessment": {
      "name": "Image Quality Assessment",
      "description": "Evaluate technical and perceptual quality of images. Detects blur, noise, exposure issues, and assesses overall aesthetic appeal. USE CASES: Photo gallery quality scoring, automated photo culling for events, blur/motion detection, exposure assessment, print-worthiness evaluation, social media post optimization, camera/lens testing, image selection for publications, photography contest screening, real estate photo quality control. SUPPORTED MODELS: EfficientNet (best quality assessment), ResNet (technical quality), MobileNet for real-time quality filtering during capture. Medium resolution input sufficient. OUTPUT: Quality classification (excellent/good/fair/poor) or numeric score. Can assess technical merit (sharpness, noise, exposure) and/or aesthetic appeal separately."
    },
    "medical_scientific_analysis": {
      "name": "Medical / Scientific Image Analysis",
      "description": "High-precision classification for medical, scientific, and diagnostic imaging where accuracy and reliability are paramount. Conservative training settings with extensive validation. USE CASES: X-ray analysis (pneumonia, fractures, tuberculosis), CT/MRI scan classification, histopathology slide screening, retinal disease detection, skin lesion classification, cell type identification, microscopy analysis, satellite imagery classification, materials science imaging, pharmaceutical quality control. SUPPORTED MODELS: EfficientNet (best accuracy, well-calibrated), ResNeXt (complex pattern recognition), ResNet (proven medical imaging backbone). Always validate with domain experts. OUTPUT: Diagnostic classification with high reliability - intended as decision SUPPORT, not replacement for professional judgment. Requires extensive validation before clinical use."
    },
    "mobilenet_edge_deployment": {
      "name": "MobileNet Family - Edge Deployment",
      "description": "Optimized for MobileNet V2/V3 architectures using depthwise separable convolutions and inverted residuals for maximum efficiency. MobileNetV3 incorporates neural architecture search (NAS) and squeeze-and-excitation blocks. Best for: mobile app deployment, real-time inference on phones/tablets, edge AI devices, Raspberry Pi/Jetson deployment, smart cameras, surveillance systems, retail analytics, autonomous robots, drone vision, AR/VR applications, wearable devices, smart home devices, industrial IoT. MobileNetV3-Small for ultra-fast inference (<5ms), V3-Large for better accuracy while still being mobile-friendly. Achieves 3-5x faster inference than ResNet50 with ~75-80% of its accuracy. Perfect when latency matters more than peak accuracy."
    },
    "multi_object_detection": {
      "name": "Scene Classification / Object Presence",
      "description": "Scene-level classification and object presence detection without localization (no bounding boxes). Determines what objects/concepts are present in an image or classifies the overall scene type. USE CASES: Scene recognition (beach, mountain, city, indoor, outdoor), room type classification (kitchen, bedroom, office), general object presence (contains_car, has_person, shows_food), environment classification, context understanding for other AI systems, image organization by scene. SUPPORTED MODELS: ResNet (proven scene classifier), EfficientNet (good scene features), all models suitable. Unlike YOLO/RCNN, this is image-level not instance-level. OUTPUT: Scene category OR multi-label presence indicators. Simpler and faster than full object detection when you only need to know WHAT is present, not WHERE."
    },
    "multi_object_multi_attribute": {
      "name": "Complex Multi-attribute Analysis",
      "description": "Comprehensive multi-attribute analysis extracting many simultaneous properties from each image. Learns complex relationships between attributes. USE CASES: E-commerce product cataloging (size, color, material, style, brand, condition), person/character analysis (age, gender, clothing, accessories, pose, emotion), vehicle inspection (make, model, year, color, damage, modifications), real estate listing analysis (room type, features, condition, style, lighting), fashion analysis. SUPPORTED MODELS: ResNeXt (best for attribute relationships), EfficientNet (many attributes efficiently), ResNet (reliable multi-output). Deeper models handle more attributes. OUTPUT: Rich attribute dictionary with confidence scores across all defined characteristics. Single model predicts 10-50+ attributes simultaneously."
    },
    "multilabel_boolean": {
      "name": "Multi-label Classification (Boolean Tags)",
      "description": "Assign multiple binary (yes/no) tags to each image. Each attribute is an independent decision with 0.5 threshold. Simpler than probability-based when you just need presence/absence decisions. USE CASES: Image tagging systems (has_people, outdoor, night_scene), content moderation flags (violence, nudity, spam), product attributes (is_red, is_leather, has_logo), document properties (is_signed, has_stamp, is_color), medical image flags (has_tumor, abnormal, requires_review). SUPPORTED MODELS: All 6 architectures work well - choose based on deployment needs. ResNet/EfficientNet for accuracy, MobileNet/ShuffleNet/SqueezeNet for speed/size. OUTPUT: Boolean tags per image, e.g., {has_people: true, outdoor: true, night: false}. Clean yes/no decisions for each defined attribute."
    },
    "multilabel_probability": {
      "name": "Multi-label Classification (Probability Scores)",
      "description": "Assign multiple tags with probability scores (0.0-1.0) to each image. Each tag is independent - an image can have any combination of tags with varying confidence levels. USE CASES: Content recommendation systems needing relevance scores, aesthetic quality assessment across multiple dimensions, emotion detection (happy: 0.85, surprised: 0.32), social media hashtag suggestion with relevance ranking, music mood tagging, movie genre prediction, e-commerce product attribute scoring, news article image categorization. SUPPORTED MODELS: ResNet/ResNeXt (accurate probabilities), EfficientNet (best calibration), MobileNet/ShuffleNet (real-time tagging), SqueezeNet (edge deployment). OUTPUT: Dictionary of tags with probability scores, e.g., {outdoor: 0.94, sunny: 0.78, people: 0.45}. Allows custom threshold tuning per application."
    },
    "resnet_deep_learning": {
      "name": "ResNet Family - Deep Feature Learning",
      "description": "Optimized for the ResNet architecture family (ResNet18/34/50/101/152) which revolutionized deep learning with residual skip connections. ResNets enable training of very deep networks (up to 152 layers) without degradation. Best for: general image classification, transfer learning foundation, object recognition, scene classification, facial recognition, vehicle identification, plant/animal species classification, product categorization, satellite imagery analysis, X-ray/CT scan classification, quality control inspection. ResNets are the industry standard backbone due to excellent balance of accuracy, speed, and reliability. ResNet50 is often the best starting point. Smaller variants (18/34) for speed-critical applications, deeper (101/152) when accuracy is paramount. Extensively tested and well-documented architecture with abundant pretrained weights."
    },
    "resnext_aggregated_networks": {
      "name": "ResNeXt Family - Aggregated Residual Networks",
      "description": "Optimized for ResNeXt architecture (ResNeXt50/101 with 32x4d, 32x8d, 64x4d cardinalities) which extends ResNet with split-transform-merge paradigm using grouped convolutions. ResNeXt introduces cardinality as a new scaling dimension beyond depth and width. Best for: complex multi-class classification, fine-grained visual recognition, multi-attribute detection, fashion/clothing classification, vehicle make/model recognition, food recognition, artwork style classification, texture analysis, material classification, detailed product categorization. Achieves 1-2% higher accuracy than equivalent ResNets with similar compute. The 32x4d variant offers best accuracy/speed ratio, 32x8d and 64x4d for maximum accuracy. Ideal when ResNet accuracy plateaus and you need that extra performance boost without architectural changes."
    },
    "shufflenet_lightweight_speed": {
      "name": "ShuffleNet - Lightweight Speed Champion",
      "description": "Optimized for ShuffleNet V2 architecture which uses channel shuffle operations for efficient information exchange between channel groups. ShuffleNet achieves excellent speed-accuracy tradeoff through pointwise group convolutions and channel shuffle. Best for: real-time mobile applications, embedded systems with strict latency requirements, edge AI devices, IoT sensors, robotics vision, drone-based recognition, wearable devices, smart cameras, traffic monitoring, retail analytics, industrial inspection on resource-constrained hardware. ShuffleNet variants range from ultra-light (x0.5) to more capable (x2.0), all maintaining fast inference. Ideal when inference speed is critical and model size must be minimal. Not recommended for tasks requiring maximum accuracy or fine-grained detail recognition.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smallest ShuffleNet variant. High risk of overfitting - early stopping essential.",
        "large": "Large datasets (10k-30k images). Reduced regularization, larger ShuffleNet variants viable. Higher learning rates and bigger batches effective.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Larger ShuffleNet variants train effectively.",
        "massive": "Massive datasets (50k-100k images). Fast learning rates, large batches, minimal regularization. ShuffleNet x1.5 or x2.0 recommended.",
        "giant": "Giant datasets (100k+ images). Maximum ShuffleNet capacity (x2.0). Can train from scratch or fine-tune with aggressive settings."
      }
    },
    "small_dataset_fewshot": {
      "name": "Small Dataset / Few-shot Learning",
      "description": "Optimized for training with very limited data (50-500 images total or per class). Uses aggressive regularization, pretrained weights, and careful learning rate scheduling to prevent overfitting. USE CASES: Prototype/MVP development, rare object classification, custom corporate datasets, niche applications (rare diseases, specialized equipment), personal projects, academic research with limited samples, new product categories before full dataset collection. SUPPORTED MODELS: ResNet18/34 (stable with small data), EfficientNet-B0/B1 (good generalization), MobileNetV3-Small (if speed needed). Avoid very deep models (ResNet152, EfficientNet-B7) which overfit easily. OUTPUT: Reasonable classification despite limited data. Expect 70-85% accuracy - augment data and collect more samples to improve. Transfer learning is essential."
    },
    "squeezenet_ultra_compact": {
      "name": "SqueezeNet - Ultra Compact Networks",
      "description": "Optimized for SqueezeNet architecture which achieves AlexNet-level accuracy with 50x fewer parameters using Fire modules (squeeze and expand layers). SqueezeNet is designed for extreme model compression while maintaining reasonable accuracy. Best for: ultra-low memory devices, microcontrollers, FPGA deployment, model compression research, bandwidth-limited edge devices, embedded systems with <5MB storage, smart sensors, always-on vision applications, battery-powered devices where model size directly impacts power consumption. SqueezeNet 1.1 is 2.4x faster than 1.0 with same accuracy. Ideal when model size is the primary constraint. Not recommended for tasks requiring high accuracy or complex feature hierarchies.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization essential for this already-compact architecture. Early stopping critical to prevent overfitting.",
        "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained Fire modules. SqueezeNet learns efficiently from limited data.",
        "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters work well. SqueezeNet's compact size allows larger batch sizes.",
        "large": "Large datasets (10k-30k images). Reduced regularization. SqueezeNet 1.0 can be used for slightly more capacity.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization. SqueezeNet reaches its capacity limits here - consider larger architectures for better accuracy.",
        "massive": "Massive datasets (50k-100k images). SqueezeNet may underfit - this preset pushes the architecture to its limits. Consider EfficientNet for better results.",
        "giant": "Giant datasets (100k+ images). SqueezeNet is NOT recommended for datasets this large - limited capacity will cause underfitting. Use only if model size is absolutely critical."
      }
    },
    "style_aesthetic_classification": {
      "name": "Style / Aesthetic Classification",
      "description": "Classify images by visual style, artistic qualities, mood, or aesthetic properties. Focuses on global image characteristics (color palette, composition, texture) rather than specific objects. USE CASES: Photo quality scoring (professional vs amateur), art style recognition (impressionist, cubist, modern, abstract), interior design style (minimalist, rustic, industrial, bohemian), fashion style classification, image mood detection (cheerful, melancholic, dramatic), visual content curation, photography style (portrait, landscape, macro, street). SUPPORTED MODELS: EfficientNet (captures global features well), ResNeXt (style patterns), ResNet (reliable baseline). All models can learn style features effectively. OUTPUT: Style category or aesthetic quality score for content filtering, recommendation, curation systems, and creative applications."
    },
    "configs": {
      "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smaller models. High risk of overfitting - early stopping essential.",
      "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained weights. Balance between learning capacity and overfitting prevention.",
      "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters with good augmentation. Balanced training approach works well.",
      "large": "Large datasets (10k-30k images). Reduced regularization, larger models viable. Higher learning rates and bigger batches effective.",
      "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Deep networks train effectively with standard augmentation.",
      "massive": "Massive datasets (50k-100k images). Very deep networks fully supported. Fast learning rates, large batches, minimal regularization.",
      "giant": "Giant datasets (100k+ images). Maximum model capacity utilized. Can train from scratch or fine-tune with aggressive settings."
    }
  }
}