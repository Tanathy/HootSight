{
  "language_name": "English",
  "app": {
    "page_not_found": {
      "title": "Page Not Found",
      "description": "The requested page does not exist"
    }
  },
  "updates": {
    "status": {
      "missing": "Missing",
      "outdated": "Outdated",
      "orphaned": "Orphaned"
    },
    "api": {
      "check_failed": "Failed to check for updates: {error}",
      "check_success": "Found {count} updates available",
      "check_no_updates": "No updates available",
      "apply_failed": "Failed to apply updates: {error}",
      "apply_nothing": "No updates to apply",
      "apply_partial": "Partial update: {updated} updated, {failed} failed",
      "apply_success": "Update complete: {updated} file(s) updated"
    }
  },
  "projects": {
    "api": {
      "create_missing": "Project name is required",
      "create_length": "Project name must be between {min} and {max} characters",
      "create_invalid": "Invalid project name",
      "create_exists": "Project '{name}' already exists",
      "create_error": "Failed to create project: {error}",
      "create_success": "Project '{name}' created successfully",
      "delete_invalid": "Invalid project name",
      "delete_not_found": "Project '{name}' not found",
      "delete_success": "Project '{name}' deleted successfully",
      "delete_error": "Failed to delete project: {error}",
      "rename_invalid": "Invalid project name",
      "rename_not_found": "Project '{name}' not found",
      "rename_missing": "New project name is required",
      "rename_success": "Project renamed to '{name}' successfully",
      "rename_error": "Failed to rename project: {error}"
    }
  },
  "dataset_editor": {
    "api": {
      "project_not_found": "Project '{project}' not found",
      "image_not_found": "No images found in project '{project}'",
      "invalid_page_size": "Invalid page size. Allowed: {allowed}",
      "invalid_size": "Invalid size. Allowed: {allowed}",
      "invalid_image_path": "Invalid image path",
      "image_missing": "Image file not found"
    }
  },
  "docs": {
    "api": {
      "missing_root": "Documentation root not configured",
      "not_found": "Document '{path}' not found",
      "decode_error": "Failed to decode '{path}'",
      "read_failed": "Failed to read document: {error}"
    }
  },
  "augmentation": {
    "preview_invalid_phase": "Invalid augmentation phase: {phase}",
    "preview_no_images": "No images found in project '{project}'",
    "preview_failed": "Failed to generate preview: {error}"
  },
  "augmentations": {
    "resize": "Resize",
    "random_crop": "Random Crop",
    "random_resized_crop": "Random Resized Crop",
    "center_crop": "Center Crop",
    "random_affine": "Random Affine",
    "random_horizontal_flip": "Random Horizontal Flip",
    "random_vertical_flip": "Random Vertical Flip",
    "random_rotation": "Random Rotation",
    "color_jitter": "Color Jitter",
    "random_grayscale": "Random Grayscale",
    "random_erasing": "Random Erasing",
    "normalize": "Normalize",
    "random_invert": "Random Invert",
    "random_posterize": "Random Posterize",
    "random_solarize": "Random Solarize",
    "random_adjust_sharpness": "Random Adjust Sharpness",
    "random_autocontrast": "Random Auto Contrast",
    "random_equalize": "Random Equalize",
    "random_perspective": "Random Perspective"
  },
  "common": {
    "project_label": "Project:",
    "cancel": "Cancel"
  },
  "params": {
    "lr": "Learning Rate",
    "betas": "Betas",
    "eps": "Epsilon",
    "weight_decay": "Weight Decay",
    "amsgrad": "AMSGrad",
    "momentum": "Momentum",
    "dampening": "Dampening",
    "nesterov": "Nesterov",
    "alpha": "Alpha",
    "centered": "Centered",
    "etas": "Etas",
    "step_sizes": "Step Sizes",
    "lr_decay": "LR Decay",
    "initial_accumulator_value": "Initial Accumulator",
    "rho": "Rho",
    "max_iter": "Max Iterations",
    "max_eval": "Max Evaluations",
    "tolerance_grad": "Gradient Tolerance",
    "tolerance_change": "Change Tolerance",
    "history_size": "History Size",
    "line_search_fn": "Line Search Function",
    "lambd": "Lambda",
    "t0": "T0",
    "momentum_decay": "Momentum Decay",
    "step_size": "Step Size",
    "gamma": "Gamma",
    "milestones": "Milestones",
    "T_max": "T Max",
    "eta_min": "Eta Min",
    "T_0": "T0",
    "T_mult": "T Multiplier",
    "mode": "Mode",
    "factor": "Factor",
    "patience": "Patience",
    "threshold": "Threshold",
    "threshold_mode": "Threshold Mode",
    "cooldown": "Cooldown",
    "min_lr": "Min LR",
    "base_lr": "Base LR",
    "max_lr": "Max LR",
    "step_size_up": "Step Size Up",
    "step_size_down": "Step Size Down",
    "scale_fn": "Scale Function",
    "scale_mode": "Scale Mode",
    "cycle_momentum": "Cycle Momentum",
    "base_momentum": "Base Momentum",
    "max_momentum": "Max Momentum",
    "total_steps": "Total Steps",
    "epochs": "Epochs",
    "steps_per_epoch": "Steps Per Epoch",
    "pct_start": "Percent Start",
    "anneal_strategy": "Anneal Strategy",
    "div_factor": "Div Factor",
    "final_div_factor": "Final Div Factor",
    "three_phase": "Three Phase",
    "total_iters": "Total Iterations",
    "power": "Power",
    "start_factor": "Start Factor",
    "end_factor": "End Factor",
    "lr_lambda": "LR Lambda",
    "reduction": "Reduction",
    "p": "P",
    "margin": "Margin",
    "swap": "Swap",
    "size_average": "Size Average",
    "reduce": "Reduce",
    "delta": "Delta",
    "beta": "Beta",
    "blank": "Blank",
    "zero_infinity": "Zero Infinity",
    "log_input": "Log Input",
    "full": "Full",
    "gain": "Gain",
    "a": "A",
    "b": "B",
    "mean": "Mean",
    "std": "Std",
    "val": "Value",
    "sparsity": "Sparsity",
    "groups": "Groups",
    "nonlinearity": "Nonlinearity",
    "size": "Size",
    "padding": "Padding",
    "pad_if_needed": "Pad If Needed",
    "fill": "Fill",
    "padding_mode": "Padding Mode",
    "scale": "Scale",
    "ratio": "Ratio",
    "interpolation": "Interpolation",
    "degrees": "Degrees",
    "expand": "Expand",
    "center": "Center",
    "translate": "Translate",
    "shear": "Shear",
    "distortion_scale": "Distortion Scale",
    "brightness": "Brightness",
    "contrast": "Contrast",
    "saturation": "Saturation",
    "hue": "Hue",
    "value": "Value",
    "inplace": "Inplace",
    "bits": "Bits",
    "sharpness_factor": "Sharpness Factor",
    "transforms": "Transforms"
  },
  "nav": {
    "projects": "Projects",
    "training_setup": "Training Setup",
    "dataset": "Dataset",
    "performance": "Performance",
    "heatmap": "Heatmap",
    "updates": "Updates"
  },
  "ui": {
    "language_select_title": "Language"
  },
  "context_menu": {
    "view_image": "View Image",
    "select": "Select",
    "deselect": "Deselect",
    "select_all": "Select All",
    "clear_selection": "Clear Selection",
    "copy_filename": "Copy Filename",
    "copy_path": "Copy Path",
    "delete": "Delete",
    "delete_selected": "Delete Selected ({count})",
    "open_folder": "Open in Explorer",
    "new_subfolder": "New Subfolder"
  },
  "training_controller": {
    "stop": "Stop",
    "clear": "Clear",
    "completed": "Completed",
    "stopped": "Stopped",
    "error": "Error",
    "already_running": "Training is already running",
    "mode": {
      "new": "New model",
      "resume": "Continuing",
      "finetune": "Fine-tuning"
    }
  },
  "projects_page": {
    "title": "Projects",
    "description": "Manage your image recognition projects",
    "empty": {
      "title": "No Projects Found",
      "description": "Create a new project to get started"
    },
    "error": {
      "title": "Error Loading Projects"
    },
    "card": {
      "unknown_type": "Unknown",
      "no_stats": "No statistics available",
      "loading": "Loading",
      "error": "Error"
    },
    "stats": {
      "total_images": "Total Images",
      "balance_score": "Balance Score",
      "balance_status": "Balance Status",
      "balance_status_excellent": "Excellent",
      "balance_status_very_good": "Very Good",
      "balance_status_good": "Good",
      "balance_status_decent": "Decent",
      "balance_status_fair": "Fair",
      "balance_status_mediocre": "Mediocre",
      "balance_status_poor": "Poor",
      "balance_status_bad": "Bad",
      "balance_status_terrible": "Terrible",
      "balance_status_nonsense": "Nonsense"
    },
    "buttons": {
      "refresh": "Refresh",
      "load": "Load",
      "loaded": "Loaded",
      "start_training": "Start Training",
      "new_training": "New Training",
      "resume_training": "Resume Training",
      "finetune_training": "Fine-tune",
      "stop_training": "Stop Training",
      "new_project": "New Project",
      "rename_project": "Rename",
      "delete_project": "Delete"
    },
    "training": {
      "stop_error": "Failed to stop training",
      "start_error": "Failed to start training",
      "no_project": "No project selected",
      "finetune_toggle": "Fine-tune existing model",
      "resume_missing_checkpoint": "No checkpoint found. Starting a new training run.",
      "resume_title": "Fine-tune existing model?",
      "resume_prompt": "A trained model was found. Do you want to fine-tune from the latest checkpoint?",
      "resume_yes": "Fine-tune",
      "resume_no": "Start New"
    },
    "new_project": {
      "title": "Create New Project",
      "prompt": "Enter project name:",
      "error": "Failed to create project"
    },
    "delete_project": {
      "title": "Delete Project",
      "confirm": "Are you sure you want to delete project '{name}'? This action cannot be undone.",
      "no_selection": "No project selected",
      "error": "Failed to delete project"
    },
    "rename_project": {
      "title": "Rename Project",
      "prompt": "Enter new project name:",
      "no_selection": "No project selected",
      "error": "Failed to rename project"
    },
    "dataset_types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    }
  },
  "training_page": {
    "tabs": {
      "presets": "Presets",
      "model": "Model",
      "hyperparameters": "Hyperparameters",
      "dataloader": "Data Loader",
      "augmentation": "Augmentation",
      "optimizer": "Optimizer",
      "scheduler": "Scheduler",
      "loss": "Loss",
      "checkpoint": "Checkpoint",
      "early_stopping": "Early Stopping",
      "gradient": "Gradient",
      "runtime": "Runtime"
    },
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to configure training settings.",
      "button": "Go to Projects"
    },
    "load_defaults_button": "Load Defaults",
    "load_project_button": "Load Project",
    "save_button": "Save",
    "saving": "Saving...",
    "saved": "Saved",
    "save_error": "Save Error",
    "defaults_loaded": "Defaults Loaded",
    "project_loaded": "Project Loaded",
    "load_error": "Load Error",
    "no_settings": "No settings available",
    "augmentation": {
      "train_title": "Training Augmentations",
      "train_description": "Augmentations applied during training to improve model generalization",
      "val_title": "Validation Augmentations",
      "val_description": "Minimal augmentations for validation data (typically just resize and normalize)",
      "preview_placeholder": "Drop an image or click Random to preview augmentations",
      "btn_random": "Random",
      "btn_train": "Train",
      "btn_val": "Val",
      "no_params": "No parameters"
    },
    "presets": {
      "title": "Training Presets",
      "description": "Quick configuration templates for common training scenarios",
      "search_placeholder": "Search presets by name, description, task...",
      "task_filter_label": "Task",
      "task_filter_all": "All tasks",
      "task_filter_classification": "Single-label / Binary",
      "task_filter_multi_label": "Multi-label",
      "category.classification": "Classification",
      "category.multi_label": "Multi-label",
      "only_compatible": "Only Compatible",
      "search_results": "presets found",
      "loading": "Loading presets...",
      "no_presets": "No presets available",
      "apply_error": "Failed to apply preset",
      "applied": "Preset applied successfully",
      "incompatible_dataset": "Incompatible with current dataset",
      "dataset_size": "images",
      "recommended": "Recommended",
      "apply_button": "Apply"
    }
  },
  "dataset_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to view the dataset.",
      "button": "Go to Projects"
    },
    "type_label": "Dataset Type",
    "types": {
      "unknown": "Unknown",
      "multi_label": "Multi-Label",
      "folder_classification": "Folder Classification",
      "annotation": "Annotation"
    },
    "sync_button": "Sync Dataset",
    "build_button": "Build Dataset",
    "progress_discovery": "Discovering images",
    "progress_starting": "Starting...",
    "progress_complete": "Complete",
    "new_folder": "New Folder",
    "rename_folder": "Rename Folder",
    "delete_folder": "Delete Folder",
    "per_page": "Per page",
    "search_filename": "Filename",
    "search_annotation": "Annotation",
    "search_placeholder": "Search images...",
    "show_duplicates": "Show Duplicates",
    "duplicates_none": "No duplicate images found",
    "duplicates_error": "Failed to check for duplicates",
    "folders": "Folders",
    "drop_images": "Drop images here to upload",
    "root_folder": "Root",
    "delete_image": "Delete Image",
    "duplicate_count": "{count} duplicates",
    "tag_placeholder": "Add tags...",
    "annotation_placeholder": "Add annotation...",
    "confirm_delete": "Are you sure you want to delete this image?",
    "uploading": "Uploading...",
    "upload_error": "Upload failed",
    "enter_folder_name": "Enter folder name:",
    "folder_create_error": "Failed to create folder",
    "enter_new_name": "Enter new name:",
    "folder_rename_error": "Failed to rename folder",
    "delete_folder_confirm_with_images": "This folder contains {count} images. Are you sure you want to delete it?",
    "delete_folder_confirm": "Are you sure you want to delete this folder?",
    "folder_delete_error": "Failed to delete folder",
    "build_error": "Build failed",
    "status_count": "{count} images",
    "clear_selection": "Clear Selection",
    "select_all": "Select All",
    "bulk_add_tags_placeholder": "Tags to add...",
    "bulk_add": "Add",
    "bulk_remove_tags_placeholder": "Tags to remove...",
    "bulk_remove": "Remove",
    "bulk_delete": "Delete Selected",
    "selected_count": "{count} selected",
    "bulk_error": "Bulk operation failed",
    "bulk_delete_confirm": "Are you sure you want to delete {count} images?"
  },
  "performance_page": {
    "tabs": {
      "training": "Training",
      "system": "System"
    },
    "training": {
      "title": "Training Progress",
      "no_active": "No active training session",
      "no_data": "No Training Data",
      "description": "Start training to see progress graphs",
      "running": "Training in progress",
      "completed": "Training completed",
      "stopped": "Training stopped",
      "train": "Train",
      "validation": "Validation",
      "train_step_loss": "Training Step Loss",
      "val_step_loss": "Validation Step Loss",
      "step_accuracy": "Step Accuracy",
      "learning_rate": "Learning Rate",
      "epoch_loss": "Epoch Loss",
      "epoch_accuracy": "Epoch Accuracy",
      "current": "Current",
      "step": "Step",
      "train_step_accuracy": "Train Step Accuracy",
      "val_step_accuracy": "Val Step Accuracy",
      "current_lr": "Current LR",
      "train_loss": "Train Loss",
      "val_loss": "Val Loss",
      "train_accuracy": "Train Accuracy",
      "val_accuracy": "Val Accuracy"
    },
    "system": {
      "title": "System Monitor",
      "monitoring_active": "Monitoring active",
      "monitoring_paused": "Monitoring paused",
      "cpu_usage": "CPU Usage",
      "system_memory": "System Memory",
      "gpu_usage": "GPU {index} Usage",
      "gpu_memory": "GPU {index} Memory",
      "speed": "Speed",
      "cores_threads": "Cores/Threads",
      "temperature": "Temperature",
      "max_memory": "Max Memory",
      "available": "Available",
      "power": "Power",
      "gpu_clock": "GPU Clock",
      "mem_clock": "Memory Clock",
      "fan": "Fan",
      "dedicated_memory": "Dedicated Memory",
      "used_memory": "Used Memory"
    }
  },
  "heatmap_page": {
    "no_project": {
      "title": "No Project Selected",
      "description": "Please select a project from the Projects page to generate heatmaps.",
      "button": "Go to Projects"
    },
    "drop_zone": {
      "text": "Drop image here",
      "hint": "or click to select"
    },
    "alpha_label": "Overlay Alpha",
    "results": {
      "title": "Results",
      "placeholder": "Drop an image to see predictions and heatmap",
      "no_predictions": "No predictions available"
    },
    "live_model_switch": "Use live model",
    "multi_label_switch": "Multi-label",
    "random_button": "Random",
    "refresh_button": "Refresh",
    "checkpoint_label": "Checkpoint:"
  },
  "updates_ui": {
    "page_title": "Updates",
    "page_description": "Check for and apply system updates",
    "intro": "Check for updates to keep your system current with the latest features and fixes.",
    "check_button": "Check for Updates",
    "apply_button": "Apply Updates",
    "status_idle": "Ready to check for updates",
    "status_checking": "Checking for updates...",
    "status_ready": "Updates available",
    "status_up_to_date": "System is up to date",
    "status_applying": "Applying updates...",
    "status_applied": "Updates applied successfully",
    "status_failed": "Update check failed",
    "no_updates": "No updates available",
    "files_to_update": "files to update",
    "orphaned_count": "orphaned files",
    "table_header_file": "File",
    "table_header_status": "Status"
  },
  "schema": {
    "description": "Configuration schema for Hootsight image recognition system",
    "ui_group": {
      "general": "General",
      "api": "API Settings",
      "ui_settings": "User Interface",
      "paths": "Paths",
      "system": "System",
      "memory": "Memory Management",
      "dataset": "Dataset",
      "dataset_editor": "Dataset Editor",
      "training_model": "Model",
      "training_hyperparameters": "Hyperparameters",
      "training_dataloader": "Data Loader",
      "training_augmentation": "Augmentation",
      "training_optimizer": "Optimizer",
      "training_scheduler": "Scheduler",
      "training_loss": "Loss Function",
      "training_checkpoint": "Checkpointing",
      "training_early_stopping": "Early Stopping",
      "training_gradient": "Gradient",
      "training_runtime": "Runtime",
      "activations_defaults": "Activation Defaults",
      "augmentations_defaults": "Augmentation Defaults",
      "optimizers": "Optimizers",
      "schedulers": "Schedulers",
      "losses": "Losses",
      "activations": "Activations",
      "augmentations": "Augmentations",
      "regularization": "Regularization",
      "normalization": "Normalization",
      "pooling": "Pooling",
      "models": "Models"
    },
    "general_description": "General application settings",
    "general_language_description": "User interface language",
    "general_language_enum_descriptor": {
      "en": "English"
    },
    "api_description": "API server configuration",
    "api_host_description": "API server host address",
    "api_port_description": "API server port number",
    "ui_description": "User interface settings",
    "ui_title_description": "Application window title",
    "ui_width_description": "Window width in pixels",
    "ui_height_description": "Window height in pixels",
    "ui_resizable_description": "Allow window resizing",
    "paths_description": "File and directory paths",
    "paths_projects_dir_description": "Directory containing projects",
    "paths_ui_dir_description": "Directory containing UI files",
    "paths_config_dir_description": "Directory containing configuration files",
    "paths_localizations_dir_description": "Directory containing localization files",
    "paths_packages_file_description": "Path to packages configuration file",
    "paths_cache_dir_description": "Directory for cached files",
    "paths_docs_dir_description": "Directory for documentation",
    "system_description": "System configuration settings",
    "system_max_threads_description": "Maximum number of worker threads",
    "system_max_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "system_fallback_batch_size_description": "Fallback batch size when auto-detection fails",
    "system_startup_wait_seconds_description": "Seconds to wait before starting UI",
    "system_http_timeout_description": "HTTP request timeout in seconds",
    "system_update_repository_url_description": "URL for update repository",
    "system_update_skip_paths_description": "Paths to skip during updates",
    "dataset_editor_description": "Dataset editor configuration",
    "dataset_editor_page_sizes_description": "Available page size options",
    "dataset_editor_default_page_size_description": "Default images per page",
    "dataset_editor_size_presets_description": "Image size presets by model type",
    "dataset_editor_default_size_description": "Default image size",
    "dataset_editor_build_workers_description": "Number of workers for dataset building",
    "dataset_editor_build_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_discovery_workers_description": "Number of workers for image discovery",
    "dataset_editor_discovery_workers_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "dataset_editor_project_name_validation_description": "Project name validation rules",
    "dataset_editor_project_name_validation_min_length_description": "Minimum project name length",
    "dataset_editor_project_name_validation_max_length_description": "Maximum project name length",
    "dataset_editor_project_name_validation_pattern_description": "Allowed project name pattern (regex)",
    "memory_description": "Memory management configuration",
    "memory_target_memory_usage_description": "Target GPU memory usage ratio",
    "memory_safety_margin_description": "Safety margin for memory allocation",
    "memory_augmentation_threads_description": "Threads for augmentation processing",
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Automatic (based on CPU cores)"
    },
    "memory_reserved_memory_ratio_description": "Reserved memory ratio",
    "memory_per_sample_multiplier_description": "Memory multiplier per sample",
    "memory_min_batch_size_description": "Minimum batch size",
    "memory_max_batch_size_description": "Maximum batch size",
    "memory_thresholds_description": "Memory usage thresholds",
    "memory_thresholds_high_usage_description": "High memory usage threshold",
    "memory_thresholds_moderate_usage_description": "Moderate memory usage threshold",
    "memory_thresholds_low_usage_description": "Low memory usage threshold",
    "memory_batch_size_limits_description": "Batch size warning limits",
    "memory_batch_size_limits_small_description": "Small batch size warning threshold",
    "memory_batch_size_limits_large_description": "Large batch size warning threshold",
    "training_description": "Training configuration",
    "training_model_type_title": "Model Type",
    "training_model_type_description": "Select the neural network architecture family. Each architecture offers unique characteristics in terms of accuracy, inference speed, memory usage, and model file size. Your choice should be based on your deployment target (server, mobile, edge device) and accuracy requirements.",
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet (Residual Networks) - The foundational deep learning architecture that introduced skip connections, solving the vanishing gradient problem and enabling training of very deep networks (up to 152 layers). ResNet is the industry benchmark with proven reliability across countless applications. Offers excellent balance between accuracy and computational cost. ResNet50 is the most popular variant, achieving ~76% ImageNet top-1 accuracy. Ideal for: general image classification, transfer learning base, medical imaging, quality inspection, facial recognition. Choose ResNet18/34 for faster inference, ResNet101/152 when maximum accuracy is critical.",
      "resnext": "ResNeXt (Residual Networks with Aggregated Transformations) - An evolution of ResNet that introduces cardinality (number of parallel pathways) as a new dimension alongside depth and width. Uses grouped convolutions to learn richer feature representations. Achieves 1-2% higher accuracy than equivalent ResNet models with similar computational cost. The 32x4d configuration means 32 parallel pathways with 4-dimensional transforms. Ideal for: fine-grained visual recognition (bird species, car models), complex multi-attribute classification, fashion/style analysis, texture recognition, cases where ResNet accuracy plateaus. Best when you need that extra accuracy boost without major architectural changes.",
      "alexnet": "AlexNet - The classic 2012 ImageNet winner. Straightforward architecture with large feature maps and ReLU activations. Heavier than modern light models but simple and dependable for quick baselines. Good for educational purposes, fast experiments, or when you need a shallow network without advanced blocks.",
      "vgg": "VGG - Deep stacks of 3x3 convolutions without residual connections. Delivers strong accuracy but is parameter-heavy and slower than ResNet. Good for transfer learning when you want smooth feature maps and a simple topology. Prefer batch-norm variants (vgg*_bn) for stability.",
      "densenet": "DenseNet - Each layer feeds into all subsequent layers, improving feature reuse and gradient flow. Typically achieves better accuracy than plain ResNet with fewer parameters. Strong for fine-grained tasks and when you want compact yet accurate models.",
      "mobilenet": "MobileNet (Mobile Networks) - Specifically designed for mobile and embedded devices using depthwise separable convolutions that dramatically reduce computation. MobileNetV3 incorporates neural architecture search (NAS) and squeeze-and-excitation blocks. Achieves 75-80% of ResNet50 accuracy while being 3-5x faster with 10x smaller model size. Typical inference: <10ms on modern phones. Ideal for: iOS/Android apps, Raspberry Pi, Jetson Nano, real-time video processing, AR/VR applications, drone vision, IoT sensors, smart cameras, retail analytics. Choose V3-Small for ultra-fast inference, V3-Large for better accuracy while remaining mobile-friendly.",
      "shufflenet": "ShuffleNet - Designed for maximum efficiency on CPU and low-power devices using channel shuffle operations and pointwise group convolutions. Achieves excellent speed-accuracy trade-off by efficiently mixing information between channel groups. Even faster than MobileNet on CPU-only devices. Model sizes range from ultra-light (x0.5, ~1MB) to more capable (x2.0, ~7MB). Ideal for: strict latency requirements (<5ms), robotics vision, wearable devices, always-on smart cameras, traffic monitoring, industrial inspection on resource-constrained hardware, battery-powered devices. Not recommended for tasks requiring maximum accuracy or fine-grained detail recognition.",
      "squeezenet": "SqueezeNet - The most compact architecture, achieving AlexNet-level accuracy with 50x fewer parameters using innovative Fire modules (squeeze and expand layers). Model size as small as ~5MB (or ~0.5MB with deep compression). Designed for extreme model compression while maintaining reasonable accuracy. Ideal for: microcontrollers with <10MB storage, FPGA deployment, over-the-air model updates on bandwidth-limited devices, embedded systems with strict memory constraints, always-on vision applications where power consumption is critical. SqueezeNet 1.1 is 2.4x faster than 1.0. Not recommended for complex tasks or large datasets - limited capacity will cause underfitting.",
      "efficientnet": "EfficientNet - State-of-the-art architecture using compound scaling that uniformly scales network depth, width, and resolution with fixed ratios derived from neural architecture search. Achieves best-in-class accuracy while being 8-10x more efficient than previous architectures. EfficientNet-B0 matches ResNet50 accuracy with 5x fewer parameters. B4-B7 variants achieve up to 84%+ ImageNet accuracy. EfficientNetV2 offers faster training with progressive learning. Ideal for: maximum accuracy requirements, medical image analysis (X-ray, MRI, pathology), fine-grained recognition (species identification, defect detection), satellite/aerial imagery, scientific imaging, art authentication, forensic analysis. The gold standard when accuracy matters most."
    },
    "training_model_name_title": "Model Name",
    "training_model_name_description": "Specific model variant within the selected architecture family. Variants differ in depth, width, and computational requirements. Smaller numbers (e.g., ResNet18) offer faster inference, larger numbers (e.g., ResNet152) provide better accuracy.",
    "training_pretrained_title": "Pretrained",
    "training_pretrained_description": "Use ImageNet pretrained weights as a starting point. Highly recommended for most use cases as it dramatically improves convergence and final accuracy, especially with small datasets. The model has already learned general visual features (edges, textures, shapes) that transfer to your new task.",
    "training_task_title": "Task",
    "training_task_description": "Define how the model should interpret and predict from images. This fundamentally determines the output format and loss function used during training.",
    "training_task_enum_descriptor": {
      "classification": "Single-label Classification - Each image belongs to exactly ONE class from a predefined set. The model outputs a probability distribution over all classes, selecting the highest as the prediction. Uses softmax activation and cross-entropy loss. Ideal for: mutually exclusive categories (cat vs dog, product types, digit recognition 0-9, sentiment from faces, pass/fail inspection). Expected output: single class label with confidence score. Accuracy typically 85-99% depending on task complexity and data quality.",
      "multi_label": "Multi-label Classification - Each image can have MULTIPLE tags/labels simultaneously. The model treats each label as an independent binary decision. Uses sigmoid activation and binary cross-entropy loss. Ideal for: image tagging (outdoor + sunny + people), content moderation (violence + nudity flags), product attributes (red + leather + large), medical symptoms (multiple conditions present). Expected output: set of applicable labels with individual confidence scores. Each label threshold can be tuned independently.",
      "detection": "Object Detection - Locate and classify MULTIPLE objects within an image with bounding boxes. Model predicts object locations (x, y, width, height), class labels, and confidence scores for each detected instance. Ideal for: counting objects, surveillance, autonomous vehicles, retail shelf analysis, document element detection. Expected output: list of detected objects with bounding boxes and class labels. Note: Requires specialized architectures (YOLO, Faster R-CNN) - standard classifiers won't work.",
      "segmentation": "Semantic Segmentation - Classify EVERY PIXEL in the image into predefined categories. Creates a dense prediction map where each pixel is assigned a class label. Ideal for: medical image analysis (tumor boundaries), autonomous driving (road/sidewalk/vehicle regions), satellite imagery (land use mapping), industrial inspection (defect area measurement). Expected output: pixel-wise class map same size as input. Note: Requires specialized architectures (U-Net, DeepLab) - computationally intensive."
    },
    "training_batch_size_title": "Batch Size",
    "training_batch_size_description": "Number of images processed together in one training step. Larger batches provide more stable gradients and faster training but require more GPU memory. Too small (1-4) causes noisy updates, too large may hurt generalization. Typical values: 16-64 for normal GPUs, 8-16 for larger images or limited memory.",
    "training_epochs_title": "Epochs",
    "training_epochs_description": "How many times the model should iterate through the entire dataset. More epochs can improve learning but too many leads to overfitting. Early stopping automatically halts training when validation performance stops improving. Typical range: 10-50 for small datasets, 50-200 for larger ones.",
    "training_epochs_enum_descriptor": {
      "auto": "Automatic (based on dataset size)"
    },
    "training_learning_rate_title": "Learning Rate",
    "training_learning_rate_description": "How big the weight update steps are at each iteration. The most critical hyperparameter in deep learning. Too high: unstable training, loss doesn't decrease or oscillates. Too low: very slow convergence, getting stuck in local minima. Typical starting values: 0.001-0.01 for Adam, 0.01-0.1 for SGD with momentum.",
    "training_weight_decay_title": "Weight Decay",
    "training_weight_decay_description": "L2 regularization strength that penalizes large weight values to prevent overfitting. Higher values mean stronger regularization. Typical range: 0.0001-0.01. Use higher values (0.01) for small datasets, lower (0.0001) for large datasets. Works best with AdamW optimizer.",
    "training_input_size_title": "Input Size",
    "training_input_size_description": "Image resize dimensions in pixels (width = height). Standard: 224px for most models. Larger sizes (320, 384, 512) capture more detail but require more memory and computation. For EfficientNet-B4+ models, 380-528px is recommended. Larger sizes benefit fine-grained recognition and small objects.",
    "training_normalize_title": "Normalize",
    "training_normalize_description": "Image normalization parameters for standardizing input values. Ensures input values are in a consistent range for stable training. For ImageNet pretrained models, use ImageNet statistics (mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]).",
    "training_normalize_mean_title": "Mean",
    "training_normalize_mean_description": "Per-channel (RGB) mean values for normalization. ImageNet standard values: [0.485, 0.456, 0.406]. For custom datasets, can be computed as the average of all images per channel.",
    "training_normalize_std_title": "Std",
    "training_normalize_std_description": "Per-channel (RGB) standard deviation values for normalization. ImageNet standard values: [0.229, 0.224, 0.225]. For custom datasets, can be computed as the standard deviation of all images per channel.",
    "training_val_ratio_title": "Validation Ratio",
    "training_val_ratio_description": "Fraction of the total dataset to use as validation set (0.0-1.0). The validation set evaluates performance during training and is not used for weight updates. Typical values: 0.1-0.2 (10-20%). Use lower ratios (0.1) for small datasets, higher (0.2) for large datasets.",
    "training_dataloader_title": "Dataloader",
    "training_dataloader_description": "The dataloader configures how images are loaded and prepared for training. Proper settings can significantly speed up training by maximizing GPU utilization.",
    "training_dataloader_num_workers_title": "Num Workers",
    "training_dataloader_num_workers_description": "Number of parallel data loading processes. More workers mean faster data loading but use more CPU and RAM. Recommended: CPU cores / 2, maximum 8-16. Too many workers can cause memory issues. On Windows, 0 may be needed for multiprocessing issues.",
    "training_dataloader_pin_memory_title": "Pin Memory",
    "training_dataloader_pin_memory_description": "Use pinned (page-locked) memory for faster CPU-GPU data transfer. Always enable for GPU training - significantly reduces data transfer time. Only disable for CPU-only training.",
    "training_dataloader_persistent_workers_title": "Persistent Workers",
    "training_dataloader_persistent_workers_description": "Keep worker processes alive between epochs instead of restarting after each epoch. Speeds up training by avoiding restart overhead but uses more memory. Recommended for longer training runs.",
    "training_dataloader_prefetch_factor_title": "Prefetch Factor",
    "training_dataloader_prefetch_factor_description": "How many batches each worker should preload in the background before they're needed. Higher values (2-4) provide smoother data flow but use more memory. Default of 2 works well for most cases.",
    "training_augmentation_title": "Augmentation",
    "training_augmentation_description": "Data augmentation configuration for creating artificial data variation. Augmentation applies random transformations to images (rotation, flipping, color changes) which improves model generalization and reduces overfitting.",
    "training_augmentation_train_title": "Training Augmentations",
    "training_augmentation_train_description": "List of augmentations applied to training data. These random transformations artificially increase data variety. Stronger augmentation recommended for small datasets, weaker or none for large datasets.",
    "training_augmentation_val_title": "Validation Augmentations",
    "training_augmentation_val_description": "Minimal augmentations applied to validation data. Usually only resize and normalize are needed - validation results should be deterministic and reproducible for objective model performance assessment.",
    "training_optimizer_type_title": "Optimizer Type",
    "training_optimizer_type_description": "The optimization algorithm determines how model weights are updated during training. Each optimizer has different characteristics for convergence speed, stability, and memory usage. The right choice depends on your model size, dataset, and training goals.",
    "training_optimizer_type_enum_descriptor": {
      "sgd": "SGD (Stochastic Gradient Descent) - The classic, fundamental optimizer. Updates weights by moving in the direction of negative gradient scaled by learning rate. With momentum, accumulates velocity for faster convergence and smoother updates. Simple, memory-efficient, and well-understood. Best for: large-scale training, when you need reproducibility, fine-tuning pretrained models, ConvNets with proper LR scheduling. Expected results: reliable convergence, often achieves best final accuracy with proper tuning. Requires careful learning rate selection and typically needs LR scheduling.",
      "adam": "Adam (Adaptive Moment Estimation) - Combines momentum (first moment) with RMSprop's adaptive learning rates (second moment). Maintains per-parameter learning rates that adapt based on gradient history. Fast convergence with minimal hyperparameter tuning. Best for: quick prototyping, transformers, NLP tasks, sparse gradients, when you want good results without extensive tuning. Expected results: fast initial convergence, good default performance. May generalize slightly worse than well-tuned SGD on some vision tasks.",
      "adamw": "AdamW (Adam with Decoupled Weight Decay) - Fixes Adam's weight decay implementation by decoupling it from gradient updates. This is the CORRECT way to apply L2 regularization with adaptive optimizers. Recommended over standard Adam for most use cases. Best for: any task where you'd use Adam, especially with weight decay/regularization, transformers, modern architectures. Expected results: better generalization than Adam, more effective regularization, often the best default choice for deep learning.",
      "adamax": "Adamax - Adam variant using infinity norm (max) instead of L2 norm for second moment. More stable with large or sparse gradients. Less sensitive to learning rate choice. Best for: embeddings with large vocabulary, NLP tasks, when Adam shows unstable behavior, sparse or noisy gradients. Expected results: more stable training than Adam in edge cases, similar convergence speed.",
      "nadam": "NAdam (Nesterov-accelerated Adam) - Incorporates Nesterov momentum into Adam, providing look-ahead gradient computation. Combines fast adaptive learning with improved momentum. Best for: tasks benefiting from Nesterov momentum, RNNs, sequence models, when you want faster convergence than Adam. Expected results: slightly faster convergence than Adam, better handling of gradient noise.",
      "radam": "RAdam (Rectified Adam) - Addresses Adam's variance problem in early training by introducing a rectification term. Provides automatic warmup without needing explicit LR warmup schedules. More stable early training. Best for: training from scratch without warmup, when Adam shows erratic early behavior, automated pipelines where manual warmup tuning isn't feasible. Expected results: stable training from step 1, eliminates need for warmup, similar final performance to Adam.",
      "rmsprop": "RMSprop (Root Mean Square Propagation) - Divides learning rate by running average of recent gradient magnitudes. Adapts learning rate per-parameter. Precursor to Adam without momentum component. Best for: RNNs and recurrent architectures, non-stationary objectives, online learning. Expected results: good for recurrent networks, less effective than Adam for most CNN tasks.",
      "rprop": "Rprop (Resilient Propagation) - Uses only the sign of gradients, not magnitude. Each parameter has its own adaptive step size that increases when gradient sign is consistent, decreases on sign changes. Best for: full-batch training only, small datasets that fit in memory, when gradient magnitudes are unreliable. Expected results: fast convergence for full-batch, NOT suitable for mini-batch training (will fail).",
      "adagrad": "Adagrad (Adaptive Gradient) - Accumulates all past squared gradients, giving frequently updated parameters smaller learning rates. Good for sparse features. Best for: sparse data (NLP, recommendations), features with very different frequencies, when some parameters need much smaller updates. Expected results: handles sparse gradients well. Drawback: learning rate continuously decreases, may stop learning too early on dense tasks.",
      "adadelta": "Adadelta - Addresses Adagrad's diminishing learning rate by using windowed accumulated gradients instead of all past gradients. No need to set initial learning rate. Best for: when you don't want to tune learning rate, continuation of Adagrad without learning rate decay. Expected results: more sustained learning than Adagrad, but generally outperformed by Adam family.",
      "sparse_adam": "Sparse Adam - Adam variant optimized for sparse gradient tensors. Only updates parameters that received non-zero gradients. Memory efficient for large embedding tables. Best for: large embedding layers, sparse NLP tasks, recommendation systems with huge item catalogs. Expected results: same as Adam but with better memory efficiency for sparse updates.",
      "lbfgs": "L-BFGS (Limited-memory BFGS) - Second-order optimizer approximating the Hessian matrix using limited memory. Powerful but requires full-batch gradients. Best for: small datasets, neural style transfer, physics-informed networks, when you can compute full-batch gradients. Expected results: very fast convergence when applicable. NOT suitable for mini-batch training or large datasets.",
      "asgd": "ASGD (Averaged SGD) - Maintains running average of parameters during training, using averaged weights for final model. Theoretical convergence guarantees. Best for: convex or nearly-convex problems, when you want theoretical guarantees, simple models. Expected results: good convergence for simpler models, less effective for deep networks compared to Adam/SGD with momentum."
    },
    "training_optimizer_lr_title": "Optimizer LR",
    "training_optimizer_lr_description": "The optimizer's learning rate determining weight update step size. Too high causes unstable training, too low causes slow convergence. Typical values: 0.001 for Adam/AdamW, 0.01-0.1 for SGD with momentum.",
    "training_optimizer_weight_decay_title": "Optimizer Weight Decay",
    "training_optimizer_weight_decay_description": "L2 regularization in the optimizer. Pushes weights toward zero to prevent overfitting. Most effective with AdamW (decoupled weight decay). Typical values: 0.01-0.1 for strong regularization, 0.0001-0.001 for weak.",
    "training_optimizer_params_title": "Optimizer Parameters",
    "training_optimizer_params_description": "Optimizer-specific advanced parameters like momentum, betas, epsilon. These fine-tune optimizer behavior. Default values work well for most cases - only modify if you're an experienced user.",
    "training_scheduler_type_title": "Scheduler Type",
    "training_scheduler_type_description": "Learning rate scheduling controls how the learning rate changes during training. Proper scheduling can dramatically improve convergence, final accuracy, and training stability. Different schedulers suit different training scenarios and model architectures.",
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR - Decreases learning rate by a fixed factor (gamma) every N epochs. Simple and predictable. Best for: standard training schedules, when you know approximately when to reduce LR, baseline experiments. Expected results: steady training with periodic jumps in loss when LR drops, then continued improvement. Typically multiply LR by 0.1 every 30-50 epochs.",
      "multi_step_lr": "Multi-Step LR - Decreases learning rate at specified epoch milestones. More control than Step LR. Best for: when you know exact epochs where LR should drop, reproducing published training schedules, fine-grained control. Expected results: similar to Step LR but with custom timing. Common pattern: drop at 60%, 80% of total epochs.",
      "exponential_lr": "Exponential LR - Multiplies learning rate by gamma every epoch for continuous smooth decay. Best for: gradual fine-tuning, when step-wise drops are too abrupt, long training runs. Expected results: smooth, continuous LR reduction. Requires careful gamma selection (typically 0.95-0.99) to avoid decaying too fast.",
      "cosine_annealing_lr": "Cosine Annealing - Learning rate follows a cosine curve from initial to minimum value. Smooth decay that slows near the end. Very popular for modern training. Best for: fixed-length training, achieving best final accuracy, most CNN training, competition models. Expected results: excellent final accuracy, smooth training. The gentle decay near minimum allows fine convergence.",
      "cosine_annealing_warm_restarts": "Cosine with Warm Restarts - Cosine annealing that periodically resets to initial LR. Each restart explores different loss surface regions. Best for: escaping local minima, ensemble-like single model training, when single cosine plateaus. Expected results: can find better optima than single decay, each restart may improve or explore differently. Good for longer training.",
      "reduce_lr_on_plateau": "Reduce on Plateau - Monitors a metric (usually val_loss) and reduces LR when it stops improving. Adaptive and requires no epoch planning. Best for: when you don't know optimal schedule, variable-length training, when validation loss behavior is unpredictable. Expected results: automatic adaptation to training dynamics. May train longer but finds good LR automatically.",
      "cyclic_lr": "Cyclic LR - Learning rate oscillates between minimum and maximum bounds. Can use triangular, triangular2, or exp_range policies. Best for: escaping local minima, when you want to explore multiple LR ranges, super-convergence experiments. Expected results: periodic loss oscillation, potentially better generalization. Requires careful bound selection.",
      "one_cycle_lr": "One Cycle - Starts low, ramps up to max LR, then anneals down. Enables super-convergence with very high learning rates. Best for: fast training with high LR, achieving good accuracy quickly, when training time is limited. Expected results: fast convergence, often matches or beats longer schedules in less time. Requires proper max LR selection (LR range test recommended).",
      "polynomial_lr": "Polynomial LR - Decays learning rate using polynomial function with specified power. Power=1 is linear, higher powers decay faster initially. Best for: fine-grained control over decay curve shape, matching specific published schedules. Expected results: customizable decay shape. Power=2 (quadratic) gives faster early decay, slower late decay.",
      "linear_lr": "Linear LR - Simple linear interpolation between start and end learning rates. Can implement warmup (increasing) or decay (decreasing). Best for: warmup phases, simple interpretable schedules, combining with other schedulers. Expected results: predictable, steady LR change. Often used for first few epochs as warmup.",
      "lambda_lr": "Lambda LR - User-defined function determines LR at each epoch. Maximum flexibility. Best for: custom schedules, research experiments, when no built-in scheduler fits your needs. Expected results: whatever your function defines. Requires programming custom lambda function.",
      "multiplicative_lr": "Multiplicative LR - Multiplies current LR by a factor returned by user function each epoch. Cumulative effect. Best for: custom decay patterns, when you want schedule to depend on current LR. Expected results: depends on your multiplicative function. Useful for implementing custom adaptive schedules."
    },
    "training_scheduler_step_size_title": "Step Size",
    "training_scheduler_step_size_description": "Number of epochs between learning rate reductions (for StepLR). Smaller values mean more frequent reductions. Typical values: 10-30 epochs. Consider reducing LR first at 1/3 or 1/2 of total training time.",
    "training_scheduler_gamma_title": "Gamma",
    "training_scheduler_gamma_description": "Learning rate multiplication factor at each reduction. Value of 0.1 means 10x reduction (e.g., 0.01 -> 0.001). Typical values: 0.1 for aggressive reduction, 0.5 for gradual. For exponential schedulers, 0.95-0.99 is common.",
    "training_scheduler_params_title": "Scheduler Parameters",
    "training_scheduler_params_description": "Scheduler-specific advanced parameters like T_max, eta_min, milestones. These fine-tune scheduler behavior for the specific scheduler type. Defaults work well for most cases.",
    "training_loss_type_title": "Loss Type",
    "training_loss_type_description": "Loss functions measure how well model predictions match ground truth. The choice of loss function fundamentally shapes what the model learns. Different losses have different gradients, sensitivities, and behaviors. Wrong choice can cause training failure, poor generalization, or biased predictions.",
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Cross Entropy - Standard loss for multi-class classification. Combines log-softmax and NLL loss. Heavily penalizes confident wrong predictions. Best for: image classification, most multi-class problems, mutually exclusive categories. Expected results: well-calibrated probabilities, fast convergence on balanced data. NEGATIVE EFFECTS: Struggles with class imbalance (rare classes get ignored), highly sensitive to label noise (mislabeled samples cause large gradients), can become overconfident on out-of-distribution data.",
      "nll_loss": "NLL Loss (Negative Log Likelihood) - Expects log-probabilities as input (apply log_softmax first). Mathematically equivalent to cross_entropy when combined with log_softmax. Best for: when you need explicit log-softmax for other purposes, custom probability handling. Expected results: same as cross_entropy. NEGATIVE EFFECTS: Same as cross_entropy. Additionally, forgetting log_softmax causes silent failure with wrong gradients.",
      "bce_loss": "BCE Loss (Binary Cross Entropy) - For binary classification or multi-label problems. Expects probabilities (apply sigmoid first). Each output is independent binary decision. Best for: binary classification, multi-label classification where items can have multiple labels. Expected results: independent probability per class, good for multi-label. NEGATIVE EFFECTS: Numerically unstable with extreme predictions (use bce_with_logits instead), sensitive to class imbalance in multi-label settings, requires careful threshold selection for predictions.",
      "bce_with_logits": "BCE with Logits - Numerically stable BCE that combines sigmoid and BCE in one operation. Preferred over separate sigmoid + BCE. Best for: all cases where you'd use BCE loss, binary and multi-label classification. Expected results: stable training, same results as BCE but more robust. NEGATIVE EFFECTS: Same conceptual issues as BCE (class imbalance, threshold selection), but numerically stable. Still struggles with extreme positive/negative imbalance.",
      "multi_margin": "Multi-Margin Loss - Hinge-based loss for multi-class SVM-style classification. Creates margin between correct class and others. Best for: SVM-style classifiers, when you want maximum margin separation, robust classification. Expected results: larger margins between classes, potentially more robust to outliers than cross_entropy. NEGATIVE EFFECTS: Harder to tune margin parameter, doesn't produce calibrated probabilities (outputs aren't interpretable as confidence), may converge slower than cross_entropy.",
      "multi_label_margin": "Multi-Label Margin Loss - Margin-based loss for multi-label classification. Ensures positive labels score higher than negative labels by a margin. Best for: multi-label ranking where relative ordering matters, multi-label SVMs. Expected results: positive labels consistently score above negatives. NEGATIVE EFFECTS: Doesn't provide calibrated probabilities, sensitive to margin hyperparameter, can struggle when positive and negative labels have very different frequencies.",
      "multi_label_soft_margin": "Multi-Label Soft Margin - Sigmoid cross-entropy for multi-label, treating each label as independent binary. Essentially BCE across all labels. Best for: multi-label classification, standard choice for multi-label problems. Expected results: similar to BCE, handles multiple labels well. NEGATIVE EFFECTS: Each label trained independently ignores label correlations, imbalanced labels can dominate training, may predict incoherent label combinations.",
      "mse_loss": "MSE Loss (Mean Squared Error) - Squared difference between predictions and targets. Standard regression loss. Penalizes large errors heavily (quadratic). Best for: regression tasks, predicting continuous values, when errors are Gaussian distributed. Expected results: predictions centered on mean, sensitive to outliers. NEGATIVE EFFECTS: Very sensitive to outliers (single outlier can dominate loss), tends to predict mean values in ambiguous cases, can cause gradient explosion with large errors early in training.",
      "l1_loss": "L1 Loss (Mean Absolute Error) - Absolute difference between predictions and targets. Linear penalty, more robust to outliers than MSE. Best for: regression with outliers, when you want median predictions, robust estimation. Expected results: predictions closer to median, less affected by outliers. NEGATIVE EFFECTS: Non-smooth gradient at zero (discontinuity), can cause unstable updates near correct predictions, slower convergence than MSE on clean data.",
      "smooth_l1": "Smooth L1 Loss - Combines L1 and L2: quadratic when error is small, linear when large. Best of both worlds. Best for: object detection (bounding box regression), robust regression, when you want MSE benefits without outlier sensitivity. Expected results: stable training, robust to outliers, smooth gradients. NEGATIVE EFFECTS: Requires tuning beta parameter for transition point, still somewhat sensitive to outliers (just less than MSE), adds hyperparameter complexity.",
      "huber_loss": "Huber Loss - Similar to Smooth L1. Quadratic for small errors, linear for large. Robust regression standard. Best for: regression with potential outliers, financial predictions, any regression where outliers are possible. Expected results: robust predictions, smooth convergence. NEGATIVE EFFECTS: Delta parameter requires tuning for your error scale, wrong delta makes it equivalent to pure L1 or MSE negating benefits.",
      "kl_div": "KL Divergence (Kullback-Leibler) - Measures how one probability distribution diverges from another. Used for distribution matching. Best for: knowledge distillation, VAE regularization, matching output to target distribution. Expected results: output distribution approaches target distribution. NEGATIVE EFFECTS: Asymmetric (KL(P||Q) != KL(Q||P)), infinite when target has zeros where prediction doesn't, requires careful log-probability handling, mode-seeking behavior can miss multimodal targets.",
      "margin_ranking": "Margin Ranking Loss - Learns to rank pairs: x1 should be ranked higher than x2 when y=1. Margin-based pairwise ranking. Best for: learning to rank, recommendation systems, when relative ordering matters more than absolute values. Expected results: correctly ordered pairs with margin separation. NEGATIVE EFFECTS: Requires pair generation (quadratic complexity), doesn't directly optimize list-wise ranking metrics, margin parameter needs tuning, sensitive to pair sampling strategy.",
      "hinge_embedding": "Hinge Embedding Loss - For learning embeddings where similar items should be close, dissimilar items far apart. Uses hinge loss formulation. Best for: embedding learning, similarity learning, metric learning with pairs. Expected results: meaningful embedding space with margin separation. NEGATIVE EFFECTS: Hard to tune margin, can collapse embeddings if margin too large, requires careful positive/negative pair mining.",
      "triplet_margin": "Triplet Margin Loss - Takes anchor, positive, and negative samples. Learns embeddings where anchor is closer to positive than negative by margin. Best for: face recognition, image retrieval, fine-grained similarity. Expected results: strong embedding space for similarity search. NEGATIVE EFFECTS: Requires expensive triplet mining, many triplets become uninformative (too easy), training can be slow, margin and mining strategy critical for success, may need hard negative mining.",
      "cosine_embedding": "Cosine Embedding Loss - Learns embeddings using cosine similarity. Similar pairs should have high cosine similarity, dissimilar pairs low. Best for: text embeddings, when magnitude shouldn't matter, semantic similarity tasks. Expected results: direction-based similarity in embedding space. NEGATIVE EFFECTS: Ignores magnitude information (sometimes important), sensitive to initialization, can struggle with fine-grained distinctions.",
      "ctc_loss": "CTC Loss (Connectionist Temporal Classification) - For sequence-to-sequence where alignment is unknown. Sums over all valid alignments. Best for: speech recognition, OCR, handwriting recognition, any sequence task without explicit alignment. Expected results: learns alignment implicitly, handles variable length. NEGATIVE EFFECTS: Assumes output sequence shorter than input, computationally expensive, can struggle with long sequences, requires blank token handling, peaky output distributions can cause recognition errors.",
      "poisson_nll": "Poisson NLL Loss - Negative log likelihood assuming Poisson-distributed targets. For non-negative integer count data. Best for: event counting, rate estimation, any non-negative count prediction. Expected results: appropriate for count data, handles zero-inflation better than MSE. NEGATIVE EFFECTS: Assumes variance equals mean (real data often overdispersed), requires positive predictions (use exp transform), not suitable for continuous targets.",
      "gaussian_nll": "Gaussian NLL Loss - NLL assuming Gaussian targets. Model predicts both mean and variance, enabling uncertainty estimation. Best for: regression with uncertainty, confidence intervals, heteroscedastic regression. Expected results: calibrated uncertainty estimates alongside predictions. NEGATIVE EFFECTS: Can predict high variance to avoid penalty (variance collapse), requires careful initialization of variance head, doubles output size, training can be unstable if variance predictions poorly initialized."
    },
    "training_loss_reduction_title": "Reduction",
    "training_loss_reduction_description": "How to aggregate individual losses within a batch. Mean: average over batch providing normalized gradients. Sum: sums all losses scaling with batch size. None: keeps individual losses for special cases.",
    "training_loss_reduction_enum_descriptor": {
      "mean": "Mean - Average over batch (recommended, batch-size independent gradients)",
      "sum": "Sum - Sum over batch (larger batch = larger gradient)",
      "none": "None - No reduction (keep individual losses)"
    },
    "training_loss_params_title": "Loss Parameters",
    "training_loss_params_description": "Loss function-specific advanced parameters like class weights, margin values, smoothing factors. These fine-tune loss function behavior for special cases.",
    "training_weight_init_title": "Weight Initialization",
    "training_weight_init_description": "How to initialize neural network weights before training. Proper initialization is critical for stable training - bad initialization can cause vanishing or exploding gradients. For pretrained models, this only affects newly added layers.",
    "training_weight_init_type_title": "Type",
    "training_weight_init_params_title": "Init Parameters",
    "training_weight_init_params_description": "Initialization method-specific parameters like gain, std, sparsity. These fine-tune initialization behavior. Defaults generally work well.",
    "weight_init_type_enum_descriptor": {
      "kaiming_normal": "Kaiming Normal - He initialization (normal)",
      "kaiming_uniform": "Kaiming Uniform - He initialization (uniform)",
      "xavier_normal": "Xavier Normal - Glorot initialization (normal)",
      "xavier_uniform": "Xavier Uniform - Glorot initialization (uniform)",
      "normal": "Normal - Gaussian initialization",
      "uniform": "Uniform - Uniform distribution",
      "trunc_normal": "Truncated Normal - Bounded Gaussian",
      "orthogonal": "Orthogonal - Orthogonal matrix",
      "sparse": "Sparse - Sparse initialization",
      "constant": "Constant - Fixed value",
      "zeros": "Zeros - All zeros",
      "ones": "Ones - All ones",
      "eye": "Eye - Identity matrix",
      "dirac": "Dirac - Delta function"
    },
    "training_checkpoint_title": "Checkpoint",
    "training_checkpoint_description": "Model checkpoint configuration for preserving training progress. Checkpoints allow resuming training after interruption and preserve the best model based on validation performance.",
    "training_checkpoint_save_best_only_title": "Save Best Only",
    "training_checkpoint_save_frequency_title": "Save Frequency",
    "training_checkpoint_best_model_filename_title": "Best Model Filename",
    "training_checkpoint_model_filename_title": "Model Filename",
    "training_early_stopping_title": "Early Stopping",
    "training_early_stopping_description": "Automatic training halt configuration when validation performance stops improving. Prevents overfitting and saves unnecessary computation time. The system monitors the selected metric and stops training if no improvement for patience epochs.",
    "training_early_stopping_enabled_title": "Enabled",
    "training_early_stopping_patience_title": "Patience",
    "training_early_stopping_min_delta_title": "Min Delta",
    "training_early_stopping_monitor_title": "Monitor",
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Validation Loss",
      "val_accuracy": "Validation Accuracy"
    },
    "training_gradient_title": "Gradient",
    "training_gradient_description": "Gradient handling settings for improved training stability. Gradient clipping prevents exploding gradients, accumulation enables larger effective batch sizes with limited memory.",
    "training_gradient_clip_norm_title": "Clip Norm",
    "training_gradient_clip_value_title": "Clip Value",
    "training_gradient_accumulation_steps_title": "Accumulation Steps",
    "training_runtime_title": "Runtime",
    "training_runtime_description": "Runtime optimization settings for speeding up training and more efficient GPU memory usage. These settings are hardware-specific and can significantly affect training speed.",
    "training_runtime_mixed_precision_title": "Mixed Precision",
    "training_runtime_mixed_precision_description": "Enable FP16/FP32 mixed precision training. Significantly reduces GPU memory usage and speeds up training on modern GPUs (Volta/Turing/Ampere and newer). Expect ~1.5-2x speedup and ~50% memory reduction. Highly recommended.",
    "training_runtime_channels_last_title": "Channels Last",
    "training_runtime_channels_last_description": "Use channels-last (NHWC) memory format instead of standard channels-first (NCHW). Can result in faster convolution operations on modern GPUs (Volta+). Native format for NVIDIA GPUs providing better tensor core utilization.",
    "training_runtime_allow_tf32_title": "Allow TF32",
    "training_runtime_allow_tf32_description": "Enable TensorFloat-32 precision on NVIDIA Ampere and newer GPUs (RTX 30xx, A100, etc.). Provides faster matrix operations while maintaining nearly full FP32 precision. Highly recommended on compatible hardware.",
    "training_runtime_cudnn_benchmark_title": "cuDNN Benchmark",
    "training_runtime_cudnn_benchmark_description": "Enable cuDNN automatic convolution algorithm selection. Tests different algorithms in the first few iterations, then uses the fastest. Speeds up training with fixed input sizes. Disable for variable-sized inputs.",
    "dataset_description": "Dataset configuration",
    "dataset_image_extensions_description": "Supported image file extensions",
    "dataset_discovery_description": "Dataset discovery settings",
    "dataset_discovery_annotation_formats_description": "Annotation format detection",
    "dataset_discovery_txt_annotations_description": "Text annotation detection",
    "dataset_discovery_txt_annotations_min_coverage_percent_description": "Minimum coverage for text annotations",
    "dataset_discovery_balance_analysis_description": "Dataset balance analysis",
    "dataset_discovery_balance_analysis_min_images_per_class_description": "Minimum images per class",
    "dataset_discovery_balance_analysis_critical_shortage_threshold_description": "Critical shortage threshold",
    "dataset_discovery_balance_analysis_over_representation_ratio_description": "Over-representation ratio",
    "dataset_discovery_balance_analysis_under_representation_ratio_description": "Under-representation ratio",
    "dataset_discovery_balance_analysis_severe_over_representation_ratio_description": "Severe over-representation ratio",
    "dataset_discovery_balance_analysis_hierarchical_balance_threshold_description": "Hierarchical balance threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_description": "Dataset size warning thresholds",
    "dataset_discovery_balance_analysis_dataset_size_warnings_tiny_dataset_description": "Tiny dataset warning threshold",
    "dataset_discovery_balance_analysis_dataset_size_warnings_small_dataset_description": "Small dataset warning threshold",
    "dataset_discovery_balance_analysis_balance_score_thresholds_description": "Balance score classification thresholds",
    "dataset_discovery_balance_analysis_balance_score_thresholds_excellent_description": "Excellent balance threshold (95%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_very_good_description": "Very good balance threshold (85%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_good_description": "Good balance threshold (75%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_decent_description": "Decent balance threshold (65%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_fair_description": "Fair balance threshold (55%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_mediocre_description": "Mediocre balance threshold (45%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_poor_description": "Poor balance threshold (35%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_bad_description": "Bad balance threshold (25%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_terrible_description": "Terrible balance threshold (15%+)",
    "dataset_discovery_balance_analysis_balance_score_thresholds_nonsense_description": "Nonsense balance threshold (0%+)",
    "optimizers_description": "Optimizer configurations",
    "optimizers_defaults_description": "Default optimizer parameters",
    "optimizers_defaults_lbfgs_line_search_fn_enum_descriptor": {
      "strong_wolfe": "Strong Wolfe conditions"
    },
    "schedulers_description": "Learning rate scheduler configurations",
    "schedulers_defaults_description": "Default scheduler parameters",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Lambda function for LR calculation",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Lambda function for multiplicative LR",
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Minimize metric",
      "max": "Maximize metric"
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Relative threshold",
      "abs": "Absolute threshold"
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Triangular - Linear oscillation",
      "triangular2": "Triangular2 - Halving amplitude",
      "exp_range": "Exp Range - Exponential decay"
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Per cycle",
      "iterations": "Per iteration"
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Cosine annealing",
      "linear": "Linear annealing"
    },
    "losses_description": "Loss function configurations",
    "losses_defaults_description": "Default loss function parameters",
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "L1 norm",
      "2": "L2 norm"
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "No reduction",
      "mean": "Mean reduction",
      "sum": "Sum reduction",
      "batchmean": "Batch mean reduction"
    },
    "models_description": "Model architecture configurations",
    "models_resnet_description": "ResNet model family",
    "models_resnet_variants_description": "Available ResNet variants",
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW optimizer",
      "adam": "Adam optimizer",
      "sgd": "SGD optimizer"
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Step LR scheduler",
      "cosine_annealing_lr": "Cosine annealing scheduler",
      "reduce_lr_on_plateau": "Reduce on plateau scheduler"
    },
    "models_resnext_description": "ResNeXt model family",
    "models_resnext_variants_description": "Available ResNeXt variants",
    "models_alexnet_description": "AlexNet model family",
    "models_alexnet_variants_description": "Available AlexNet variants",
    "models_vgg_description": "VGG model family",
    "models_vgg_variants_description": "Available VGG variants",
    "models_densenet_description": "DenseNet model family",
    "models_densenet_variants_description": "Available DenseNet variants",
    "models_mobilenet_description": "MobileNet model family",
    "models_mobilenet_variants_description": "Available MobileNet variants",
    "models_shufflenet_description": "ShuffleNet model family",
    "models_shufflenet_variants_description": "Available ShuffleNet variants",
    "models_squeezenet_description": "SqueezeNet model family",
    "models_squeezenet_variants_description": "Available SqueezeNet variants",
    "models_efficientnet_description": "EfficientNet model family",
    "models_efficientnet_variants_description": "Available EfficientNet variants",
    "models_supported_types_description": "List of supported model architectures",
    "activations_description": "Activation function configurations",
    "activations_defaults_description": "Default activation parameters",
    "activations_properties_description": "Activation function properties",
    "activations_defaults_leaky_relu_description": "Leaky ReLU activation",
    "activations_defaults_leaky_relu_negative_slope_description": "Slope for negative values",
    "activations_defaults_prelu_description": "PReLU activation",
    "activations_defaults_prelu_num_parameters_description": "Number of learnable parameters",
    "activations_defaults_prelu_init_description": "Initial slope value",
    "activations_defaults_elu_description": "ELU activation",
    "activations_defaults_elu_alpha_description": "Alpha value for negative inputs",
    "activations_defaults_celu_description": "CELU activation",
    "activations_defaults_celu_alpha_description": "Alpha value",
    "activations_defaults_hardtanh_description": "Hardtanh activation",
    "activations_defaults_hardtanh_min_val_description": "Minimum output value",
    "activations_defaults_hardtanh_max_val_description": "Maximum output value",
    "activations_defaults_hardshrink_description": "Hard shrink activation",
    "activations_defaults_hardshrink_lambd_description": "Lambda threshold",
    "activations_defaults_softshrink_description": "Soft shrink activation",
    "activations_defaults_softshrink_lambd_description": "Lambda threshold",
    "activations_defaults_threshold_description": "Threshold activation",
    "activations_defaults_threshold_threshold_description": "Threshold value",
    "activations_defaults_threshold_value_description": "Replacement value below threshold",
    "activations_defaults_softplus_description": "Softplus activation",
    "activations_defaults_softplus_beta_description": "Beta scaling factor",
    "activations_defaults_softplus_threshold_description": "Linear transition threshold",
    "augmentations_description": "Data augmentation configurations",
    "augmentations_defaults_description": "Default augmentation parameters",
    "augmentations_properties_description": "Augmentation properties",
    "augmentations_defaults_random_crop_description": "Random crop augmentation",
    "augmentations_defaults_random_crop_size_description": "Output crop size",
    "augmentations_defaults_random_crop_padding_description": "Padding before crop",
    "augmentations_defaults_random_crop_pad_if_needed_description": "Pad if smaller than crop size",
    "augmentations_defaults_random_crop_fill_description": "Fill value for padding",
    "augmentations_defaults_random_crop_padding_mode_description": "Padding mode",
    "augmentations_defaults_random_resized_crop_description": "Random resized crop augmentation",
    "augmentations_defaults_random_resized_crop_size_description": "Output size",
    "augmentations_defaults_random_resized_crop_scale_description": "Scale range (min, max)",
    "augmentations_defaults_random_resized_crop_ratio_description": "Aspect ratio range",
    "augmentations_defaults_random_resized_crop_interpolation_description": "Interpolation method",
    "augmentations_defaults_center_crop_description": "Center crop augmentation",
    "augmentations_defaults_center_crop_size_description": "Output crop size",
    "augmentations_defaults_random_horizontal_flip_description": "Random horizontal flip",
    "augmentations_defaults_random_horizontal_flip_p_description": "Flip probability",
    "augmentations_defaults_random_vertical_flip_description": "Random vertical flip",
    "augmentations_defaults_random_vertical_flip_p_description": "Flip probability",
    "augmentations_defaults_random_rotation_description": "Random rotation augmentation",
    "augmentations_defaults_random_rotation_degrees_description": "Rotation range in degrees",
    "augmentations_defaults_random_rotation_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_rotation_expand_description": "Expand to fit rotated image",
    "augmentations_defaults_random_rotation_fill_description": "Fill value for empty areas",
    "augmentations_defaults_color_jitter_description": "Color jitter augmentation",
    "augmentations_defaults_color_jitter_brightness_description": "Brightness variation",
    "augmentations_defaults_color_jitter_contrast_description": "Contrast variation",
    "augmentations_defaults_color_jitter_saturation_description": "Saturation variation",
    "augmentations_defaults_color_jitter_hue_description": "Hue variation",
    "augmentations_defaults_random_grayscale_description": "Random grayscale conversion",
    "augmentations_defaults_random_grayscale_p_description": "Conversion probability",
    "augmentations_defaults_random_erasing_description": "Random erasing augmentation",
    "augmentations_defaults_random_erasing_p_description": "Erasing probability",
    "augmentations_defaults_random_erasing_scale_description": "Erased area scale range",
    "augmentations_defaults_random_erasing_ratio_description": "Erased area aspect ratio",
    "augmentations_defaults_random_erasing_value_description": "Fill value for erased area",
    "augmentations_defaults_random_erasing_inplace_description": "Erase in-place",
    "augmentations_defaults_normalize_description": "Normalize augmentation",
    "augmentations_defaults_normalize_mean_description": "Mean values (RGB)",
    "augmentations_defaults_normalize_std_description": "Standard deviation (RGB)",
    "augmentations_defaults_random_invert_description": "Random color inversion",
    "augmentations_defaults_random_invert_p_description": "Inversion probability",
    "augmentations_defaults_random_posterize_description": "Random posterize",
    "augmentations_defaults_random_posterize_bits_description": "Bits per channel",
    "augmentations_defaults_random_posterize_p_description": "Posterize probability",
    "augmentations_defaults_random_solarize_description": "Random solarize",
    "augmentations_defaults_random_solarize_threshold_description": "Solarize threshold",
    "augmentations_defaults_random_solarize_p_description": "Solarize probability",
    "augmentations_defaults_random_adjust_sharpness_description": "Random sharpness adjustment",
    "augmentations_defaults_random_adjust_sharpness_sharpness_factor_description": "Sharpness factor",
    "augmentations_defaults_random_adjust_sharpness_p_description": "Adjustment probability",
    "augmentations_defaults_random_autocontrast_description": "Random auto contrast",
    "augmentations_defaults_random_autocontrast_p_description": "Auto contrast probability",
    "augmentations_defaults_random_equalize_description": "Random histogram equalization",
    "augmentations_defaults_random_equalize_p_description": "Equalization probability",
    "augmentations_defaults_random_perspective_description": "Random perspective transform",
    "augmentations_defaults_random_perspective_distortion_scale_description": "Distortion scale",
    "augmentations_defaults_random_perspective_p_description": "Transform probability",
    "augmentations_defaults_random_perspective_interpolation_description": "Interpolation method",
    "augmentations_defaults_random_perspective_fill_description": "Fill value for empty areas",
    "regularization_description": "Regularization configurations",
    "regularization_defaults_description": "Default regularization parameters",
    "regularization_properties_description": "Regularization properties",
    "normalization_description": "Normalization layer configurations",
    "normalization_defaults_description": "Default normalization parameters",
    "normalization_properties_description": "Normalization properties",
    "pooling_description": "Pooling layer configurations",
    "pooling_defaults_description": "Default pooling parameters",
    "pooling_properties_description": "Pooling properties"
  },
  "presets": {
    "binary_multiclass_classification": {
      "name": "Binary/Multi-class Classification",
      "description": "Classic single-label classification for mutually exclusive classes. Works best when each image clearly belongs to one category and you need clean decisions. Fits everyday splits like cat vs dog, pass/fail quality checks, or picking a catalog category. Start with ResNet or EfficientNet for balanced accuracy; drop to MobileNet/ShuffleNet/SqueezeNet when latency or model size dominates. Outputs one label with a calibrated confidence that you can threshold or display."
    },
    "alexnet_classic_baseline": {
      "name": "AlexNet Classic Baseline",
      "description": "Simple starter network that trains fast on modest hardware. Good for quick baselines, education, or shallow problems where heavy depth is unnecessary. Handles clean, low-resolution datasets without fuss. Move to deeper backbones if you need fine detail, tricky textures, or tight accuracy targets. Keeps experiments lightweight when you want results in minutes."
    },
    "vgg_feature_rich": {
      "name": "VGG Feature-Rich",
      "description": "Deep 3x3 convolution stacks that keep texture and shape detail intact. Shines when you care about visual richness more than latency or file size. Expect solid accuracy on detail-sensitive classes but slower training and higher VRAM use. Regularization and early stopping matter because the parameter count is high. Use it when you want straightforward blocks and can tolerate slower runs. Prefer batch-norm variants for stability on real-world data."
    },
    "densenet_compact_accuracy": {
      "name": "DenseNet Compact Accuracy",
      "description": "Feature sharing between layers gives strong accuracy per parameter. Works well for single- or multi-label tasks when you want compact models without losing detail. Better than ultra-light backbones on small or attribute-heavy datasets with only moderate compute cost. Keep batch sizes reasonable to keep batch-norm stable, especially at higher resolutions. Watch VRAM if you push input size or depth."
    },
    "wide_resnet_balanced_depth": {
      "name": "Wide ResNet Balanced Depth",
      "description": "Widened residual blocks add capacity without changing the familiar ResNet training behavior. Helps when standard ResNet50 plateaus on larger or trickier datasets. Delivers steadier gains than jumping to very deep variants while staying manageable to tune. Needs more VRAM and compute than plain ResNet50, so adjust batch size or resolution if you hit limits. A good middle ground when you need extra capacity without architectural surprises."
    },
    "confidence_based_classification": {
      "name": "Confidence-based Classification",
      "description": "Single-label classifier tuned for reliable confidence scores, not just top-1 accuracy. Use it when thresholds and human review rules depend on the probability being honest. Favor EfficientNet or well-regularized ResNet/ResNeXt to keep calibration tight; avoid the lightest models for safety-critical flows. Pair with validation that checks reliability curves, not only accuracy. Ideal for medical triage, quality gates, fraud flags, and any workflow where 90% confidence should mean roughly 90% correct."
    },
    "content_moderation_safety": {
      "name": "Content Moderation / Safety",
      "description": "Multi-label safety gate that flags policy violations and risky content in one pass. Handles adult content, violence, hate symbols, spam, and other trust-and-safety signals together. Train on diverse, up-to-date examples and keep thresholds conservative to minimize misses. EfficientNet or ResNet/ResNeXt catch nuance; MobileNet covers real-time scanning at scale. Plan for human review on edge cases and refresh the model as adversaries adapt."
    },
    "document_classification": {
      "name": "Document / Text Image Classification",
      "description": "Targets documents, forms, and text-heavy images where layout and fine edges matter. Classifies types like invoices, receipts, IDs, passports, or forms without needing full OCR. Works with standard 224px crops for most workflows, scaling up only when tiny text drives accuracy. ResNet is a steady baseline; EfficientNet helps when you need sharper text detail; MobileNet keeps throughput high for bulk ingestion. Great as a routing step before OCR or downstream automation."
    },
    "efficientnet_balanced_scaling": {
      "name": "EfficientNet Family - Balanced Scaling",
      "description": "Compound scaling keeps depth, width, and resolution in sync for efficient accuracy gains. Choose B0-B1 for balanced starts, B4-B7 when accuracy dominates, and V2 when you need faster training. Excels on medical images, fine-grained categories, scientific and satellite imagery, and any case where small details decide the label. Demands more VRAM as you climb the scale, so size inputs sensibly. A dependable choice when you want top-tier accuracy without hand-tuning a backbone."
    },
    "fast_mobile_inference": {
      "name": "Fast Mobile Inference",
      "description": "Prioritizes latency, model size, and power use over peak accuracy. Built for phones, edge devices, browsers, and batch jobs that need millions of fast calls. MobileNetV3 is the sweet spot; ShuffleNet is faster; SqueezeNet wins on footprint. Expect 3-10x speedups versus heavy backbones and plan to trade a few points of accuracy. Perfect when sub-10ms responses or tiny binaries matter more than squeezing the last percent."
    },
    "feature_extraction_embedding": {
      "name": "Feature Extraction / Embedding Learning",
      "description": "Trains for embeddings instead of direct labels so similar images land near each other in vector space. Ideal for search, deduplication, recommendations, and retrieval pipelines. ResNet50 is a proven baseline; EfficientNet and ResNeXt give richer, compact features. Export features from the penultimate layer and compare with cosine or L2, then index with FAISS/Annoy for speed. Delivers fixed-size vectors you can reuse across products without re-labeling."
    },
    "embedding_retrieval_balanced": {
      "name": "Embedding Retrieval - Balanced",
      "description": "Balanced embedding retriever that targets solid recall and precision without needing heavy hardware. Builds dense vectors for semantic search, deduplication, similarity scoring, and cold-start recommendations. Works with moderate inputs and ResNet/ResNeXt/EfficientNet backbones so you can scale from prototype to production. Use cosine or L2 distance with FAISS/Annoy or a vector database, and keep labels consistent even as classes evolve. A safe default when you want reusable embeddings that behave predictably across product lines."
    },
    "embedding_retrieval_high_fidelity": {
      "name": "Embedding Retrieval - High Fidelity",
      "description": "Optimized for high-detail embeddings when small visual cues decide a match. Uses larger inputs and stronger EfficientNet/ResNeXt/ResNet stacks to preserve texture and structure in the vector space. Fits retrieval for fine-grained items, quality inspection, art or media archives, and scientific imagery. Expect higher VRAM use and longer training; trim batch size before lowering resolution to keep the signal. Pair with cosine or L2 search plus tight validation to ensure vectors stay sharp over time."
    },
    "finegrained_recognition": {
      "name": "Fine-grained Visual Recognition",
      "description": "Focuses on subtle differences within large families of similar classes. Larger inputs and deeper backbones capture the texture, color, and shape cues that separate close siblings. EfficientNet-B4+ or ResNeXt/ResNet101+ are safer picks; avoid ultra-light models that smooth away detail. Curate high-quality labels and keep resolution higher (380-528px) to preserve nuance. Expect better breed/species/variant recall when the dataset is clean and well lit."
    },
    "highres_detail_preservation": {
      "name": "High Resolution / Detail Preservation",
      "description": "Keeps input resolution high so tiny defects and textures survive preprocessing. Great for inspection, satellite work, artwork checks, and any job where 224px hides the evidence. Needs more VRAM and may require trimming batch sizes to fit. EfficientNet-B4/B5/B6 or detail-friendly ResNeXt variants benefit most from the extra pixels. Use when fine detail truly decides correctness."
    },
    "image_quality_assessment": {
      "name": "Image Quality Assessment",
      "description": "Rates technical and perceptual quality rather than content. Spots blur, noise, exposure issues, and can score aesthetic appeal for curation. Medium input sizes are enough; EfficientNet leads, with ResNet and MobileNet covering throughput needs. Useful for auto-culling photo sets, camera testing, print-worthiness checks, or social feed optimization. Outputs quality grades or scores you can threshold for keep/reject pipelines."
    },
    "medical_scientific_analysis": {
      "name": "Medical / Scientific Image Analysis",
      "description": "Designed for high-stakes imaging where correctness matters more than speed. Use conservative training, heavy validation, and domain expert review before deployment. EfficientNet and ResNeXt capture complex patterns; ResNet remains a trustworthy backbone. Treat outputs as decision support with clear uncertainty handling, not as an autonomous diagnosis. Demands rigorous testing on in-distribution data before clinical or scientific use."
    },
    "mobilenet_edge_deployment": {
      "name": "MobileNet Family - Edge Deployment",
      "description": "MobileNet V2/V3 tuned for edge devices with depthwise separable convolutions and inverted residuals. Excellent for real-time apps on phones, tablets, cameras, robots, or IoT boxes. Pick V3-Small for ultra-low latency and V3-Large when you need a little more accuracy without breaking budgets. Expect 3-5x faster inference than ResNet50 at roughly 75-80% of its accuracy. Ideal when battery, thermal limits, and bandwidth make heavier models impractical."
    },
    "multi_object_detection": {
      "name": "Scene Classification / Object Presence",
      "description": "Image-level scene and presence classifier without bounding boxes. Answers what is in the frame or what type of scene it is, not where things are. Faster and simpler than full detection when you just need tags like contains_car or indoor/office. ResNet or EfficientNet handle scene variety well, and lighter backbones still work for throughput-heavy jobs. Great as a precursor to downstream workflows that only need coarse context."
    },
    "multi_object_multi_attribute": {
      "name": "Complex Multi-attribute Analysis",
      "description": "Predicts many attributes at once so a single pass yields a rich property set. Learns relationships between labels for catalogs, people analysis, vehicles, real estate, and fashion. ResNeXt is strong for attribute interactions; EfficientNet and ResNet balance capacity and cost. Depth helps when you juggle dozens of outputs, so size the backbone to your attribute count. Returns a dictionary of attributes with confidences you can threshold per field."
    },
    "multilabel_boolean": {
      "name": "Multi-label Classification (Boolean Tags)",
      "description": "Multi-label setup that returns plain yes/no tags for each attribute. Simple when you only care about presence or absence and want fixed thresholds. Works across tagging, moderation flags, product traits, document properties, or medical indicators. Any backbone fits; choose ResNet/EfficientNet for accuracy or the lighter trio for speed and size. Produces clean booleans like has_people=true, night=false that are easy to wire into rules."
    },
    "multilabel_probability": {
      "name": "Multi-label Classification (Probability Scores)",
      "description": "Multi-label variant that keeps probabilities for each tag instead of hard booleans. Lets you tune thresholds per label or rank tags by relevance. Handy for recommendations, emotions, aesthetics, hashtags, or multi-attribute scoring in commerce. Accuracy and calibration improve with ResNet/ResNeXt/EfficientNet; lighter models trade that for speed. Outputs dictionaries of scores you can post-process to fit your product logic."
    },
    "resnet_deep_learning": {
      "name": "ResNet Family - Deep Feature Learning",
      "description": "Standard ResNet family with residual skip connections that make deep training stable. Excellent balance of accuracy, speed, and maturity with plentiful pretrained weights. Start with ResNet50; drop to 18/34 for latency budgets or climb to 101/152 when accuracy rules. Fits general classification, transfer learning, vision search, inspection, and medical imaging alike. A dependable baseline when you want predictable behavior and broad community support."
    },
    "resnext_aggregated_networks": {
      "name": "ResNeXt Family - Aggregated Residual Networks",
      "description": "ResNeXt adds grouped convolutions and cardinality to the ResNet recipe for richer features. Delivers a small accuracy bump over equivalent ResNets with similar compute. Great for fine-grained, multi-attribute, fashion, vehicle, food, or texture-heavy tasks. Use 32x4d for the best balance; larger cardinalities push accuracy further if you can afford the cost. Choose it when plain ResNet flattens out but you want familiar training dynamics."
    },
    "shufflenet_lightweight_speed": {
      "name": "ShuffleNet - Lightweight Speed Champion",
      "description": "ShuffleNet V2 focuses on channel shuffle efficiency to minimize latency on CPUs and small GPUs. Ideal for robotics, wearables, smart cameras, and any edge job with strict budgets. Variants from x0.5 to x2.0 let you dial capacity without losing speed. Expect blazing inference but weaker fine-detail accuracy, so avoid it for high-stakes or nuance-heavy tasks. Pair with strong augmentation and early stopping on tiny datasets.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smallest ShuffleNet variant. High risk of overfitting - early stopping essential.",
        "large": "Large datasets (10k-30k images). Reduced regularization, larger ShuffleNet variants viable. Higher learning rates and bigger batches effective.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Larger ShuffleNet variants train effectively.",
        "massive": "Massive datasets (50k-100k images). Fast learning rates, large batches, minimal regularization. ShuffleNet x1.5 or x2.0 recommended.",
        "giant": "Giant datasets (100k+ images). Maximum ShuffleNet capacity (x2.0). Can train from scratch or fine-tune with aggressive settings."
      }
    },
    "small_dataset_fewshot": {
      "name": "Small Dataset / Few-shot Learning",
      "description": "Tuned for scarce data, leaning on pretrained weights, regularization, and careful schedules. Great for prototypes, rare classes, niche corporate sets, or early-stage products. ResNet18/34 and EfficientNet-B0/B1 are safe bets; MobileNetV3-Small helps if speed matters. Keep augmentation strong and monitor overfitting closely because variance is high. Aim for reasonable accuracy first, then collect more data to climb higher."
    },
    "squeezenet_ultra_compact": {
      "name": "SqueezeNet - Ultra Compact Networks",
      "description": "Extremely compact Fire-module network for environments where every megabyte counts. Trades accuracy for tiny models that run on microcontrollers, FPGAs, and bandwidth-limited devices. Good when you must stay under tight storage or power ceilings and still need basic vision. Regularization and early stopping are important because capacity is limited. Not a fit for complex tasks or very large datasetsconsider bigger backbones if accuracy matters.",
      "configs": {
        "tiny": "Very small datasets (50-500 images). Heavy regularization essential for this already-compact architecture. Early stopping critical to prevent overfitting.",
        "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained Fire modules. SqueezeNet learns efficiently from limited data.",
        "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters work well. SqueezeNet's compact size allows larger batch sizes.",
        "large": "Large datasets (10k-30k images). Reduced regularization. SqueezeNet 1.0 can be used for slightly more capacity.",
        "huge": "Huge datasets (30k-50k images). Minimal regularization. SqueezeNet reaches its capacity limits here - consider larger architectures for better accuracy.",
        "massive": "Massive datasets (50k-100k images). SqueezeNet may underfit - this preset pushes the architecture to its limits. Consider EfficientNet for better results.",
        "giant": "Giant datasets (100k+ images). SqueezeNet is NOT recommended for datasets this large - limited capacity will cause underfitting. Use only if model size is absolutely critical."
      }
    },
    "style_aesthetic_classification": {
      "name": "Style / Aesthetic Classification",
      "description": "Classifies style, mood, and aesthetic qualities instead of concrete objects. Looks at palette, composition, and texture to decide how an image feels or which style it fits. Useful for curation, recommendations, photo quality sorting, art style tagging, or interior/fashion style detection. EfficientNet and ResNeXt capture global patterns well, with ResNet as a solid fallback. Outputs style labels or quality scores you can feed into creative workflows."
    },
    "configs": {
      "tiny": "Very small datasets (50-500 images). Heavy regularization, aggressive augmentation, smaller models. High risk of overfitting - early stopping essential.",
      "small": "Small datasets (500-2.5k images). Moderate regularization with pretrained weights. Balance between learning capacity and overfitting prevention.",
      "medium": "Medium datasets (2.5k-10k images). Standard hyperparameters with good augmentation. Balanced training approach works well.",
      "large": "Large datasets (10k-30k images). Reduced regularization, larger models viable. Higher learning rates and bigger batches effective.",
      "huge": "Huge datasets (30k-50k images). Minimal regularization needed. Deep networks train effectively with standard augmentation.",
      "massive": "Massive datasets (50k-100k images). Very deep networks fully supported. Fast learning rates, large batches, minimal regularization.",
      "giant": "Giant datasets (100k+ images). Maximum model capacity utilized. Can train from scratch or fine-tune with aggressive settings."
    }
  }
}