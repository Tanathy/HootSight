{
  "language_name": "Português",
  "activations": {
    "not_supported": "Ativação não suportada: {activation}. Opções disponíveis: {available}",
    "created": "Função de ativação criada com sucesso: {activation} com parâmetros {params}",
    "creation_failed": "Falha ao criar função de ativação: {error}",
    "invalid_params": "Parâmetros inválidos para função de ativação: {error}",
    "config_missing_type": "Falta 'type' na configuração de ativação",
    "recommendations_removed": "Recomendações de ativação removidas do sistema"
  },
  "augmentation": {
    "not_supported": "Aumento não suportado: {aug}. Opções disponíveis: {available}",
    "created": "Aumento criado com sucesso: {aug} com parâmetros {params}",
    "creation_failed": "Falha ao criar aumento: {error}",
    "invalid_params": "Parâmetros inválidos para aumento: {error}",
    "config_missing_type": "Falta 'type' na configuração de aumento",
    "preview_invalid_phase": "Fase de aumento inválida: {phase}",
    "preview_no_images": "Nenhuma imagem disponível para pré-visualização de aumento no projeto {project}.",
    "preview_failed": "Falha ao gerar pré-visualização de aumento: {error}",
    "preview_generated": "Pré-visualização de aumento gerada para fase {phase}"
  },
  "coordinator_settings": {
    "user_settings_saved": "Configurações do usuário salvas em {path}",
    "user_settings_save_failed": "Falha ao salvar configurações do usuário: {error}"
  },
  "recommendations": {
    "critical_shortage": "Escassez crítica de dados detectada em: {labels}",
    "reduce_oversampled": "Considere reduzir rótulos superamostrados: {labels}",
    "augment_undersampled": "Considere aumentar rótulos subamostrados: {labels}",
    "weighted_loss": "Considere usar funções de perda ponderadas",
    "stratified_sampling": "Considere amostragem estratificada",
    "hierarchical_imbalance": "Desequilíbrio hierárquico detectado em: {categories}",
    "schema": {
      "description": "Esquema JSON para Hootsight config.json – define tipos, intervalos e a estrutura hierárquica de todas as opções configuráveis",
      "general_description": "Configurações gerais da aplicação",
      "general_language_description": "Código de idioma da interface",
      "api_description": "Configuração do servidor de API",
      "api_host_description": "Host do servidor de API",
      "api_port_description": "Porta do servidor de API",
      "ui_description": "Configurações da interface do usuário",
      "ui_title_description": "Título da janela da aplicação",
      "ui_width_description": "Largura da janela em pixels",
      "ui_height_description": "Altura da janela em pixels",
      "ui_resizable_description": "Define se a janela pode ser redimensionada",
      "system_description": "Configurações em nível de sistema",
      "system_max_threads_description": "Número máximo de threads",
      "system_fallback_batch_size_description": "Tamanho de lote reserva quando o cálculo automático falha",
      "system_memory_cleanup_interval_description": "Intervalo de limpeza de memória em segundos",
      "system_thread_pool_timeout_description": "Timeout do pool de threads em segundos",
      "system_startup_wait_seconds_description": "Tempo de espera durante a inicialização em segundos",
      "memory_description": "Parâmetros de gerenciamento de memória",
      "memory_target_memory_usage_description": "Proporção alvo de uso de memória (0.0-1.0)",
      "memory_safety_margin_description": "Margem de segurança para cálculos de memória (0.0-1.0)",
      "memory_augmentation_threads_description": "Número de threads para augmentação de dados",
      "training_description": "Configuração de treinamento",
      "training_model_type_description": "Tipo de modelo a ser usado",
      "training_model_name_description": "Nome específico do modelo",
      "training_pretrained_description": "Inicializa a rede com pesos pré-treinados no ImageNet quando disponíveis; desative para treinar do zero",
      "training_task_description": "Tipo de tarefa de aprendizado de máquina",
      "training_batch_size_description": "Tamanho do lote de treinamento",
      "training_epochs_description": "Número de épocas de treinamento",
      "training_learning_rate_description": "Taxa de aprendizado",
      "training_weight_decay_description": "Weight decay (regularização L2)",
      "training_input_size_description": "Tamanho da aresta em pixels para o tensor de entrada quadrado (deve permanecer coerente com o redimensionamento das suas augmentações)",
      "training_normalize_description": "Parâmetros de normalização de imagens",
      "training_normalize_mean_description": "Valores médios para os canais RGB",
      "training_normalize_std_description": "Desvios padrão para os canais RGB",
      "training_val_ratio_description": "Proporção de validação (0.0-1.0)",
      "training_dataloader_description": "Configuração do DataLoader",
      "training_dataloader_num_workers_description": "Número de processos worker",
      "training_dataloader_pin_memory_description": "Define se deve prender memória para acelerar a cópia para a GPU",
      "training_dataloader_persistent_workers_description": "Define se os workers permanecem vivos entre épocas",
      "training_dataloader_prefetch_factor_description": "Número de lotes pré-carregados por worker",
      "training_augmentation_description": "Configuração de augmentação de dados",
      "training_augmentation_train_description": "Augmentações utilizadas no treinamento",
      "training_augmentation_val_description": "Augmentações utilizadas na validação",
      "training_optimizer_type_description": "Tipo de otimizador",
      "training_optimizer_params_description": "Parâmetros do otimizador",
      "training_scheduler_type_description": "Tipo de scheduler de taxa de aprendizado",
      "training_scheduler_params_description": "Parâmetros do scheduler",
      "training_loss_type_description": "Tipo de função de perda",
      "training_loss_params_description": "Parâmetros da função de perda",
      "training_weight_init_description": "Configuração de inicialização de pesos",
      "training_checkpoint_description": "Configuração de checkpoints",
      "training_early_stopping_description": "Configuração de parada antecipada",
      "training_gradient_description": "Opções relacionadas a gradiente",
      "training_runtime_description": "Ajustes de otimização em tempo de execução",
      "training_runtime_mixed_precision_description": "Ativar treinamento automático em precisão mista",
      "training_runtime_channels_last_description": "Usar formato de memória channels-last para melhor aproveitamento da GPU",
      "training_runtime_allow_tf32_description": "Permitir TF32 para acelerar operações matriciais em GPUs Ampere ou superiores",
      "training_runtime_cudnn_benchmark_description": "Ativar benchmark do cuDNN para encontrar algoritmos de convolução otimizados",
      "dataset_description": "Configuração do conjunto de dados",
      "dataset_image_extensions_description": "Extensões de imagem suportadas",
      "optimizers_description": "Sobrescrita de padrões de otimizadores",
      "optimizers_defaults_description": "Parâmetros padrão para otimizadores",
      "schedulers_description": "Sobrescrita de padrões de schedulers",
      "schedulers_defaults_description": "Parâmetros padrão para schedulers",
      "schedulers_defaults_lambda_lr_lr_lambda_description": "Função lambda como string, por exemplo 'lambda epoch: 0.95 ** epoch'",
      "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Função lambda como string, por exemplo 'lambda epoch: 0.95'",
      "losses_description": "Sobrescrita de padrões de perdas",
      "losses_defaults_description": "Parâmetros padrão para perdas",
      "models_description": "Configurações de modelos",
      "models_resnet_description": "Configurações da família de modelos ResNet",
      "models_resnet_variants_description": "Configuração de variantes ResNet",
      "models_resnext_description": "Configurações da família ResNeXt",
      "models_resnext_variants_description": "Configuração de variantes ResNeXt",
      "models_mobilenet_description": "Configurações da família MobileNet",
      "models_mobilenet_variants_description": "Configuração de variantes MobileNet",
      "models_shufflenet_description": "Configurações da família ShuffleNet",
      "models_shufflenet_variants_description": "Configuração de variantes ShuffleNet",
      "models_squeezenet_description": "Configurações da família SqueezeNet",
      "models_squeezenet_variants_description": "Configuração de variantes SqueezeNet",
      "models_efficientnet_description": "Configurações da família EfficientNet",
      "models_efficientnet_variants_description": "Configuração de variantes EfficientNet",
      "models_supported_types_description": "Tipos de modelo suportados",
      "general_language_enum_descriptor": {
        "en": "Idioma inglês – controla toda a localização da interface, incluindo rótulos de menu, mensagens de erro, tooltips e textos de validação. Impacta todo o texto renderizado na interface web e nas respostas da API. Determina qual pacote de idioma é carregado no início da aplicação. Atualmente é a única opção suportada, portanto atua como padrão em todas as instalações."
      },
      "system_max_threads_enum_descriptor": {
        "auto": "Contagem automática de threads – calcula dinamicamente o tamanho ideal do pool com base nos núcleos de CPU disponíveis (normalmente núcleos menos um). Impacta o carregamento paralelo de dados, o pré-processamento de imagens, o batching de inferência e as tarefas em segundo plano. Controla a alocação de threads para workers do PyTorch DataLoader, pipelines de augmentação e tratamento simultâneo de requisições HTTP. Ajusta-se automaticamente à capacidade do hardware e ao uso de memória disponível."
      },
      "memory_augmentation_threads_enum_descriptor": {
        "auto": "Contagem automática de threads de augmentação – calcula o número ideal de threads para augmentação de imagens em paralelo com base nos núcleos de CPU e na RAM disponível. Impacta a vazão da pipeline de transformações, incluindo rotação, escala, variação de cor e normalização. Controla a alocação de memória para buffers de augmentação e armazenamento intermediário. Equilibra o uso de CPU com a pressão de memória para evitar sobrecarregar o sistema durante pré-processamento intenso."
      },
      "training_model_type_enum_descriptor": {
        "resnet": "ResNet (Residual Network) – rede neural convolucional profunda com conexões residuais que permitem treinar arquiteturas muito profundas (18-152 camadas). Impacta o fluxo de gradientes, a estabilidade do treinamento, a profundidade das representações e a capacidade do modelo. Usa blocos residuais com batch normalization e ativação ReLU. Controla a complexidade arquitetônica de 11M de parâmetros (ResNet-18) até 60M (ResNet-152). Influencia uso de memória, tempo de treinamento, velocidade de inferência e precisão final em tarefas de classificação de imagens.",
        "resnext": "ResNeXt (Aggregated Residual Transformations) – evolução do ResNet que usa cardinalidade (convoluções agrupadas) para aumentar a capacidade sem inflar significativamente a contagem de parâmetros. Aumenta a diversidade do aprendizado de características, a expressividade e a eficiência computacional. Controla caminhos paralelos de transformação dentro de cada bloco residual. Impacta o uso de memória da GPU, a duração do treinamento e alcança maior precisão que o ResNet padrão com custo computacional semelhante.",
        "mobilenet": "MobileNet – CNN leve que utiliza convoluções separáveis em profundidade para reduzir tamanho e custo computacional. Impacta a latência de inferência, o consumo de energia, o tamanho do modelo e a viabilidade de implantação em dispositivos móveis. Controla o equilíbrio entre precisão e eficiência por meio do multiplicador de largura e da resolução. Afeta a bateria, o processamento em tempo real e a compatibilidade com dispositivos de borda.",
        "shufflenet": "ShuffleNet – CNN extremamente eficiente que usa operações de embaralhamento de canais e convoluções agrupadas ponto a ponto. Impacta o uso de banda de memória, o custo computacional por inferência, o tamanho do modelo e a velocidade de processamento. Controla a comunicação entre canais agrupados para manter o fluxo de informação. Otimizada para processadores ARM e dispositivos de baixo consumo. Afeta requisitos de desempenho em tempo real e cenários com recursos limitados.",
        "squeezenet": "SqueezeNet – CNN ultracompacta que emprega módulos Fire (etapas squeeze + expand) para alcançar precisão próxima à do AlexNet com 50× menos parâmetros. Reduz requisitos de armazenamento do modelo, tempo de download, eficiência de cache e largura de banda de implantação. Controla a contagem de parâmetros por meio de redução agressiva de dimensões seguida de expansão. Minimiza a pegada em disco mantendo precisão adequada para tarefas básicas de classificação.",
        "efficientnet": "EfficientNet – CNN com escalonamento composto que ajusta de forma conjunta profundidade, largura e resolução usando neural architecture search. Impacta a eficiência computacional, a evolução da precisão, os recursos necessários para treinamento e a otimização de inferência. Controla a complexidade do modelo via coeficiente composto que equilibra simultaneamente as três dimensões. Fornece melhor relação precisão/eficiência do que métodos tradicionais de escalonamento."
      },
      "training_task_enum_descriptor": {
        "classification": "Classificação de rótulo único – atribui exatamente uma classe mutuamente exclusiva a cada imagem. Impacta a arquitetura da camada final (ativação softmax), a escolha da função de perda (entropia cruzada categórica), a dimensionalidade de saída (número de classes) e a interpretação da confiança. Controla os limites de decisão, a distribuição de probabilidades e os padrões de convergência. Requer distribuição equilibrada do dataset e classes bem separadas para desempenho ideal.",
        "multi_label": "Classificação multirrótulo – atribui zero, um ou vários rótulos não exclusivos a cada imagem ao mesmo tempo. Impacta a ativação da camada de saída (sigmoide por classe), a composição da perda (entropia cruzada binária por rótulo), a escolha de limiares e as métricas (F1, mAP). Controla caminhos de previsão independentes, o tratamento de correlação entre rótulos e estratégias de balanceamento para classes desproporcionais. Lida com cenários em que as imagens contêm múltiplos conceitos semânticos.",
        "detection": "Detecção de objetos – localiza e classifica múltiplas instâncias dentro de uma imagem usando predição de caixas delimitadoras. Impacta a complexidade arquitetônica (redes tipo pirâmide, geração de anchors), a composição da perda (classificação + regressão), os requisitos de dados (caixas anotadas), o pós-processamento (supressão não máxima) e o custo computacional. Controla a profundidade de extração espacial, o reconhecimento multiescala, os mecanismos de proposta e os cálculos de IoU.",
        "segmentation": "Segmentação semântica – realiza classificação densa por pixel atribuindo rótulos semânticos a cada ponto da imagem. Exige memória alta (mapas de características em resolução completa), arquiteturas encoder-decoder com conexões de atalho, desenho de perdas (entropia cruzada por pixel, focal para desbalanceamento) e gerenciamento de complexidade (desbalanceamento em nível de pixel). Controla estratégias de upsampling, qualidade do refinamento de bordas, precisão espacial e raciocínio contextual."
      },
      "training_epochs_enum_descriptor": {
        "auto": "Determinação automática de épocas – monitora perda e acurácia de validação para encontrar a duração ideal usando critérios de parada antecipada. Impacta o tempo total de treinamento, a qualidade da convergência, o uso de recursos e a prevenção de overfitting. Acompanha melhorias durante o período de paciência e encerra automaticamente quando não há progresso significativo. Equilibra exaustividade e eficiência."
      },
      "training_optimizer_type_enum_descriptor": {
        "sgd": "Stochastic Gradient Descent – passo de gradiente determinístico com momentum opcional e lookahead de Nesterov. Expõe learning_rate, momentum, dampening e weight_decay como controles críticos. Funciona melhor quando você pode planejar um scheduler e quer controle apertado sobre a generalização. Entrega resultados sólidos em grandes datasets de visão com schedulers cosseno ou por etapas, mas ajuste o momentum (0.9 costuma ser bom ponto de partida) e mantenha taxas entre 0.01 e 0.1 conforme o lote.",
        "adam": "Otimizador Adam – método adaptativo de primeira ordem que armazena médias móveis de gradientes (beta1) e de gradientes ao quadrado (beta2). Os padrões 0.9/0.999 e eps=1e-8 atendem à maioria dos workloads. Lida com gradientes ruidosos ou esparsos sem ajustar manualmente a taxa, tornando-se base confiável para classificação e transfer learning. Se notar convergência lenta com weight decay acoplado, considere AdamW.",
        "adamw": "AdamW – separa o weight decay das atualizações adaptativas do Adam para que a regularização L2 funcione como esperado. Mantém betas e epsilon do Adam expondo weight_decay como regularizador real. Preferido para vision transformers, fine-tuning de ResNet ou qualquer modelo onde estabilidade e generalização previsível importam. Comece com weight_decay≈0.01 e ajuste learning_rate entre 3e-5 e 3e-4 em cenários de transferência.",
        "adamax": "AdaMax – variante do Adam que usa a norma infinito para o segundo momento. Compartilha hiperparâmetros, mas oferece maior resiliência a picos esporádicos nos gradientes. Útil quando o Adam fica instável por magnitudes extremas, especialmente em GANs ou RL. Mantenha beta2 em torno de 0.999 e trate learning_rate como no Adam padrão; espere convergência um pouco mais lenta, porém com menos saltos catastróficos.",
        "nadam": "Adam acelerado por Nesterov – adiciona momentum de Nesterov à adaptação do Adam. Compartilha betas e epsilon, mas faz uma avaliação lookahead que pode apertar a convergência em objetivos suaves. Implica pequeno custo extra por passo. Recomendado quando o Adam converge mas estagna cedo; reduza a learning_rate em relação ao Adam puro (por exemplo, 1e-4 em vez de 3e-4) para evitar ultrapassar.",
        "radam": "Rectified Adam (RAdam) – versão com warmup automático derivado da retificação da variância. Elimina a necessidade de agenda de warmup manual, reduzindo o tamanho dos passos até a variância estabilizar. Hiperparâmetros coincidem com os do Adam. Use quando precisa de adaptatividade mas o treinamento é sensível às primeiras iterações. Funciona bem em datasets pequenos onde warmup manual geraria overfitting.",
        "rmsprop": "RMSprop – mantém média exponencial de gradientes ao quadrado (alpha) para normalizar atualizações. Por padrão alpha=0.99 e eps=1e-8. Popular em redes recorrentes e RL; continua útil quando os gradientes oscilam muito e o Adam parece agressivo demais. Combine com scheduler decrescente; comece perto de 1e-3 com momentum desligado ou baixo (≤0.1).",
        "rprop": "Resilient Backpropagation (RProp) – otimizador baseado no sinal que ajusta o tamanho do passo por parâmetro usando apenas mudanças no sinal do gradiente. Ignora tamanho de lote ao assumir atualizações full-batch, portanto raramente é adequado para CNN com mini-batches. Empregue em contextos deterministas (datasets pequenos com passagens completas) onde você quer convergência tipo segunda ordem sem armazenar a Hessiana. eta_plus=1.2 e eta_minus=0.5 governam a adaptação.",
        "adagrad": "AdaGrad – acumula gradientes ao quadrado reduzindo a taxa para pesos atualizados com frequência. Quase não exige manutenção em características esparsas, mas a soma acumulada força a taxa efetiva a zero em treinamentos longos. Use para embeddings ou problemas clássicos de NLP, não para CNN profundas que treinam centenas de épocas. Learning_rate inicial típico 1e-2 com epsilon ~1e-10 para evitar divisão por zero.",
        "adadelta": "AdaDelta – corrige o decaimento de taxa do AdaGrad rastreando uma janela móvel de gradientes e atualizações. Requer pouco ajuste manual além de rho (0.9) e eps (1e-6). Funciona em objetivos ruidosos onde o Adam pode ser agressivo, embora a precisão final costume ficar abaixo de AdamW. Prefira quando precisar evitar schedulers manuais e ainda assim exigir comportamento adaptativo.",
        "sparse_adam": "Sparse Adam – versão do Adam que atualiza apenas índices com gradiente, reduzindo memória e computação para tabelas de embeddings. Usa os mesmos hiperparâmetros do Adam, mas assume gradientes praticamente nulos. Essencial para NLP com vocabulários enormes. Evite em modelos convolucionais densos; a contabilidade esparsa só desperdiça tempo.",
        "lbfgs": "L-BFGS – método quasi-Newton de memória limitada que aproxima a Hessiana inversa usando gradientes passados. Requer gradientes full-batch e uma busca linear por passo, portanto implemente um closure que recalcule perda e gradientes. Excelente para refinar modelos pequenos ou resolver problemas convexos com alta precisão. Não viável com mini-lotes grandes porque cada passo é caro e a memória cresce com history_size (controlado por max_iter e history_size).",
        "asgd": "SGD médio (ASGD) – mantém uma média em andamento dos parâmetros para amortecer oscilações causadas por gradientes ruidosos. Você ainda ajusta a learning_rate base do SGD, mas a média passa a valer após averaging_start para suavizar a convergência. Use quando o SGD puro oscila no final e você quer evitar trocar para Adam. Funciona melhor com taxas constantes ou decaimento lento e momentum desligado."
      },
      "training_scheduler_type_enum_descriptor": {
        "step_lr": "Scheduler de taxa por etapas – multiplica a learning rate por gamma a cada step_size épocas. Ideal quando você já sabe onde o progresso desacelera (ex.: 30/60/90 no ImageNet). Escolha gamma entre 0.1 e 0.3 e alinhe step_size com o orçamento total de épocas. Sem essa referência pode parecer brusco, então acompanhe métricas de validação.",
        "multi_step_lr": "Scheduler de taxa multi-etapas – generaliza o esquema por etapas aceitando uma lista de épocas marco. Permite programar várias reduções em pontos arbitrários, perfeito para replicar agendas de artigos ou experimentos prévios. Mantenha gamma uniforme salvo motivo contrário e garanta que os marcos sejam inteiros estritamente crescentes.",
        "exponential_lr": "Scheduler exponencial – aplica lr_t = lr_0 * gamma^t, oferecendo decaimento suave em troca de calibrar gamma com cuidado. Útil em treinamentos longos quando você prefere deslizamento gradual a quedas discretas. Gamma típico entre 0.97 e 0.995 por época. Combine com warmup se a inclinação inicial for muito acentuada.",
        "cosine_annealing_lr": "Scheduler cossenoidal – reduz a taxa seguindo uma curva cosseno ao longo de T_max épocas e opcionalmente reinicia em eta_min. Garante aterrissagens suaves que elevam a precisão final em modelos de visão. Ajuste T_max ao número de épocas do ciclo e eta_min a um valor pequeno como lr_0/100. Ótimo para refinamento automático sem marcos manuais.",
        "cosine_annealing_warm_restarts": "Cosseno com reinícios quentes – repete ciclos cossenoidais reiniciando a taxa inicial após cada ciclo. Excelente para escapar de mínimos rasos em sessões longas. T_0 define a duração inicial e T_mult escala os ciclos seguintes. Mantenha eta_min baixo mas não zero para evitar congelar o otimizador.",
        "reduce_lr_on_plateau": "Reduzir taxa em platô – observa uma métrica (geralmente perda de validação) e diminui a taxa por um fator quando a melhora estagna durante a paciência. Essencial quando o momento do platô é imprevisível. Configure cooldown para evitar disparos consecutivos e use threshold para filtrar ruído. Gamma entre 0.1 e 0.5 costuma equilibrar bem.",
        "cyclic_lr": "Scheduler cíclico – alterna a learning rate entre base_lr e max_lr em janelas curtas, opcionalmente reduzindo a amplitude conforme o modo. Útil para convergência rápida em objetivos difíceis ou para explorar faixas de taxa. Defina step_size_up/down como o número de iterações por meio ciclo; mantenha max_lr em torno de 3-10× base_lr. Combine com ciclo de momentum se habilitar cycle_momentum.",
        "one_cycle_lr": "Política One Cycle – varredura única que eleva a taxa até max_lr e depois a reduz a uma fração enquanto inverte o momentum. Entrega convergência rápida quando os passos totais são conhecidos. Informe total_steps ou (epochs × steps_per_epoch); defina pct_start para marcar a parte de warmup (0.3 é comum). Funciona melhor com SGD ou AdamW e não espera schedulers adicionais.",
        "polynomial_lr": "Scheduler polinomial – reduz a taxa até zero seguindo (1 - t/T)^power. Defina total_iters como número de passos do otimizador e power para controlar a curvatura (1 linear, 2 quadrática). Útil em segmentação e detecção quando você quer uma descida determinística até o fim.",
        "linear_lr": "Scheduler linear – interpola linearmente entre start_factor e end_factor durante total_iters passos. Ideal para warmup (start_factor < 1) ou fases de resfriamento. Mantenha total_iters alinhado às iterações que deseja cobrir e combine com outro scheduler para o restante do treinamento.",
        "lambda_lr": "Scheduler lambda – gancho direto que multiplica a taxa base pela função lambda(epoch) personalizada. Fornece controle total para agendas de pesquisa ou aprendizado curricular. Informe uma expressão Python que retorne float; lembre que será avaliada como string no processo de treinamento. Valide a função com cuidado: erros de sintaxe ou saídas negativas derrubam a execução.",
        "multiplicative_lr": "Scheduler multiplicativo – semelhante ao lambda_lr, mas espera um callable que devolva um multiplicador a cada passo, usado com frequência para escalonamento por época. Passe uma lambda baseada na contagem de passos do otimizador se precisar de controle por iteração. Mantenha multiplicadores positivos e limitados; valores >1 podem disparar a taxa e desestabilizar o treinamento."
      },
      "training_loss_type_enum_descriptor": {
        "cross_entropy": "Entropia cruzada – combina softmax e log-verossimilhança negativa em uma chamada. É a escolha padrão para classificação de rótulo único. Aceita logits brutos, lida com desbalanceamento via weight ou label_smoothing e fornece probabilidades calibradas. Mantenha reduction='mean' para gradientes estáveis e monitore label_smoothing para não apagar classes minoritárias.",
        "nll_loss": "Log-verossimilhança negativa – mesma matemática da entropia cruzada, mas espera que você aplique log_softmax. Útil quando o modelo já entrega log-probabilidades (ex.: escalonamento de temperatura ou precisão mista manual). Garanta que as entradas sejam log-probabilidades; fornecer logits brutos produzirá resultados inválidos sem aviso.",
        "bce_loss": "Entropia cruzada binária – opera sobre probabilidades em [0,1], portanto use sigmoide explícito. Adequada para classificação binária quando você controla a ativação separadamente. Evite underflow numérico com logits extremos recortando entradas ou troque para BCEWithLogitsLoss se surgirem NaNs.",
        "bce_with_logits": "Entropia cruzada binária com logits – versão numericamente estável que aplica sigmoide internamente. Opção padrão para classificação multirrótulo e tarefas binárias. Suporta pos_weight para desbalanceamento sem hacks manuais. Gera perdas ilimitadas se você esquecer de restringir os alvos a {0,1}.",
        "multi_margin": "Perda de margem multiclasse – objetivo baseado em margem (estilo hinge) que força a pontuação correta a superar as demais por pelo menos o valor de margem. Oferece normas L1 ou L2 via parâmetro p. Use quando quiser comportamento de grande margem em vez de probabilístico, mas a convergência pode ser mais lenta sem controlar bem a taxa.",
        "multi_label_margin": "Perda de margem multirrótulo – estende a perda de margem para problemas multirrótulo ordenando classes positivas à frente das negativas. Requer codificar alvos como listas de índices, tornando difícil trabalhar com tensores densos. Reserve para pesquisas que exijam ranking por margem em espaço multirrótulo.",
        "multi_label_soft_margin": "Perda de margem suave multirrótulo – aplica uma formulação de margem suave sobre ativações sigmoide, produzindo gradientes mais suaves que perdas de margem rígida. Trata melhor rótulos sobrepostos e desbalanceamento do que BCE padrão. Os rótulos ainda devem estar em {0,1}; ajuste limiares na inferência para aproveitar o panorama mais suave.",
        "mse_loss": "Erro quadrático médio – penalização L2 clássica. Penaliza erros grandes de forma quadrática, ampliando o impacto de outliers. Ótima para autoencoders e regressão com pouco ruído, mas corte alvos extremos ou troque para Huber se houver explosão de gradientes.",
        "l1_loss": "Perda L1 (erro absoluto médio) – penalização linear sobre o erro absoluto, robusta a outliers às custas de convergência mais lenta perto de zero. Use quando precisar de comportamento próximo à mediana ou quando a métrica de avaliação for MAE. Os gradientes têm magnitude constante, portanto combine com schedulers suaves para evitar oscilações.",
        "smooth_l1": "Perda Smooth L1 – perda tipo Huber com região beta que se comporta como L2 perto de zero e L1 fora. Escolha padrão para regressão de caixas delimitadoras (beta≈1). Ajuste beta se sua escala diferir; valores menores estreitam a região quadrática e aumentam a penalização para erros médios.",
        "huber_loss": "Perda Huber – semelhante à Smooth L1, mas parametrizada por delta em vez de beta. Dá controle explícito sobre o ponto de transição entre penalizações quadráticas e lineares. Excelente para regressão com outliers ocasionais; defina delta próximo ao desvio-padrão esperado do ruído.",
        "kl_div": "Divergência Kullback-Leibler – mede a divergência entre a distribuição prevista e a distribuição alvo. Requer log-probabilidades como entrada e probabilidades cruas como alvo por padrão (ou vice-versa com log_target). Essencial em distilação de conhecimento e modelos variacionais. Verifique o modo de redução; 'batchmean' preserva a teoria ao somar sobre as classes e fazer a média no lote.",
        "margin_ranking": "Perda de ranking por margem – opera sobre pares de pontuações (x1, x2) com ordenação real y ∈ {−1, 1}. Treina o modelo para que x1 supere x2 por pelo menos o valor de margem quando y=1. Combine com amostragem cuidadosa de pares positivos/negativos ou triplas; pares aleatórios raramente trazem sinal útil.",
        "hinge_embedding": "Perda hinge embedding – para aprendizado de similaridade onde os rótulos indicam se pares devem ficar próximos (+1) ou distantes (−1). Penaliza distâncias que violem a margem especificada. Use quando só há supervisão binária mesmo/diferente e você deseja embeddings agrupados conforme isso.",
        "triplet_margin": "Perda tripleta – consome embeddings âncora, positivo e negativo e impõe margem entre as distâncias positiva e negativa. Exige mineração de triplas duras ou semi-duras para ser eficaz; triplas aleatórias costumam desperdiçar computação. A margem padrão é 1.0, ajuste conforme a escala dos embeddings (menor se estiverem normalizados).",
        "cosine_embedding": "Perda de embedding cosseno – otimiza diretamente a similaridade cosseno, enfatizando distância angular em vez de magnitude. Ideal com vetores normalizados ou quando a direção carrega a semântica (ex.: reconhecimento facial). Garanta que os embeddings estejam normalizados para evitar reintroduzir efeitos de magnitude.",
        "ctc_loss": "Perda CTC (Connectionist Temporal Classification) – alinha entradas de comprimento variável com sequências alvo sem anotações por quadro. Requer log-probabilidades com forma (T, N, C) e sequências alvo sem inserir brancos (a perda cuida disso). Configure o índice do blank e ordene os comprimentos alvo; discrepâncias geram erros de execução.",
        "poisson_nll": "Perda log-verossimilhança negativa de Poisson – para modelar dados de contagem com alvos inteiros não negativos. Aceita log_input para impor predições positivas ou logits completos com clamp para permanecer acima de zero. Use full=true se o modelo prever taxas cruas. Não forneça alvos negativos; isso quebra a suposição da distribuição.",
        "gaussian_nll": "Perda log-verossimilhança negativa gaussiana – treina o modelo a emitir média e variância para alvos contínuos. Espera que o modelo retorne tensores (média, variância). Suporta covariância completa via cholesky_factor; caso contrário a variância deve permanecer positiva. Muito útil para regressão com incerteza; adicione pequeno epsilon à variância para evitar log(0)."
      },
      "training_loss_reduction_enum_descriptor": {
        "mean": "Redução média – calcula a perda média dividindo a perda total pelo tamanho do lote. Normaliza a magnitude do gradiente, torna o treinamento independente do tamanho do lote e estabiliza a sensibilidade à taxa. Mantém gradientes proporcionais ao erro de cada amostra em vez do lote completo.",
        "sum": "Redução soma – calcula a perda total somando as perdas individuais sem normalizar. Faz a magnitude do gradiente depender do tamanho do lote, exigindo ajustar a taxa de aprendizado proporcionalmente e alterando a dinâmica do treinamento. Útil quando você quer que o gradiente escale com o número de amostras processadas.",
        "none": "Sem redução – retorna a perda por amostra sem agregação. Permite ponderações personalizadas, análise por amostra, combinações manuais e estratégias avançadas de treinamento. Essencial quando você precisa manipular a perda por amostra."
      },
      "training_early_stopping_monitor_enum_descriptor": {
        "val_loss": "Monitoramento da perda de validação – interrompe o treinamento quando a perda deixa de melhorar. Previna overfitting, otimize a duração, melhore a generalização e economize recursos. O treinamento é encerrado ao detectar a meseta, normalmente indicando que o modelo já capturou os padrões generalizáveis.",
        "val_accuracy": "Monitoramento da acurácia de validação – interrompe o treinamento quando a acurácia deixa de subir. Focado em otimizar o desempenho de classificação, detectar overfitting e melhorar a eficiência. Adequado para tarefas balanceadas em que a acurácia é a métrica principal e se correlaciona com a generalização."
      },
      "optimizers_defaults_lbfgs_line_search_fn_oneOf[1]_enum_descriptor": {
        "strong_wolfe": "Busca linear Strong Wolfe – algoritmo avançado para L-BFGS que garante as condições de diminuição suficiente (Armijo) e curvatura. Assegura comprimentos de passo adequados que atendem critérios de otimalidade mantendo eficiência computacional. Essencial para as garantias teóricas do L-BFGS e fornece passos robustos em métodos quasi-Newton."
      },
      "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
        "min": "Modo mínimo – monitora métricas em que valores menores são melhores (por exemplo, perda de validação). Dispara a redução quando a métrica deixa de cair abaixo do limiar durante a paciência configurada. Evita estagnação ao reduzir a taxa quando a perda se achata.",
        "max": "Modo máximo – monitora métricas em que valores maiores são melhores (por exemplo, acurácia). Reduz a taxa quando a métrica deixa de subir acima do limiar durante a paciência definida. Facilita ajuste fino adicional ao detectar platôs de acurácia."
      },
      "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
        "rel": "Limiar relativo – define a melhoria como porcentagem do melhor valor atual. Ajusta a sensibilidade à medida que o modelo melhora, tornando o limiar mais rígido conforme o desempenho sobe. Evita reduzir a taxa prematuramente em modelos de alto desempenho.",
        "abs": "Limiar absoluto – estabelece uma melhoria fixa independente do nível atual da métrica. Mantém requisito constante durante todo o treinamento. Útil quando você precisa de padrões uniformes independentemente do desempenho alcançado."
      },
      "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
        "triangular": "Modo triangular – cria ciclos básicos com aumentos e quedas lineares entre base_lr e max_lr mantendo amplitude constante. Oferece equilíbrio consistente entre exploração e aproveitamento sem decaimento adicional.",
        "triangular2": "Modo triangular2 – semelhante ao triangular, mas reduz a amplitude pela metade após cada ciclo completo. Entrega ciclos cada vez mais conservadores combinando exploração inicial agressiva com refinamento progressivo.",
        "exp_range": "Modo de faixa exponencial – escala a amplitude do ciclo exponencialmente usando gamma para ajustar a faixa dinamicamente. Permite padrões em que a amplitude cresce ou diminui conforme o número do ciclo."
      },
      "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
        "cycle": "Escala por ciclo – aplica a função de escala conforme o número de ciclos completos. Altera a amplitude apenas nos limites de cada ciclo, útil quando você quer ajustes discretos entre ciclos.",
        "iterations": "Escala por iterações – aplica a função conforme o número total de iterações desde o início. Permite transições suaves e contínuas durante todo o treinamento, ideal para mudanças graduais."
      },
      "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
        "cos": "Estratégia cossenoidal – usa uma função cosseno para transições suaves, com mudanças graduais nos extremos e mais intensas no centro. Reduz oscilações e favorece convergência suave.",
        "linear": "Estratégia linear – usa interpolação linear para mudanças em ritmo constante ao longo do ciclo. Proporciona evolução previsível e uniforme quando você não precisa da suavidade cossenoidal."
      },
      "losses_defaults_multi_margin_p_enum_descriptor": {
        "1": "Norma L1 (distância Manhattan) – usa diferenças absolutas para calcular a margem na perda multi-margem. Penaliza de forma linear e é menos sensível a outliers que L2, ideal para dados ruidosos ou extremos.",
        "2": "Norma L2 (distância Euclidiana) – usa diferenças ao quadrado para calcular a margem. Penaliza fortemente erros grandes e fornece gradientes suaves, adequado para dados limpos onde erros grandes devem ser punidos com firmeza."
      },
      "losses_defaults_kl_div_reduction_enum_descriptor": {
        "none": "Sem redução – retorna a divergência KL individual por amostra. Permite ponderação personalizada, filtragem ou análise detalhada. Essencial para estratégias avançadas que precisam manipular a perda por amostra.",
        "mean": "Redução média – calcula a divergência KL média sobre todos os elementos do lote, incluindo dimensões espaciais. Normaliza a magnitude do gradiente em relação ao número total de elementos, mantendo gradientes consistentes independentemente da resolução.",
        "sum": "Redução soma – agrega a divergência KL de todos os elementos sem normalizar. A magnitude do gradiente escala com o número de elementos, exigindo ajuste da taxa quando o tamanho do lote ou a resolução mudam.",
        "batchmean": "Média por lote – faz a média apenas sobre a dimensão de lote, preservando a contribuição espacial completa. É a redução recomendada para KL porque mantém as propriedades teóricas e proporciona treinamento estável."
      },
      "models_resnet_default_optimizer_type_enum_descriptor": {
        "adamw": "AdamW para ResNet – Adam com weight decay desacoplado ajustado para arquiteturas ResNet. Impacta a eficácia da regularização e a estabilidade do treinamento ao separar adaptação e penalização L2. Mantém o weight_decay independente das atualizações para evitar interferência. É a escolha recomendada por oferecer boa generalização e treinamento estável em várias profundidades e datasets.",
        "adam": "Adam para ResNet – estimador adaptativo clássico que ajusta automaticamente a taxa por parâmetro. Impacta a velocidade de convergência e a sensibilidade a hiperparâmetros. Controla momentum e gradientes ao quadrado para equilibrar exploração e aproveitamento. É uma opção de propósito geral com bom desempenho e pouco ajuste.",
        "sgd": "SGD com momentum para ResNet – descenso estocástico tradicional configurado especificamente. Requer agendas cuidadosas de taxa para alcançar o máximo desempenho. Controla as atualizações via momentum enquanto mantém comportamento determinista. Entrega resultados excelentes com o ajuste certo, embora exija mais atenção que otimizadores adaptativos."
      },
      "models_resnet_default_scheduler_type_enum_descriptor": {
        "step_lr": "Scheduler por etapas para ResNet – decaimento escalonado com épocas marco pré-definidas otimizadas para as fases de treinamento. Reduz a taxa em momentos específicos alinhados aos padrões de convergência do ResNet. Ideal quando você conhece os momentos ideais a partir de experimentos anteriores ou literatura.",
        "cosine_annealing_lr": "Scheduler cossenoidal para ResNet – transições suaves que normalmente melhoram a precisão final ao reduzir a taxa gradualmente. Minimiza oscilações e permite ajustes finos nas fases finais do treinamento. Frequentemente supera esquemas por etapas em precisão final.",
        "reduce_lr_on_plateau": "Scheduler adaptativo para ResNet – reduz automaticamente a taxa quando o progresso estagna segundo métricas de validação. Ajusta o calendário ao comportamento real do treinamento em vez de marcos fixos. É ótimo quando você não sabe o momento ideal ou quando as dinâmicas variam entre datasets."
      },
      "paths_description": "Configuração de caminhos",
      "paths_projects_dir_description": "Diretório que contém as pastas dos projetos",
      "paths_ui_dir_description": "Diretório que contém os recursos da interface",
      "paths_config_dir_description": "Diretório que contém os arquivos de configuração",
      "paths_localizations_dir_description": "Diretório que contém os arquivos de localização",
      "paths_packages_file_description": "Caminho para o arquivo packages.jsonc",
      "paths_mappings_file_description": "Caminho para o arquivo de mapeamentos",
      "paths_cache_dir_description": "Diretório para arquivos de cache"
    },
    "status_graph": {
      "epoch_accuracy": "Acurácia por época",
      "epoch_loss": "Perda por época",
      "step_loss": "Perda por passo",
      "learning_rate": "Taxa de aprendizado",
      "loss": "Perda",
      "no_data": "Aguardando atualizações",
      "no_training": "Nenhum treinamento ativo.",
      "active_count": "Treinamentos ativos: {count}",
      "label_training_id": "ID do treinamento",
      "label_status": "Status",
      "label_phase": "Fase",
      "label_epoch": "Época",
      "label_step": "Passo",
      "badge_training": "Treinando: {project}",
      "footer_training": "Treinando {project} — Época {epoch} • Passo {step}"
    },
    "error": "Erro",
    "schema_not_loaded": "Esquema ainda não carregado. Por favor aguarde...",
    "config_not_loaded": "Configuração ainda não carregada. Por favor aguarde...",
    "augmentation_phase": "Aumento {phase}",
    "add": "Adicionar",
    "remove": "Remover",
    "transform": "transformar",
    "no_project_loaded": "Nenhum projeto carregado",
    "load_project_first": "Por favor carregue um projeto primeiro na aba Projetos.",
    "go_to_projects": "Ir para Projetos",
    "dataset_overview": "Visão Geral do Conjunto de Dados",
    "balance_analysis": "Análise de Equilíbrio",
    "label_distribution": "Distribuição de Rótulos (Top 20)",
    "recommendations": "Recomendações",
    "failed_to_load_dataset": "Falha ao carregar informações do conjunto de dados.",
    "current_project": "PROJETO ATUAL",
    "load": "Carregar",
    "start_training": "Iniciar Treinamento",
    "stop_training": "Parar Treinamento",
    "stop_training_disabled": "Nenhum treinamento ativo para este projeto para parar.",
    "training_in_progress": "Treinamento em andamento",
    "loading": "Carregando...",
    "training_status": "Status do Treinamento",
    "idle": "Ocioso",
    "prediction": "Predição",
    "predictions": "Predições",
    "no_predictions_above_threshold": "Nenhuma predição acima do limiar",
    "image": "Imagem",
    "checkpoint": "Checkpoint",
    "auto": "auto",
    "value": "valor",
    "one_number_per_line": "Um número por linha",
    "empty_object": "Objeto vazio",
    "language_warning": "Mudar idioma reiniciará o sistema",
    "language_select_title": "Selecionar Idioma",
    "not_available": "N/A",
    "unknown": "Desconhecido",
    "configuration_empty": "Nenhuma seção de configuração disponível",
    "configuration_schema_missing": "Esquema de configuração ainda não carregado."
  },
  "augmentation_ui": {
    "page_title": "Aumento de Dados",
    "page_description": "Configure transformações de imagem para melhorar a generalização e robustez do modelo.",
    "train_title": "Aumentos de Treinamento",
    "train_description": "Aplicados durante o treinamento para aumentar a diversidade visual mantendo os rótulos.",
    "val_title": "Aumentos de Validação",
    "val_description": "Aplicados durante a validação para avaliação determinística.",
    "toggle_help": "Alterne o aumento para ligar ou desligar para esta fase.",
    "no_options": "Nenhuma opção de aumento disponível.",
    "custom_warning": "As seguintes transformações são preservadas, mas não podem ser editadas aqui:",
    "unknown_transform": "Transformação desconhecida",
    "random_resized_crop": "Corte Redimensionado Aleatório",
    "random_resized_crop_description": "Corta e redimensiona a imagem aleatoriamente para o tamanho alvo respeitando intervalos de escala e proporção de aspecto.",
    "random_horizontal_flip": "Inversão Horizontal Aleatória",
    "random_horizontal_flip_description": "Inverte a imagem horizontalmente com probabilidade configurada para variações esquerda-direita.",
    "random_vertical_flip": "Inversão Vertical Aleatória",
    "random_vertical_flip_description": "Inverte a imagem verticalmente para mudanças de perspectiva cima-baixo.",
    "random_rotation": "Rotação Aleatória",
    "random_rotation_description": "Aplica rotação aleatória dentro do intervalo definido de graus para reduzir viés de orientação.",
    "color_jitter": "Variação de Cor",
    "color_jitter_description": "Varia aleatoriamente brilho, contraste, saturação e matiz para melhorar a robustez de cor.",
    "random_grayscale": "Escala de Cinza Aleatória",
    "random_grayscale_description": "Converte imagens para escala de cinza com probabilidade configurada para reconhecimento de luminância.",
    "random_erasing": "Apagamento Aleatório",
    "random_erasing_description": "Mascara regiões retangulares aleatoriamente para aumentar a robustez espacial e raciocínio de completude de objetos.",
    "random_perspective": "Perspectiva Aleatória",
    "random_perspective_description": "Aplica transformação de perspectiva aleatória usando escala de distorção e probabilidade.",
    "center_crop": "Corte Central",
    "center_crop_description": "Corta a região central para o tamanho alvo para entradas de validação consistentes.",
    "random_resized_crop.size_label": "Tamanho de Saída",
    "random_resized_crop.size_description": "Comprimento da borda em pixels após redimensionar o corte.",
    "random_resized_crop.scale_min_label": "Escala Mínima",
    "random_resized_crop.scale_min_description": "Limite inferior para escala de área relativa à imagem original (0-1).",
    "random_resized_crop.scale_max_label": "Escala Máxima",
    "random_resized_crop.scale_max_description": "Limite superior para escala de área relativa à imagem original.",
    "random_resized_crop.ratio_min_label": "Proporção de Aspecto Mínima",
    "random_resized_crop.ratio_min_description": "Limite inferior para proporção de aspecto amostrada antes do redimensionamento.",
    "random_resized_crop.ratio_max_label": "Proporção de Aspecto Máxima",
    "random_resized_crop.ratio_max_description": "Limite superior para proporção de aspecto amostrada antes do redimensionamento.",
    "random_horizontal_flip.p_label": "Probabilidade de Inversão",
    "random_horizontal_flip.p_description": "Probabilidade de a imagem ser espelhada horizontalmente.",
    "random_vertical_flip.p_label": "Probabilidade de Inversão",
    "random_vertical_flip.p_description": "Probabilidade de a imagem ser invertida verticalmente.",
    "random_rotation.min_label": "Graus Mínimos",
    "random_rotation.min_description": "Limite inferior de rotação em graus (valores negativos rotacionam no sentido horário).",
    "random_rotation.max_label": "Graus Máximos",
    "random_rotation.max_description": "Limite superior de rotação em graus (valores positivos rotacionam no sentido anti-horário).",
    "color_jitter.brightness_label": "Variação de Brilho",
    "color_jitter.brightness_description": "Quantidade máxima de mudança de brilho adicionada a cada canal.",
    "color_jitter.contrast_label": "Variação de Contraste",
    "color_jitter.contrast_description": "Escala máxima de contraste aplicada à imagem.",
    "color_jitter.saturation_label": "Variação de Saturação",
    "color_jitter.saturation_description": "Mudança máxima de saturação aplicada no espaço HSV.",
    "color_jitter.hue_label": "Variação de Matiz",
    "color_jitter.hue_description": "Intervalo máximo de mudança de matiz (0-0.5).",
    "random_grayscale.p_label": "Probabilidade de Escala de Cinza",
    "random_grayscale.p_description": "Probabilidade de a imagem ser convertida para escala de cinza.",
    "random_erasing.p_label": "Probabilidade de Apagamento",
    "random_erasing.p_description": "Probabilidade de uma região aleatória ser apagada na imagem.",
    "random_erasing.scale_min_label": "Escala Mínima",
    "random_erasing.scale_min_description": "Limite inferior para escala de área relativa da região apagada à imagem inteira.",
    "random_erasing.scale_max_label": "Escala Máxima",
    "random_erasing.scale_max_description": "Limite superior para escala de área relativa da região apagada à imagem inteira.",
    "random_erasing.ratio_min_label": "Proporção de Aspecto Mínima",
    "random_erasing.ratio_min_description": "Limite inferior para proporção de aspecto do patch apagado.",
    "random_erasing.ratio_max_label": "Proporção de Aspecto Máxima",
    "random_erasing.ratio_max_description": "Limite superior para proporção de aspecto do patch apagado.",
    "random_erasing.value_label": "Valor de Preenchimento",
    "random_erasing.value_description": "Valor de pixel usado para preencher a região apagada (0-1).",
    "random_erasing.inplace_label": "No Local",
    "random_erasing.inplace_description": "Aplicar apagamento diretamente no tensor de entrada sem alocar uma cópia.",
    "random_perspective.distortion_scale_label": "Escala de Distorção",
    "random_perspective.distortion_scale_description": "Controla a força da distorção de perspectiva (0-1).",
    "random_perspective.p_label": "Probabilidade de Perspectiva",
    "random_perspective.p_description": "Probabilidade de aplicar distorção de perspectiva aleatória.",
    "center_crop.size_label": "Tamanho do Corte",
    "center_crop.size_description": "Comprimento da borda alvo em pixels para corte central.",
    "preview_section_title": "Pré-visualização",
    "preview_description": "Aplica o pipeline atual a uma imagem aleatória do conjunto de dados.",
    "preview_button": "Mostrar Pré-visualização",
    "preview_idle": "Clique em Mostrar Pré-visualização para uma imagem aumentada.",
    "preview_loading": "Gerando pré-visualização...",
    "preview_no_project": "Carregue um projeto para pré-visualizar aumentos.",
    "preview_empty_pipeline": "Configure pelo menos uma transformação para pré-visualização.",
    "preview_generic_error": "Falha ao gerar pré-visualização.",
    "preview_original_label": "Original",
    "preview_augmented_label": "Aumentado",
    "preview_image_path_label": "Caminho da Imagem"
  },
  "about_ui": {
    "page_title": "Sobre o Hootsight",
    "page_description": "Entenda o propósito, arquitetura central e princípios de desenvolvimento que moldam o Hootsight.",
    "card_title": "Ferramenta offline de aumento de imagens para treinamento",
    "intro": "Hootsight é uma ferramenta offline de classificação de imagens que combina treinamento PyTorch com UI configurável.",
    "content_markdown": "## Sobre o Hootsight\n\nOi, sou Tanathy! A única desenvolvedora mantendo o Hootsight funcionando. Eu o criei porque queria uma ferramenta confiável de classificação de imagens offline na qual pudesse confiar no meu próprio hardware, e pensei que outros também mereciam essa liberdade.\n\n### Filosofia\n- Seus dados nunca deixam sua máquina, a menos que você mesmo os mova. Sem tarefas de sincronização em segundo plano ou chamadas de nuvem surpresa.\n- Recuso-me a enviar telemetria ou hooks de rastreamento. O diagnóstico permanece local, para que você possa decidir o que compartilhar.\n- Toda configuração vive em JSON. Versione, compare, implante no Git—tudo o que mantém seu fluxo de trabalho honesto.\n- O instalador configura um ambiente virtual isolado que mantém seu Python global limpo.\n- Pesos pré-treinados vivem em `cache/`, para que você possa fazer backup, auditar ou queimar em segundos.\n- Mapas de estrada seguem vida real. Versões saem quando tenho largura de banda, não quando o sprint board diz para frente.\n- Ferramentas são neutras; como você as usa é o que importa. Espero que todos nós, eu incluída, as usemos com cuidado.\n\n>Disclaimer: Cada conjunto de dados que você ingere, cada rótulo que você prevê e cada modelo que você exporta é sua responsabilidade. Seja consciente do consentimento, legalidade e pessoas afetadas pelo seu trabalho.\n\n### Estado do Desenvolvimento\n- Hootsight está firmemente em alfa. Espere atualizações, experimentos e ocasionalmente bordas ásperas.\n- ResNet tem validação end-to-end completa. ResNeXt, EfficientNet e outras arquiteturas estão em testes de longo prazo quando o tempo permite.\n- Encontrou um bug? Por favor registre no [GitHub Issues](https://github.com/Tanathy/HootSight/issues). Relatórios claros me ajudam a corrigir as coisas mais rápido.\n\n### Fundamentos Técnicos\n- **Backend**: Serviços FastAPI coordenam descoberta de conjunto de dados, orquestração de treinamento e endpoints de estado.\n- **Núcleo ML**: PyTorch lida com loops de treinamento, inferência e pipelines de aumento.\n- **Frontend**: UI web leve HTML/JS/CSS suportada por biblioteca auxiliar básica (`qte.js`) em vez de framework pesado.\n- **Configuração**: Tudo é configurável—sem valores padrão ocultos no código.\n- **Operação Offline**: A aplicação funciona sem acesso à internet. Verificações de atualização opcionais chamam GitHub apenas quando você quiser.\n- **Gerenciamento de Memória**: Utilitários personalizados ajustam tamanhos de lote em tempo real e monitoram uso de GPU/CPU para prevenir erros OOM.\n- **Manipulação de Dados**: Projetos, conjuntos de dados, checkpoints e logs permanecem sob seu controle do sistema de arquivos. Sem sincronização automática, sem espelhamento remoto.\n\n### Privacidade e Conformidade\n- Projetado com expectativas de GDPR em mente: sem dados pessoais deixando seu ambiente por padrão.\n- Você decide o que importar e mantém controle total sobre modificações, exportações ou exclusões.\n- Configurações, logs e checkpoints permanecem no disco, a menos que explicitamente compartilhados.\n- Verificações de atualização são opt-in e transmitem apenas metadados de solicitação; conteúdo do projeto não vai com elas.\n- Se manipular categorias sensíveis (biométricas, médicas, qualquer coisa regulamentada), mapeie essas obrigações contra suas próprias políticas antes de treinar.\n- Sem SDKs de analytics, reportadores de crash ou trackers de terceiros incluídos na aplicação.\n\n### Suporte\nEstou feliz em compartilhar este projeto para seu uso. Se você quiser apoiar meu trabalho, pode me comprar um café no [ko-fi.com/tanathy](https://ko-fi.com/tanathy).\n\n### Licença e Créditos\nHootsight vem com fonte [Roboto](https://fonts.google.com/specimen/Roboto/license), distribuída sob Open Font License, Version 1.1."
  },
  "training_ui": {
    "page_title": "Configuração de Treinamento",
    "page_description": "Configure arquitetura do modelo, parâmetros de treinamento e configurações de otimização.",
    "optimizer_params_title": "Parâmetros do Otimizador",
    "scheduler_params_title": "Parâmetros do Scheduler",
    "loss_params_title": "Parâmetros da Perda",
    "select_type_first": "Selecione um tipo primeiro para ver parâmetros.",
    "no_extra_params": "Nenhum parâmetro adicional para esta seleção."
  },
  "dataset_ui": {
    "page_title": "Conjunto de Dados",
    "page_description": "Explore e analise a estrutura do seu conjunto de dados, rótulos e distribuição de dados.",
    "summary": {
      "project": "Projeto",
      "dataset_type": "Tipo de Conjunto de Dados",
      "total_images": "Total de Imagens",
      "total_labels": "Total de Rótulos",
      "balance_status": "Status de Equilíbrio",
      "balance_score": "Pontuação de Equilíbrio",
      "images_per_label_ideal": "Imagens por Rótulo (Ideal)",
      "min_images": "Mín Imagens",
      "max_images": "Máx Imagens",
      "max_min_ratio": "Razão Máx/Mín"
    },
    "table": {
      "label": "Rótulo",
      "count": "Contagem",
      "percentage": "Porcentagem"
    }
  },
  "projects_ui": {
    "page_title": "Projetos",
    "page_description": "Gerencie e alterne entre diferentes projetos e conjuntos de dados de ML.",
    "card": {
      "images": "Imagens",
      "labels": "Rótulos",
      "balance_score": "Pontuação de Equilíbrio",
      "balance_status": "Equilíbrio",
      "dataset_type": "Tipo de Conjunto de Dados",
      "status": {
        "balanced": "Equilibrado",
        "imbalanced": "Desequilibrado",
        "critical": "Crítico",
        "warning": "Aviso",
        "good": "Bom",
        "poor": "Ruim",
        "excellent": "Excelente",
        "fair": "Justo",
        "ok": "OK",
        "unstable": "Instável"
      }
    },
    "toolbar_hint": "Projetos mantêm datasets, configurações e checkpoints isolados.",
    "toolbar_create": "Criar Novo Projeto",
    "create_disabled_hint": "A criação de projetos não está disponível no momento.",
    "empty_title": "Ainda não há projetos",
    "empty_message": "Use Criar Novo Projeto para gerar pastas dataset, data_source, model e heatmap.",
    "create_title": "Criar novo projeto",
    "create_description": "Nomeie seu projeto para gerar pastas dataset, data_source, model e heatmap.",
    "create_name_label": "Nome do projeto",
    "create_name_placeholder": "ex. wildlife_classification",
    "create_name_hint": "Use apenas letras, números, hífens e sublinhados.",
    "create_cancel": "Cancelar",
    "create_submit": "Criar",
    "create_creating": "Criando...",
    "create_validation_required": "Nome do projeto é obrigatório.",
    "create_validation_length": "Nome do projeto deve ter entre {min} e {max} caracteres.",
    "create_validation_pattern": "Use apenas letras, números, hífens e sublinhados.",
    "create_error_exists": "Já existe um projeto com este nome.",
    "create_success_status": "Projeto {name} criado.",
    "create_error_unknown": "Falha na criação do projeto.",
    "create_network_error": "Falha na solicitação de rede."
  },
  "status_ui": {
    "page_title": "Status",
    "page_description": "Monitore progresso do treinamento, estado do sistema e métricas de desempenho em tempo real."
  },
  "heatmap_ui": {
    "page_title": "Heatmap",
    "page_description": "Gere e visualize mapas de atenção do modelo para entender áreas de foco da predição."
  },
  "environment": {
    "venv_creating": "Criando ambiente virtual em {path}...",
    "venv_created": "Ambiente virtual pronto.",
    "venv_create_failed": "Criação do ambiente virtual falhou: {error}",
    "venv_exists": "Ambiente virtual já existe.",
    "pip_upgrading": "Atualizando pip no ambiente virtual...",
    "pip_upgraded": "Atualização do pip concluída.",
    "pip_upgrade_failed": "Atualização do pip falhou: {error}",
    "cuda_debug_nvcc": "Saída do nvcc --version:\n{output}",
    "cuda_debug_nvcc_error": "Falha ao detectar nvcc: {error}",
    "cuda_debug_nvidia_smi": "Saída do nvidia-smi:\n{output}",
    "cuda_debug_nvidia_smi_error": "Falha ao detectar nvidia-smi: {error}",
    "cuda_debug_detected": "Versão CUDA detectada: {version}",
    "pytorch_install": "Instalando PyTorch para CUDA {cuda} no {platform}...",
    "pytorch_installed": "Instalação do PyTorch concluída.",
    "pytorch_install_failed": "Instalação do PyTorch falhou: {error}",
    "xformers_already_installed": "xFormers já está instalado e atualizado.",
    "xformers_installing": "Instalando xFormers (CUDA {cuda_version})...",
    "xformers_installed": "Instalação do xFormers concluída.",
    "xformers_install_failed": "Instalação do xFormers falhou: {error}",
    "pytorch_skip": "Pulando instalação do PyTorch (CUDA={cuda} detectado, plataforma={platform}).",
    "config_loading": "Carregando configuração do ambiente...",
    "config_loaded": "Configuração do ambiente carregada.",
    "using_compatible_xformers": "Usando índice CUDA {cuda_version} para xFormers.",
    "env_packages_all_installed": "Pacotes do ambiente já estão instalados.",
    "env_packages_progress_desc": "Instalando pacotes do ambiente",
    "env_package_installing": "Instalando pacote do ambiente {package}...",
    "env_package_installed": "Pacote do ambiente instalado: {package}",
    "env_package_install_failed": "Falha ao instalar pacote do ambiente {package}: {error}",
    "env_vars_configured": "Variáveis de ambiente {count} preparadas para processo de treinamento.",
    "env_vars_config_failed": "Falha ao configurar variáveis de ambiente: {error}",
    "entry_not_found": "Script de entrada não encontrado em {path}.",
    "venv_python_not_found": "Executável Python do ambiente virtual faltando: {path}.",
    "venv_python_test_failed": "Python do ambiente virtual falhou no teste --version: {error}",
    "re_exec_starting": "Iniciando treinamento via {venv_python} -> {entry_py} (raiz {root}).",
    "re_exec_timeout": "Re-exec expirou.",
    "re_exec_failed": "Falha ao iniciar processo de treinamento: {error}",
    "re_exec_unexpected_error": "Erro inesperado ao iniciar processo de treinamento: {error}"
  },
  "updates_ui": {
    "page_title": "Atualizações do Sistema",
    "page_description": "Mantenha sua instalação sincronizada com o repositório upstream sem sobrescrever substituições de configuração específicas do projeto.",
    "card_title": "Gerenciador de Atualizações",
    "intro": "Compare arquivos locais com o repositório de referência e sincronize patches faltantes mantendo config.json intocado.",
    "check_button": "Verificar Atualizações",
    "apply_button": "Aplicar Atualizações",
    "apply_disabled_hint": "Execute uma verificação para ativar atualizações.",
    "status_idle": "Nenhuma verificação de atualização executada ainda.",
    "status_checking": "Verificando atualizações...",
    "status_ready": "Resumo de atualizações pronto.",
    "status_up_to_date": "Tudo está atualizado.",
    "status_failed": "Verificação de atualizações falhou.",
    "status_applying": "Aplicando arquivos...",
    "status_applied": "Atualizações aplicadas com sucesso.",
    "status_apply_failed": "Algumas atualizações falharam.",
    "table_header_file": "Arquivo",
    "table_header_status": "Status",
    "table_header_local": "Local",
    "table_header_remote": "Remoto",
    "table_row_missing": "Faltando localmente",
    "table_row_outdated": "Checksum incompatível",
    "table_footnote": "Hashes abreviados para legibilidade.",
    "no_updates": "Todos os arquivos rastreados estão atualizados.",
    "hash_missing": "—",
    "orphaned_title": "Arquivos locais não rastreados",
    "orphaned_none": "Nenhum arquivo extra local detectado."
  },
  "schema": {
    "description": "JSON Schema for Hootsight config.json - defines types, ranges, and hierarchical structure for all configurable settings",
    "general_description": "General application settings",
    "general_language_description": "UI language code",
    "api_description": "API server configuration",
    "api_host_description": "API server host",
    "api_port_description": "API server port",
    "ui_description": "User interface settings",
    "ui_title_description": "Application window title",
    "ui_width_description": "Window width in pixels",
    "ui_height_description": "Window height in pixels",
    "ui_resizable_description": "Whether the window is resizable",
    "system_description": "System-level settings",
    "system_max_threads_description": "Maximum number of threads",
    "system_fallback_batch_size_description": "Fallback batch size when auto-calculation fails",
    "system_memory_cleanup_interval_description": "Memory cleanup interval in seconds",
    "system_thread_pool_timeout_description": "Thread pool timeout in seconds",
    "system_startup_wait_seconds_description": "Startup wait time in seconds",
    "memory_description": "Memory management settings",
    "memory_target_memory_usage_description": "Target memory usage ratio (0.0-1.0)",
    "memory_safety_margin_description": "Safety margin for memory calculations (0.0-1.0)",
    "memory_augmentation_threads_description": "Number of threads for data augmentation",
    "training_description": "Training configuration",
    "training_model_type_description": "Type of model to use",
    "training_model_name_description": "Specific model name",
    "training_pretrained_description": "Initializes the network with ImageNet pretrained weights when available; disable to train from scratch",
    "training_task_description": "Machine learning task type",
    "training_batch_size_description": "Batch size for training",
    "training_epochs_description": "Number of training epochs",
    "training_learning_rate_description": "Learning rate",
    "training_weight_decay_description": "Weight decay (L2 regularization)",
    "training_input_size_description": "Edge length in pixels for the square input tensor (must stay consistent with your augmentation resizing)",
    "training_normalize_description": "Image normalization parameters",
    "training_normalize_mean_description": "Mean values for RGB channels",
    "training_normalize_std_description": "Standard deviation values for RGB channels",
    "training_val_ratio_description": "Validation split ratio (0.0-1.0)",
    "training_dataloader_description": "DataLoader configuration",
    "training_dataloader_num_workers_description": "Number of worker processes",
    "training_dataloader_pin_memory_description": "Whether to pin memory for faster GPU transfer",
    "training_dataloader_persistent_workers_description": "Whether to keep workers alive between epochs",
    "training_dataloader_prefetch_factor_description": "Number of batches to prefetch per worker",
    "training_augmentation_description": "Data augmentation configuration",
    "training_augmentation_train_description": "Training augmentations",
    "training_augmentation_val_description": "Validation augmentations",
    "training_optimizer_type_description": "Optimizer type",
    "training_optimizer_params_description": "Optimizer parameters",
    "training_scheduler_type_description": "Learning rate scheduler type",
    "training_scheduler_params_description": "Scheduler parameters",
    "training_loss_type_description": "Loss function type",
    "training_loss_params_description": "Loss function parameters",
    "training_weight_init_description": "Weight initialization configuration",
    "training_checkpoint_description": "Checkpoint configuration",
    "training_early_stopping_description": "Early stopping configuration",
    "training_gradient_description": "Gradient configuration",
    "training_runtime_description": "Runtime performance optimization settings",
    "training_runtime_mixed_precision_description": "Habilitar treinamento automático de precisão mista",
    "training_runtime_channels_last_description": "Use channels-last memory format for better GPU utilization",
    "training_runtime_allow_tf32_description": "Habilitar TF32 para operações matriciais mais rápidas em GPUs Ampere+",
    "training_runtime_cudnn_benchmark_description": "Habilitar benchmark cuDNN para algoritmos de convolução otimizados",
    "dataset_description": "Dataset configuration",
    "dataset_image_extensions_description": "Supported image file extensions",
    "optimizers_description": "Optimizer defaults override",
    "optimizers_defaults_description": "Default parameters for optimizers",
    "schedulers_description": "Scheduler defaults override",
    "schedulers_defaults_description": "Default parameters for schedulers",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Lambda function as string, e.g., 'lambda epoch: 0.95 ** epoch'",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Lambda function as string, e.g., 'lambda epoch: 0.95'",
    "losses_description": "Loss defaults override",
    "losses_defaults_description": "Default parameters for losses",
    "models_description": "Model configurations",
    "models_resnet_description": "ResNet model family settings",
    "models_resnet_variants_description": "ResNet variants configuration",
    "models_resnext_description": "ResNeXt model family settings",
    "models_resnext_variants_description": "ResNeXt variants configuration",
    "models_mobilenet_description": "MobileNet model family settings",
    "models_mobilenet_variants_description": "MobileNet variants configuration",
    "models_shufflenet_description": "ShuffleNet model family settings",
    "models_shufflenet_variants_description": "ShuffleNet variants configuration",
    "models_squeezenet_description": "SqueezeNet model family settings",
    "models_squeezenet_variants_description": "SqueezeNet variants configuration",
    "models_efficientnet_description": "EfficientNet model family settings",
    "models_efficientnet_variants_description": "EfficientNet variants configuration",
    "models_supported_types_description": "Supported model types",
    "general_language_enum_descriptor": {
      "en": "Idioma inglês - Controla toda a localização da interface do usuário incluindo rótulos de menu, mensagens de erro, dicas de ferramentas e texto de validação. Afeta toda a renderização de texto na interface web e nas respostas da API. Determina o carregamento do pacote de idioma na inicialização da aplicação. Atualmente a única opção de idioma suportada, tornando-a padrão para todas as implementações."
    },
    "system_max_threads_enum_descriptor": {
      "auto": "Contagem automática de threads - Calcula dinamicamente o tamanho ideal do pool de threads baseado nos núcleos de CPU disponíveis (tipicamente núcleos - 1). Afeta carregamento paralelo de dados, pré-processamento de imagens, processamento em lote de inferência do modelo e processamento de tarefas em segundo plano. Controla alocação de threads para workers do PyTorch DataLoader, pipelines de aumento de imagem e manuseio de requisições HTTP concorrentes. Escala automaticamente com capacidades de hardware e ajusta baseado na memória disponível do sistema."
    },
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Contagem automática de threads de aumento - Calcula a contagem ideal de threads para aumento paralelo de imagens baseado em núcleos de CPU e RAM disponível. Afeta o throughput do pipeline de transformação de imagem incluindo rotação, escalonamento, jitter de cores e operações de normalização. Controla alocação de memória para buffers de aumento e armazenamento intermediário de imagens. Equilibra utilização de CPU contra pressão de memória para prevenir sobrecarga do sistema durante fases intensivas de pré-processamento."
    },
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet (Rede Residual) - Rede neural convolucional profunda usando conexões de salto para permitir treinamento de redes muito profundas (18-152 camadas). Afeta fluxo de gradiente, estabilidade de treinamento, profundidade de representação de características e capacidade do modelo. Usa blocos residuais com normalização em lote e ativação ReLU. Controla complexidade arquitetural de 11M parâmetros (ResNet-18) a 60M parâmetros (ResNet-152). Influencia uso de memória, tempo de treinamento, velocidade de inferência e precisão final do modelo em tarefas de classificação de imagem.",
      "resnext": "ResNeXt (Transformações Residuais Agregadas) - Evolução ResNet usando cardinalidade (convoluções agrupadas) para aumentar capacidade do modelo sem aumentar significativamente contagem de parâmetros. Afeta diversidade de aprendizado de características, expressividade do modelo e eficiência computacional. Controla caminhos de transformação paralelos dentro de cada bloco residual. Impacta uso de memória GPU, duração de treinamento e alcança maior precisão que ResNet padrão com custo computacional similar.",
      "mobilenet": "MobileNet - CNN leve usando convoluções separáveis em profundidade para reduzir tamanho do modelo e requisitos computacionais. Afeta latência de inferência, consumo de energia, tamanho de armazenamento do modelo e viabilidade de implantação em dispositivos móveis. Controla trade-off entre precisão e eficiência através de multiplicador de largura e parâmetros de resolução. Impacta vida da bateria em aplicações móveis, capacidade de processamento em tempo real e compatibilidade com dispositivos edge.",
      "shufflenet": "ShuffleNet - CNN extremamente eficiente usando operações de embaralhamento de canais e convoluções de grupo pontuais. Afeta utilização de largura de banda de memória, custo computacional por inferência, tamanho do modelo e velocidade de processamento. Controla comunicação de canais entre convoluções de grupo para manter fluxo de informação. Otimizado para processadores ARM e dispositivos de baixo consumo. Impacta requisitos de desempenho em tempo real e cenários de implantação com recursos limitados.",
      "squeezenet": "SqueezeNet - CNN ultra-compacta usando módulos Fire (camadas squeeze + expand) para alcançar precisão nível AlexNet com 50x menos parâmetros. Afeta requisitos de armazenamento do modelo, tempo de download, eficiência de cache e largura de banda de implantação. Controla contagem de parâmetros através de redução agressiva de dimensionalidade seguida por expansão. Minimiza pegada em disco mantendo precisão razoável para tarefas básicas de classificação.",
      "efficientnet": "EfficientNet - CNN de escalonamento composto que escala uniformemente profundidade, largura e resolução da rede usando busca de arquitetura neural. Afeta eficiência computacional, escalonamento de precisão, requisitos de recursos de treinamento e otimização de inferência. Controla complexidade do modelo através de coeficiente composto que equilibra todas as três dimensões simultaneamente. Fornece trade-offs superiores de precisão-eficiência comparado a métodos tradicionais de escalonamento."
    },
    "training_task_enum_descriptor": {
      "classification": "Classificação Single-label - Atribui exatamente um rótulo de classe mutuamente exclusivo para cada imagem de entrada. Afeta arquitetura da camada final (ativação softmax), seleção de função de perda (entropia cruzada categórica), dimensionalidade de saída (número de classes), e interpretação de confiança de predição. Controla fronteiras de decisão do modelo, distribuição de probabilidade de classe, e padrões de convergência de treinamento. Requer distribuição balanceada de dataset e separabilidade clara de classes para performance ótima.",
      "multi_label": "Classificação Multi-rótulo - Atribui zero, um, ou múltiplos rótulos de classe não exclusivos simultaneamente a cada imagem de entrada. Afeta ativação da camada de saída (sigmoid por classe), composição de função de perda (entropia cruzada binária por rótulo), seleção de limiar para predições positivas, e métricas de avaliação (F1-score, mAP). Controla caminhos independentes de predição de classe, tratamento de correlação de rótulos, e estratégias de peso de classe desbalanceada. Lida com cenários complexos do mundo real onde imagens contêm múltiplos conceitos semânticos.",
      "detection": "Detecção de Objetos - Localiza e classifica simultaneamente múltiplas instâncias de objetos dentro de imagens usando predições de caixas delimitadoras. Afeta complexidade de arquitetura do modelo (redes de pirâmide de características, geração de âncoras), composição de função de perda (classificação + regressão de caixa delimitadora), requisitos de dados de treinamento (caixas delimitadoras anotadas), pipeline de pós-processamento (supressão não-máxima) e overhead computacional. Controla profundidade de extração de características espaciais, reconhecimento de objetos multi-escala, mecanismos de proposta de região e cálculos de interseção sobre união.",
      "segmentation": "Segmentação Semântica - Realiza classificação densa pixel a pixel para atribuir rótulos de classe semântica a cada pixel na imagem de entrada. Afeta requisitos de memória (mapas de características de resolução completa), arquitetura do modelo (codificador-decodificador com conexões de salto), design de função de perda (entropia cruzada pixel a pixel, perda focal para desequilíbrio de classe), complexidade de treinamento (manuseio de desequilíbrio de classe ao nível do pixel) e restrições de resolução de saída. Controla estratégias de upsampling, qualidade de refinamento de fronteiras, precisão espacial e capacidades de raciocínio contextual."
    },
    "training_epochs_enum_descriptor": {
      "auto": "Determinação automática de épocas - Monitora tendências de perda de validação e acurácia para determinar duração ideal de treinamento usando critérios de parada antecipada. Afeta tempo total de treinamento, qualidade de convergência do modelo, uso de recursos computacionais e prevenção de overfitting. Rastreia melhorias de métricas de validação durante períodos de paciência e automaticamente termina o treinamento quando nenhum progresso significativo é detectado. Equilibra minuciosidade do treinamento contra eficiência computacional."
    },
    "training_optimizer_type_enum_descriptor": {
      "sgd": "Descida Estocástica de Gradiente - Passo de gradiente determinístico com momentum opcional e lookahead de Nesterov. Expõe learning_rate, momentum, dampening e weight_decay como controles críticos. Funciona melhor quando você pode pré-planejar um scheduler e quer controle rígido sobre generalização. Espere resultados fortes em grandes datasets de visão quando pareado com decaimento cosseno ou por passos, mas esteja pronto para ajustar momentum (0.9 é ponto de partida típico) e manter taxas de aprendizado entre 0.01-0.1 dependendo do tamanho do lote.",
      "adam": "Otimizador Adam - Método adaptativo de primeira ordem que armazena médias móveis de gradientes (beta1) e gradientes ao quadrado (beta2). Betas padrão de 0.9/0.999 e eps de 1e-8 servem para a maioria das cargas de trabalho. Lida com gradientes ruidosos ou esparsos sem escalonamento manual da taxa de aprendizado, tornando-o uma linha de base confiável para classificação e aprendizado por transferência. Observe convergência lenta se weight decay estiver acoplado às atualizações adaptativas do Adam; considere AdamW quando regularização importar.",
      "adamw": "Otimizador AdamW - Desacopla weight decay das atualizações adaptativas do Adam para que regularização L2 se comporte como pretendido. Mantém os mesmos parâmetros beta e padrões epsilon do Adam enquanto expõe weight_decay como um verdadeiro regularizador. Preferido para vision transformers, fine-tuning ResNet, ou qualquer modelo onde você se preocupa com treinamento estável com generalização previsível. Comece com weight_decay em torno de 0.01 e ajuste learning_rate entre 3e-5 e 3e-4 para cenários de aprendizado por transferência.",
      "adamax": "Otimizador AdaMax - Variante do Adam usando norma infinita para rastreamento de segundo momento. Hiperparâmetros similares ao Adam, mas mais resiliente quando gradientes têm picos esporádicos. Útil quando Adam fica instável devido a magnitudes extremas de gradiente, particularmente em cargas de trabalho GAN ou reinforcement learning. Mantenha beta2 próximo a 0.999 e trate learning_rate como Adam padrão; espere convergência ligeiramente mais lenta mas menos saltos catastróficos.",
      "nadam": "Adam Acelerado por Nesterov - Adiciona momentum Nesterov sobre o escalonamento adaptativo do Adam. Compartilha os mesmos betas e epsilon mas realiza avaliação de gradiente lookahead, que pode apertar convergência em objetivos suaves. Planeje overhead computacional modesto por passo. Recomendado quando Adam converge mas plateou cedo; ajuste learning_rate ligeiramente menor que Adam puro para evitar ultrapassagem (ex: 1e-4 em vez de 3e-4).",
      "radam": "Adam Retificado - Adam com mecanismo de warmup automático derivado de retificação de variância. Elimina necessidade de cronograma de warmup manual ao encolher tamanhos de passo até que variância corrente estabilize. Hiperparâmetros coincidem com padrões Adam. Aproveite quando precisa de comportamento adaptativo mas seu treinamento é sensível às primeiras centenas de passos. Funciona bem para datasets pequenos onde warmup manual causaria overfitting.",
      "rmsprop": "Otimizador RMSprop - Mantém média exponencial de gradientes ao quadrado (alpha) para normalizar atualizações. Padrão alpha=0.99 e eps=1e-8. Historicamente popular para redes recorrentes e aprendizado por reforço, ainda funciona bem quando gradientes oscilam pesadamente e Adam parece muito agressivo. Pareie com cronograma decrescente de learning_rate; valores iniciais típicos ficam próximos a 1e-3 com momentum desabilitado ou baixo (≤0.1).",
      "rprop": "Retropropagação Resiliente - Otimizador baseado em sinal que adapta tamanhos de passo por parâmetro usando apenas inversões de sinal de gradiente. Ignora tamanho de lote porque assume atualizações de lote completo, então raramente é apropriado para treinamento CNN mini-lote. Use apenas em configurações determinísticas (ex: datasets pequenos com passadas de lote completo) onde quer convergência rápida tipo segunda ordem sem armazenar Hessiana. Hiperparâmetros eta_plus (1.2) e eta_minus (0.5) governam adaptação de passo.",
      "adagrad": "Otimizador Adagrad - Acumula gradientes ao quadrado, diminuindo taxa de aprendizado para pesos frequentemente atualizados. Quase sem manutenção em características esparsas, mas a soma cumulativa força taxa de aprendizado efetiva para zero em execuções longas. Use para embeddings de características ou problemas NLP esparsos clássicos, não para CNNs profundas que treinam centenas de épocas. Learning_rate inicial típico é 1e-2 com epsilon em torno de 1e-10 para evitar divisão por zero.",
      "adadelta": "Otimizador Adadelta - Corrige taxa de aprendizado desvanecente do Adagrad rastreando janela móvel de gradientes ao quadrado e atualizações. Requer quase nenhum ajuste manual além de rho (0.9) e eps (1e-6). Funciona em objetivos ruidosos onde Adam pode ser muito agressivo, embora sua precisão final frequentemente fique atrás do AdamW. Prefira quando deve evitar cronogramas manuais de taxa de aprendizado e ainda precisa de comportamento adaptativo.",
      "sparse_adam": "Adam Esparso - Adam com atualizações aplicadas apenas a índices que recebem gradientes, reduzindo memória e computação para tabelas de embedding. Usa os mesmos hiperparâmetros que Adam mas assume que gradientes são zero quase em todo lugar. Essencial para modelos NLP com vocabulários enormes. Pule para modelos convolucionais densos; a contabilidade de atualizações esparsas apenas desperdiça tempo.",
      "lbfgs": "Otimizador L-BFGS - Método quasi-Newton de memória limitada que aproxima Hessiana inversa usando gradientes passados. Requer gradientes de lote completo e busca de linha por passo, então deve implementar closure que recomputa perda e gradientes. Excelente para fine-tuning modelos pequenos ou resolver problemas convexos com alta precisão. Não viável para treinamento mini-lote grande porque cada passo é caro e memória cresce com tamanho do histórico (max_iter e history_size controlam isso).",
      "asgd": "SGD Médio - Mantém média corrente de parâmetros para amortecer oscilações causadas por gradientes ruidosos. Ainda ajusta learning_rate SGD base, mas média entra em ação após época averaging_start para suavizar convergência. Considere quando SGD simples balança no final do treinamento mas quer evitar mudar para Adam. Funciona melhor com taxas de aprendizado constantes ou decaindo lentamente e momentum desligado."
    },
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Agendador Taxa de Aprendizado por Passos - Multiplica taxa de aprendizado por gamma a cada step_size épocas. Perfeito quando já sabe as épocas onde progresso diminui (ex: 30/60/90 no ImageNet). Escolha gamma entre 0.1 e 0.3 e alinhe step_size com seu orçamento total de épocas. Sem conhecimento prévio pode parecer abrupto, então monitore métricas de validação para confirmar que as quedas estão ajudando.",
      "multi_step_lr": "Agendador Taxa Multi-Passos - Cronograma de passos generalizado que aceita lista de épocas marco. Permite escalonar múltiplas quedas de taxa em pontos arbitrários, ideal para portar cronogramas de artigos ou experimentos anteriores. Mantenha gamma idêntico entre marcos a menos que tenha razão para variá-lo, e garanta que marcos sejam inteiros estritamente crescentes.",
      "exponential_lr": "Agendador Taxa Exponencial - Aplica lr_t = lr_0 * gamma^t, dando decaimento suave em troca de ajuste cuidadoso de gamma. Funciona para execuções muito longas onde quer deslizamento gradual em vez de saltos discretos. Valores típicos de gamma ficam próximos a 0.97 e 0.995 para atualizações por época. Combine com warmup se inclinação inicial for muito íngreme para seu modelo.",
      "cosine_annealing_lr": "Agendador Taxa Cosine Annealing - Varre taxa de aprendizado para baixo seguindo curva cosseno sobre T_max épocas e opcionalmente reinicia em eta_min. Fornece aterrissagens suaves que impulsionam precisão final em modelos de visão. Defina T_max para número de épocas em ciclo e eta_min para piso pequeno como lr_0 / 100. Use quando quiser fine-tuning automático próximo ao final sem marcos manuais.",
      "cosine_annealing_warm_restarts": "Cosine Annealing with Warm Restarts - Repeats cosine decay cycles, resetting to the initial learning rate after each cycle. Great for escaping shallow minima during long training sessions. T_0 defines the first cycle length, and T_mult scales subsequent cycle lengths. Keep eta_min small but non-zero to avoid freezing the optimizer.",
      "reduce_lr_on_plateau": "Reduzir Taxa em Platô - Observa uma métrica (geralmente perda de validação) e diminui taxa de aprendizado por fator quando melhoria estagna por épocas de paciência. Essencial quando não pode prever timing de platô. Configure cooldown para evitar triggers consecutivos e use threshold para filtrar métricas ruidosas. Gamma entre 0.1 e 0.5 tipicamente atinge equilíbrio correto.",
      "cyclic_lr": "Agendador Taxa Aprendizado Cíclica - Faz ciclos de taxa de aprendizado entre base_lr e max_lr em janelas curtas, opcionalmente reduzindo amplitude usando modo. Útil para convergência rápida em objetivos difíceis ou testes de faixa LR. Configure step_size_up/down para número de iterações por meio-ciclo; mantenha max_lr aproximadamente 3-10× base_lr. Combine com cycling de momentum se habilitar cycle_momentum.",
      "one_cycle_lr": "Política One Cycle Taxa Aprendizado - Varredura única que aumenta taxa de aprendizado até max_lr então reduz a fração do valor base enquanto inverte momentum. Entrega convergência rápida quando passos totais de treinamento são conhecidos. Forneça total_steps ou (epochs × steps_per_epoch); defina pct_start para definir proporção de warmup (0.3 é comum). Funciona melhor com SGD ou AdamW e não espera agendadores adicionais.",
      "polynomial_lr": "Agendador Taxa Polinomial - Decai taxa de aprendizado a zero seguindo (1 - t/T)^power. Escolha total_iters como número de passos do otimizador no cronograma e power para controlar curvatura (1 para linear, 2 para quadrática). Útil para cargas de trabalho de segmentação e detecção onde quer deslizamento determinístico a zero pela iteração final.",
      "linear_lr": "Linear Learning Rate Scheduler - Simple linear interpolation between start_factor and end_factor over total_iters steps. Ideal for warmup (start_factor < 1) or cool-down phases. Keep total_iters aligned with the number of iterations you want the ramp to cover; combine with another scheduler for the remaining training window.",
      "lambda_lr": "Agendador Taxa Lambda - Gancho direto que multiplica taxa de aprendizado base por sua função lambda(epoch) personalizada. Dá controle total para cronogramas de pesquisa ou aprendizado curricular. Forneça expressão Python que avalia para float; lembre que será avaliada como string dentro do processo de treinamento. Valide função cuidadosamente—erros de sintaxe ou saídas negativas matarão sua execução.",
      "multiplicative_lr": "Agendador Taxa Multiplicativo - Similar ao lambda_lr mas espera chamável que retorna multiplicador a cada passo, frequentemente usado para escalonamento época por época. Forneça lambda que depende de contagem de passos do otimizador ao invés de época se precisar controle por iteração. Mantenha multiplicadores positivos e limitados; valores >1 crescem taxa de aprendizado e podem desestabilizar treinamento rapidamente."
    },
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Perda Entropia Cruzada - Softmax + negative log-likelihood em uma chamada. A escolha preferencial para classificação single-label. Aceita logits brutos, lida com desequilíbrio de classes via peso ou label_smoothing, e fornece probabilidades calibradas. Mantenha reduction='mean' para gradientes estáveis e monitore label_smoothing para não apagar classes minoritárias.",
      "nll_loss": "Negative Log-Likelihood Loss - Same math as cross-entropy but expects you to call log_softmax yourself. Useful when the model already outputs log-probabilities (e.g., custom temperature scaling or mixed precision under manual control). Make sure inputs are log probabilities; feeding raw logits will silently give garbage.",
      "bce_loss": "Binary Cross-Entropy Loss - Works on probabilities in [0,1], so pair it with an explicit sigmoid. Suitable for binary classification when you need to control the activation separately. Beware of numerical underflow on extreme logits—clip the inputs or switch to BCEWithLogitsLoss if you see NaNs.",
      "bce_with_logits": "Entropia Cruzada Binária com Logits - BCE numericamente estável que aplica sigmoid internamente. Opção padrão para classificação multi-rótulo e tarefas binárias. Suporta pos_weight para desequilíbrio de classe sem truques manuais de ponderação. Produz perda ilimitada se você esquecer de restringir alvos a {0,1}.",
      "multi_margin": "Multi-Class Margin Loss - Margin-based classification objective (hinge-style) that pushes the correct class score above others by at least margin. Offers optional L1 or L2 norms via parameter p. Use it when you want large-margin behavior instead of probabilistic cross-entropy, but note that it can converge slower without careful learning rate control.",
      "multi_label_margin": "Multi-Label Margin Loss - Extends margin loss to multi-label problems by ranking positive classes ahead of negatives. Requires targets to be encoded as index lists and therefore is tricky to integrate with dense label tensors. Reserve it for research scenarios that explicitly call for margin ranking in multi-label space.",
      "multi_label_soft_margin": "Perda Margem Suave Multi-Rótulo - Aplica formulação de margem suave sobre ativações sigmoid, produzindo gradientes mais suaves que perdas de margem dura. Melhor em lidar com rótulos sobrepostos e desequilíbrio que BCE vanilla. Alvos ainda devem ser {0,1}; considere ajuste de limiar na inferência para explorar panorama de treinamento mais suave.",
      "mse_loss": "Mean Squared Error Loss - Classic L2 regression penalty. Penalizes large errors quadratically, which magnifies the impact of outliers. Great for autoencoders and low-noise regression, but consider clipping extreme targets or swapping to Huber when you see gradient explosions.",
      "l1_loss": "L1 Loss (Mean Absolute Error) - Linear penalty on absolute error, offering robustness to outliers at the cost of slower convergence near zero. Use it when you need median-like behavior or when your evaluation metric is MAE. Gradients are constant magnitude, so combine with smooth schedulers to avoid jitter.",
      "smooth_l1": "Smooth L1 Loss - Huber-style loss with a beta region that behaves like L2 near zero and L1 outside. Default choice for bounding-box regression (beta ≈ 1). Tune beta if your scale differs significantly; smaller beta tightens the quadratic window and gives sharper penalties to mid-sized errors.",
      "huber_loss": "Huber Loss - Similar to SmoothL1 but parameterized by delta instead of beta. Offers explicit control over the switch point between quadratic and linear penalties. Excellent for regression tasks with occasional outliers; set delta close to your expected noise standard deviation.",
      "kl_div": "Perda Divergência Kullback-Leibler - Mede divergência entre distribuição predita e distribuição alvo. Requer log-probabilidades como entrada e probabilidades brutas como alvo por padrão (ou vice-versa com log_target). Essencial para destilação de conhecimento e modelos variacionais. Verifique duplo modo de redução; 'batchmean' preserva teoria KL (somando sobre classes e fazendo média sobre lote).",
      "margin_ranking": "Margin Ranking Loss - Operates on pairs of scores (x1, x2) with ground-truth ordering y ∈ {−1, 1}. Trains the model to rank x1 above x2 by at least margin when y=1. Combine it with careful sampling of positive/negative pairs or triplets—random pairs rarely convey useful signal.",
      "hinge_embedding": "Perda Hinge Embedding - Para aprendizado de similaridade onde rótulos indicam se pares devem estar próximos (+1) ou distantes (−1). Penaliza distâncias que violam margem especificada. Use quando tiver apenas supervisão binária igual/diferente e quiser embeddings agrupados de acordo.",
      "triplet_margin": "Triplet Margin Loss - Consumes anchor, positive, and negative embeddings and enforces a margin between positive and negative distances. Requires hard or semi-hard triplet mining to shine; naive random triplets usually waste computation. Margin defaults to 1.0 but tune it based on embedding scale (smaller for normalized vectors).",
      "cosine_embedding": "Cosine Embedding Loss - Optimizes cosine similarity directly, emphasizing angular distance over magnitude. Ideal when vectors are normalized or when direction carries the semantics (e.g., face recognition). Ensure embeddings are normalized to avoid mixing magnitude effects back in.",
      "ctc_loss": "Connectionist Temporal Classification Loss - Aligns variable-length inputs to target label sequences without frame-level annotation. Requires log-probabilities with size (T, N, C) and target sequences without blanks inserted (the loss handles blanks). Configure blank index and ensure targets are sorted by sample; mis-sized target lengths will throw runtime errors.",
      "poisson_nll": "Perda Poisson Negative Log-Likelihood - Para modelagem de dados de contagem onde alvos são inteiros não-negativos. Aceita log_input para enforçar predições positivas ou logits completos com clamp para ficar acima de zero. Defina full=True se seu modelo prediz taxas brutas. Não alimente alvos negativos; suposição de distribuição quebra imediatamente.",
      "gaussian_nll": "Perda Gaussian Negative Log-Likelihood - Treina modelo para dar saída tanto média quanto variância para alvos contínuos. Espera que modelo retorne tensores (média, variância). Suporta covariância completa via cholesky_factor; caso contrário variância deve permanecer positiva. Ótimo para regressão consciente de incerteza; adicione pequeno epsilon à variância para evitar problemas log(0)."
    },
    "training_loss_reduction_enum_descriptor": {
      "mean": "Redução Média - Computa perda média através de todos elementos do lote dividindo perda total por tamanho do lote. Afeta normalização de magnitude de gradiente, independência de tamanho de lote, estabilidade de treinamento e sensibilidade de taxa de aprendizado. Controla escalonamento de perda para fornecer gradientes consistentes independentemente de variações de tamanho de lote. Escolha padrão para maioria dos cenários de treinamento pois mantém magnitudes de gradiente proporcionais a erros de amostra individual ao invés de tamanho de lote.",
      "sum": "Redução Soma - Computa perda total somando todas perdas individuais de amostra no lote sem normalização. Afeta escalonamento de magnitude de gradiente, dependência de tamanho de lote, requisitos de taxa de aprendizado e dinâmicas de treinamento. Controla acumulação de perda que resulta em gradientes maiores para lotes maiores, requerendo ajuste de taxa de aprendizado proporcional ao tamanho do lote. Útil quando quer que magnitude de gradiente escalone com número de amostras processadas.",
      "none": "Nenhuma Redução - Retorna valores de perda individuais para cada amostra no lote sem operação de agregação. Afeta capacidades de ponderação de perda personalizada, análise específica de amostra, combinação manual de perda e estratégias avançadas de treinamento. Controla acesso de perda de amostra individual para implementar esquemas de redução personalizados, ponderação de importância de amostra ou análise detalhada de perda. Essencial para aplicações avançadas requerendo manipulação de perda por amostra."
    },
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Monitoramento Perda Validação - Mecanismo de parada antecipada que rastreia valores de perda de validação para determinar quando treinamento deve parar devido à falta de melhoria. Afeta prevenção de overfitting, otimização de duração de treinamento, qualidade de generalização do modelo e uso de recursos computacionais. Controla terminação de treinamento baseada em platô de perda, que tipicamente indica que modelo aprendeu padrões generalizáveis e treinamento adicional pode levar a overfitting. Particularmente efetivo para tarefas de regressão e situações onde minimização de perda correlaciona diretamente com qualidade do modelo.",
      "val_accuracy": "Monitoramento Acurácia Validação - Mecanismo de parada antecipada que rastreia métricas de acurácia de validação para determinar ponto ótimo de terminação de treinamento. Afeta otimização de performance de modelo, detecção de overfitting, eficiência de treinamento e qualidade final do modelo. Controla parada de treinamento baseada em platôs de acurácia, focando em performance de classificação ao invés de minimização de perda. Mais adequado para tarefas de classificação balanceadas onde acurácia é métrica primária de sucesso e correlaciona bem com capacidade de generalização do modelo."
    },
    "optimizers_defaults_lbfgs_line_search_fn_oneOf[1]_enum_descriptor": {
      "strong_wolfe": "Busca de Linha Strong Wolfe - Algoritmo avançado de busca de linha para otimização L-BFGS que assegura tanto diminuição suficiente (condição Armijo) quanto condições de curvatura (condições Strong Wolfe). Afeta qualidade de convergência de otimização garantindo tamanhos de passo apropriados que satisfazem critérios de optimalidade matemática. Controla seleção de comprimento de passo através de condições matemáticas rigorosas que asseguram propriedades de convergência mantendo eficiência computacional. Essencial para garantias teóricas L-BFGS e fornece seleção robusta de tamanho de passo para métodos de otimização quasi-Newton."
    },
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Modo Mínimo - Monitora métricas onde valores mais baixos indicam melhor performance (como perda de validação). Afeta trigger de redução de taxa de aprendizado rastreando quando métrica monitorada para de diminuir abaixo do threshold pelo período de paciência especificado. Controla comportamento de agendador para reduzir taxa de aprendizado quando perda platea, prevenindo estagnação de treinamento. Ótimo para monitoramento baseado em perda onde valores decrescentes representam progresso de treinamento.",
      "max": "Modo Máximo - Monitora métricas onde valores mais altos indicam melhor performance (como acurácia de validação). Afeta trigger de redução de taxa de aprendizado rastreando quando métrica monitorada para de aumentar acima do threshold pelo período de paciência especificado. Controla comportamento de agendador para reduzir taxa de aprendizado quando acurácia platea, permitindo fine-tuning adicional. Ótimo para monitoramento baseado em acurácia onde valores crescentes representam progresso de treinamento."
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Modo Threshold Relativo - Define threshold de melhoria como porcentagem do melhor valor de métrica atual. Afeta sensibilidade a melhorias de métrica requerendo mudanças proporcionais relativas ao nível de performance atual. Controla escalonamento adaptativo de threshold que se torna mais rigoroso conforme performance do modelo melhora. Útil quando magnitude de melhoria deve escalonar com valores de métrica atuais, prevenindo redução prematura de taxa de aprendizado em modelos de alta performance.",
      "abs": "Modo Threshold Absoluto - Define threshold de melhoria como valor absoluto fixo que deve ser excedido independentemente do nível de métrica atual. Afeta sensibilidade de detecção de melhoria através de requisitos de threshold constantes independentes de performance atual. Controla padrões uniformes de melhoria ao longo do treinamento independentemente de magnitude de métrica. Útil quando níveis consistentes de melhoria são requeridos independentemente do estado de performance atual do modelo."
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Modo Ciclo Triangular - Cria ciclos básicos triangulares de taxa de aprendizado com amplitude constante ao longo do treinamento. Afeta padrão de oscilação de taxa de aprendizado através de aumentos e diminuições lineares entre fronteiras base_lr e max_lr. Controla equilíbrio consistente exploração-exploração com faixa fixa de taxa de aprendizado. Fornece padrões simples cíclicos de taxa de aprendizado adequados para encontrar faixas ótimas de taxa de aprendizado sem decaimento.",
      "triangular2": "Modo Ciclo Triangular2 - Cria ciclos triangulares de taxa de aprendizado com amplitude reduzindo pela metade após cada ciclo completo. Afeta redução de faixa de taxa de aprendizado ao longo do tempo mantendo padrão de oscilação triangular. Controla decaimento gradual de taxa de aprendizado dentro de framework cíclico, combinando benefícios de cycling com refinamento progressivo. Permite exploração inicial agressiva com ajustes cada vez mais conservativos de taxa de aprendizado.",
      "exp_range": "Modo Faixa Exponencial - Escalona amplitude de ciclo de taxa de aprendizado exponencialmente usando fator gamma para ajuste dinâmico de faixa. Afeta modificação de limites de taxa de aprendizado através de escalonamento exponencial de max_lr relativo ao número de ciclo. Controla evolução sofisticada de amplitude que pode aumentar ou diminuir magnitude de ciclo baseado no valor gamma. Fornece padrões avançados cíclicos de taxa de aprendizado com modulação exponencial de amplitude."
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Modo Escalonamento Baseado em Ciclo - Aplica função de escalonamento baseada em contagem de ciclos completos ao invés de passos de iteração individuais. Afeta frequência de avaliação de função de escala e ritmo de modificação de amplitude de taxa de aprendizado. Controla aplicação de escalonamento em fronteiras de ciclo, habilitando ajustes de amplitude diferentes para cada ciclo completo de taxa de aprendizado. Útil quando comportamento de escalonamento deve mudar discretamente entre ciclos ao invés de continuamente ao longo do treinamento.",
      "iterations": "Iteration-based Scaling Mode - Applies scaling function based on total iteration count since training start, providing continuous scaling evolution. Affects scale function evaluation at every step, enabling smooth amplitude transitions throughout training process. Controls fine-grained scaling application that evolves continuously rather than at discrete cycle boundaries. Useful when gradual, continuous scaling changes are preferred over step-wise cycle-based adjustments."
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Estratégia Annealing Cosseno - Usa função cosseno para transições suaves de taxa de aprendizado com mudanças graduais em extremos e mudanças mais acentuadas em regiões médias. Afeta suavidade de trajetória de taxa de aprendizado fornecendo fases naturais de aceleração e desaceleração. Controla evolução sinusoidal de taxa de aprendizado que imita dinâmicas naturais de otimização. Particularmente efetivo para alcançar convergência suave com oscilações reduzidas próximo a fronteiras de taxa de aprendizado.",
      "linear": "Estratégia Annealing Linear - Usa interpolação linear para mudanças de taxa de aprendizado de taxa constante ao longo do ciclo. Afeta transições de taxa de aprendizado através de progressão uniforme sem fases de aceleração ou desaceleração. Controla evolução previsível e estável de taxa de aprendizado com taxa de mudança consistente. Alternativa mais simples quando transições suaves de cosseno não são necessárias e progressão uniforme de taxa de aprendizado é preferida."
    },
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "Norma L1 (Distância Manhattan) - Usa diferenças absolutas para cálculo de margem na função de perda multi-margem. Afeta computação de perda através de penalidade linear que trata todos os erros uniformemente independentemente da magnitude. Controla medição de distância usando soma de diferenças absolutas entre valores preditos e alvos. Fornece cálculo robusto de margem menos sensível a outliers comparado à norma L2, tornando adequado quando dados de treinamento contêm ruído significativo ou valores extremos.",
      "2": "Norma L2 (Distância Euclidiana) - Usa diferenças ao quadrado para cálculo de margem na função de perda multi-margem. Afeta computação de perda através de penalidade quadrática que penaliza pesadamente erros maiores sendo leniente com menores. Controla medição de distância usando soma de diferenças ao quadrado entre valores preditos e alvos. Fornece características suaves de gradiente e forte penalidade para grandes violações de margem, tornando adequado para dados limpos onde erros grandes devem ser fortemente desencorajados."
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "No Reduction - Returns individual KL divergence values for each sample without aggregation. Affects loss output dimensionality by preserving per-sample loss values for custom processing or analysis. Controls individual sample loss access enabling sample-specific weighting, filtering, or detailed loss examination. Essential for advanced training strategies requiring per-sample loss manipulation or when implementing custom reduction schemes.",
      "mean": "Redução Média - Computa divergência KL média através de todos elementos no lote incluindo dimensões espaciais. Afeta escalonamento de gradiente normalizando magnitude de perda relativa ao número total de elementos ao invés de apenas tamanho do lote. Controla escalonamento de perda que considera tanto tamanho do lote quanto dimensões espaciais em tarefas de predição densa. Fornece gradientes consistentes independentemente de resolução de entrada ou composição do lote.",
      "sum": "Redução Soma - Computa divergência KL total somando todas divergências de elementos individuais sem normalização. Afeta escalonamento de magnitude de gradiente proporcionalmente ao número total de elementos no lote e dimensões espaciais. Controla acumulação de perda que resulta em gradientes maiores para lotes maiores ou entradas de resolução mais alta. Requer ajuste cuidadoso de taxa de aprendizado quando tamanhos de lote ou dimensões de entrada variam significativamente.",
      "batchmean": "Redução Média de Lote - Computa divergência KL média apenas sobre dimensão de lote, preservando contribuições de dimensão espacial. Afeta computação de perda fazendo média através de amostras mantendo contribuição completa de perda espacial. Controla redução padrão de divergência KL que foca em média por amostra ao invés de normalização por elemento. Método de redução recomendado para perda de divergência KL pois preserva propriedades teóricas fornecendo dinâmicas estáveis de treinamento."
    },
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW Optimizer for ResNet - Adam with decoupled weight decay specifically tuned for ResNet architectures. Affects regularization effectiveness and training stability through proper separation of gradient-based adaptation and L2 penalty. Controls weight decay independently from gradient updates, preventing regularization interference with adaptive learning rates. Recommended default choice for ResNet models due to superior generalization performance and stable training characteristics across various ResNet depths and datasets.",
      "adam": "Otimizador Adam para ResNet - Algoritmo clássico de estimação de momento adaptativo fornecendo escalonamento automático de taxa de aprendizado para treinamento ResNet. Afeta velocidade de convergência e sensibilidade de hiperparâmetros através de adaptação de taxa de aprendizado por parâmetro. Controla momento e estimativas de gradiente ao quadrado para equilibrar exploração e exploração durante otimização ResNet. Boa escolha de propósito geral oferecendo performance confiável através de diferentes variantes ResNet com requisitos mínimos de ajuste de hiperparâmetros.",
      "sgd": "SGD com Momentum para ResNet - Descida estocástica de gradiente tradicional com momentum especificamente configurado para treinamento ResNet. Afeta dinâmicas de treinamento através de acumulação de momentum e requer agendamento cuidadoso de taxa de aprendizado para performance ótima. Controla atualizações de parâmetros através de aceleração baseada em momentum mantendo comportamento de otimização determinístico. Escolha tradicional que alcança excelentes resultados com ajuste adequado mas requer seleção mais cuidadosa de hiperparâmetros comparado a otimizadores adaptativos."
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Step Learning Rate for ResNet - Step-based learning rate decay with predetermined milestone epochs optimized for ResNet training phases. Affects learning rate reduction at specific training stages corresponding to ResNet convergence patterns. Controls deterministic learning rate schedule that aligns with typical ResNet training progression and loss landscape characteristics. Suitable when optimal decay epochs are known from prior ResNet experiments or research papers with similar datasets and architectures.",
      "cosine_annealing_lr": "Cosine Annealing for ResNet - Smooth cosine-based learning rate schedule that often improves ResNet final accuracy through gradual learning rate reduction. Affects training dynamics by providing smooth transitions that help ResNet models achieve better final convergence. Controls sinusoidal learning rate progression that reduces training oscillations and enables fine-tuned parameter adjustment in later training phases. Frequently achieves superior final accuracy compared to step-based schedules for ResNet architectures.",
      "reduce_lr_on_plateau": "Adaptive Scheduling for ResNet - Plateau-based learning rate reduction that automatically responds to ResNet training stagnation periods. Affects learning rate adaptation through validation metric monitoring, reducing learning rate when ResNet training progress plateaus. Controls automatic schedule adjustment that responds to actual training dynamics rather than predetermined milestones. Optimal for ResNet training when optimal decay timing is unknown or when training characteristics vary across different datasets or experimental conditions."
    },
    "paths_description": "Paths configuration",
    "paths_projects_dir_description": "Directory containing project folders",
    "paths_ui_dir_description": "Directory containing UI assets",
    "paths_config_dir_description": "Directory containing configuration files",
    "paths_localizations_dir_description": "Directory containing localization files",
    "paths_packages_file_description": "Path to packages.jsonc file",
    "paths_mappings_file_description": "Path to mappings file",
    "paths_cache_dir_description": "Directory for cache files"
  },
  "status_graph": {
    "epoch_accuracy": "Epoch Accuracy",
    "epoch_loss": "Epoch Loss",
    "step_loss": "Step Loss",
    "learning_rate": "Learning Rate",
    "loss": "Loss",
    "no_data": "Waiting for updates",
    "no_training": "No active training.",
    "active_count": "Active trainings: {count}",
    "label_training_id": "Training ID",
    "label_status": "Status",
    "label_phase": "Phase",
    "label_epoch": "Epoch",
    "label_step": "Step",
    "badge_training": "Training: {project}",
    "footer_training": "Training {project} — Epoch {epoch} • Step {step}"
  },
  "updates": {
    "log": {
      "check_started": "Checking upstream checksums...",
      "check_complete": "Update check complete. Pending files: {count}",
      "check_failed": "Update check failed: {error}",
      "remote_config_failed": "Failed to download remote config.json: {error}",
      "remote_checksum_failed": "Failed to download remote checksum manifest: {error}",
      "remote_payload_invalid": "Remote payload from {url} was not a mapping.",
      "local_checksum_missing": "Local checksum.json missing; assuming empty manifest.",
      "local_checksum_invalid": "Failed to parse local checksum.json: {error}",
      "path_escape": "Blocked unsafe path {path}",
      "apply_started": "Applying updates...",
      "apply_failed": "Failed to apply updates: {error}",
      "apply_nothing": "No updates required.",
      "apply_file_success": "Updated {path}",
      "apply_file_failed": "Failed to update {path}: {error}",
      "apply_partial": "Applied {updated} updates with {failed} failures.",
      "apply_complete": "Applied {count} updates."
    },
    "api": {
      "check_success": "Update check completed. {count} file(s) pending.",
      "check_no_updates": "Everything is already up to date.",
      "nav": {
        "training_group": "Treinamento",
        "projects": "Projetos",
        "dataset": "Conjunto de dados",
        "training_setup": "Configuração de Treinamento",
        "augmentation": "Augmentação",
        "status_group": "Status",
        "status": "Status",
        "heatmap": "Mapa de calor",
        "system_group": "Sistema",
        "updates": "Atualizações",
        "docs": "Documentação",
        "about": "Sobre"
      },
      "page": {
        "projects": "Projetos",
        "dataset": "Conjunto de dados",
        "training": "Configuração de Treinamento",
        "augmentation": "Augmentação",
        "status": "Status",
        "heatmap": "Mapa de calor",
        "updates": "Atualizações",
        "docs": "Documentação",
        "about": "Sobre"
      },
      "check_failed": "Update check failed: {error}",
      "apply_success": "Updates applied successfully. {updated} file(s) updated.",
      "apply_partial": "Updates applied with {updated} success and {failed} failure(s).",
      "apply_failed": "Failed to apply updates: {error}",
      "apply_nothing": "No updates were necessary."
    },
    "status": {
      "missing": "Missing locally",
      "outdated": "Checksum mismatch"
    }
  },
  "status": {
    "project_load_failed": "Carregamento do projeto falhou",
    "project_loading": "Carregando projeto {projectName}...",
    "project_loaded_custom": "Projeto {projectName} carregado com configuração personalizada",
    "project_loaded_defaults": "Projeto {projectName} carregado com padrões globais",
    "project_load_error": "Erro ao carregar projeto {projectName}: {error}",
    "no_project_loaded": "Nenhum projeto carregado",
    "validation_errors": "Corrija erros de validação antes de salvar",
    "saving_training_config": "Salvando configuração de treinamento...",
    "training_config_saved": "Configuração de treinamento salva",
    "save_failed": "Salvamento falhou",
    "loading_schema": "Carregando esquema & configuração...",
    "init_failed": "Init falhou",
    "checking_updates": "Verificando atualizações...",
    "updates_ready": "Atualizações disponíveis",
    "updates_none": "Nenhuma atualização disponível",
    "updates_check_failed": "Verificação de atualizações falhou",
    "updates_applying": "Aplicando atualizações...",
    "updates_applied": "Atualizações aplicadas com sucesso",
    "updates_apply_failed": "Aplicação de atualizações falhou",
    "generating_heatmap": "Gerando heatmap...",
    "heatmap_generated": "Heatmap gerado",
    "heatmap_generation_failed": "Geração de heatmap falhou",
    "saving_system_settings": "Salvando configurações do sistema...",
    "system_settings_saved": "Configurações do sistema salvas",
    "starting_training": "Iniciando treinamento...",
    "training_started": "Treinamento iniciado",
    "training_stopping": "Parando treinamento...",
    "training_stop_requested": "Parada de treinamento solicitada",
    "training_stop_failed": "Parada de treinamento falhou",
    "training_start_failed": "Iniciação de treinamento falhou",
    "switching_language": "Trocando idioma...",
    "language_switched": "Idioma alterado com sucesso",
    "language_switch_failed": "Troca de idioma falhou",
    "augmentation_preview_ready": "Pré-visualização de aumento gerada"
  },
  "docs_ui": {
    "page_title": "Documentação",
    "page_description": "Consulte a documentação sem sair do Hootsight.",
    "sidebar_title": "Documentação",
    "empty": "Nenhum arquivo de documentação encontrado.",
    "empty_content": "Não há arquivos de documentação disponíveis na pasta docs.",
    "loading_placeholder": "Selecione um documento para visualizar.",
    "loading_list": "Carregando lista de documentação...",
    "loading_file": "Carregando documento...",
    "list_failed": "Falha ao carregar a lista de documentação.",
    "open_externally": "Abrir arquivo bruto",
    "placeholder_title": "Documentação"
  },
  "docs": {
    "api": {
      "missing_root": "Diretório de documentação não está disponível.",
      "not_found": "Arquivo de documentação {path} não foi encontrado.",
      "decode_error": "Não foi possível decodificar o arquivo de documentação solicitado.",
      "read_failed": "Falha ao recuperar documentação: {error}"
    }
  },
  "nav": {
    "docs": "Docs"
  },
  "page": {
    "docs": "Docs"
  },
  "projects": {
    "api": {
      "create_missing": "Project name is required.",
      "create_length": "Project name must be between {min} and {max} characters.",
      "create_invalid": "Project name must start with a letter or number and may contain letters, numbers, hyphens, and underscores.",
      "create_exists": "Project {name} already exists.",
      "create_success": "Project {name} created successfully.",
      "create_error": "Failed to create project: {error}"
    }
  }
}
