{
  "language_name": "Deutsch",
  "activations": {
    "not_supported": "Nicht unterstützte Aktivierung: {activation}. Verfügbare Optionen: {available}",
    "created": "Aktivierungsfunktion erfolgreich erstellt: {activation} mit Parametern {params}",
    "creation_failed": "Erstellung der Aktivierungsfunktion fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für die Aktivierungsfunktion: {error}",
    "config_missing_type": "'type' in der Aktivierungs-Konfiguration fehlt",
    "recommendations_removed": "Empfehlungen für Aktivierungen wurden aus dem System entfernt"
  },
  "augmentation": {
    "not_supported": "Nicht unterstützte Augmentation: {aug}. Verfügbare Optionen: {available}",
    "created": "Augmentation erfolgreich erstellt: {aug} mit Parametern {params}",
    "creation_failed": "Erstellung der Augmentation fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für die Augmentation: {error}",
    "config_missing_type": "'type' in der Augmentations-Konfiguration fehlt",
    "preview_invalid_phase": "Ungültige Augmentations-Phase: {phase}",
    "preview_no_images": "Für Projekt {project} stehen keine Bilder für die Augmentations-Vorschau zur Verfügung.",
    "preview_failed": "Vorschau konnte nicht erstellt werden: {error}",
    "preview_generated": "Augmentations-Vorschau für Phase {phase} erstellt"
  },
  "coordinator_settings": {
    "user_settings_saved": "Benutzereinstellungen gespeichert: {path}",
    "user_settings_save_failed": "Speichern der Benutzereinstellungen fehlgeschlagen: {error}"
  },
  "recommendations": {
    "critical_shortage": "Kritischer Datenmangel entdeckt in: {labels}",
    "reduce_oversampled": "Überrepräsentierte Labels reduzieren: {labels}",
    "augment_undersampled": "Unterrepräsentierte Labels augmentieren: {labels}",
    "weighted_loss": "Gewichtete Loss-Funktionen in Betracht ziehen",
    "stratified_sampling": "Stratifiziertes Sampling in Betracht ziehen",
    "hierarchical_imbalance": "Hierarchische Ungleichgewichte festgestellt in: {categories}",
    "small_dataset": "Kleines Dataset erkannt – Augmentation erwägen",
    "tiny_dataset": "Sehr kleines Dataset – hohes Risiko für Overfitting"
  },
  "losses": {
    "not_supported": "Nicht unterstützte Loss-Funktion: {loss}. Verfügbare Optionen: {available}",
    "created": "Loss-Funktion erfolgreich erstellt: {loss} mit Parametern {params}",
    "creation_failed": "Erstellung der Loss-Funktion fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für die Loss-Funktion: {error}",
    "config_missing_type": "'type' in der Loss-Konfiguration fehlt",
    "recommendations_removed": "Loss-Empfehlungen wurden aus dem System entfernt"
  },
  "normalization": {
    "not_supported": "Nicht unterstützte Normalisierung: {norm}. Verfügbare Optionen: {available}",
    "missing_num_features": "'num_features' fehlt für {norm}",
    "missing_num_channels": "'num_channels' fehlt für {norm}",
    "missing_normalized_shape": "'normalized_shape' fehlt für {norm}",
    "created": "Normalisierungs-Layer erfolgreich erstellt: {norm} mit Parametern {params}",
    "creation_failed": "Erstellung des Normalisierungs-Layers fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für den Normalisierungs-Layer: {error}",
    "config_missing_type": "'type' in der Normalisierungs-Konfiguration fehlt",
    "recommendations_removed": "Normalisierungs-Empfehlungen wurden aus dem System entfernt"
  },
  "optimizers": {
    "not_supported": "Nicht unterstützter Optimizer: {optimizer}. Verfügbare Optionen: {available}",
    "created": "Optimizer erfolgreich erstellt: {optimizer} mit Parametern {params}",
    "creation_failed": "Erstellung des Optimizers fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für den Optimizer: {error}",
    "config_missing_type": "'type' in der Optimizer-Konfiguration fehlt",
    "recommendations_removed": "Optimizer-Empfehlungen wurden aus dem System entfernt"
  },
  "pooling": {
    "not_supported": "Nicht unterstützter Pooling-Layer: {pool}. Verfügbare Optionen: {available}",
    "created": "Pooling-Layer erfolgreich erstellt: {pool} mit Parametern {params}",
    "creation_failed": "Erstellung des Pooling-Layers fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für den Pooling-Layer: {error}",
    "config_missing_type": "'type' in der Pooling-Konfiguration fehlt",
    "recommendations_removed": "Pooling-Empfehlungen wurden aus dem System entfernt"
  },
  "regularization": {
    "not_supported": "Nicht unterstützte Regularisierung: {reg}. Verfügbare Optionen: {available}",
    "created": "Regularisierungs-Layer erfolgreich erstellt: {reg} mit Parametern {params}",
    "creation_failed": "Erstellung des Regularisierungs-Layers fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für den Regularisierungs-Layer: {error}",
    "config_missing_type": "'type' in der Regularisierungs-Konfiguration fehlt",
    "recommendations_removed": "Regularisierungs-Empfehlungen wurden aus dem System entfernt"
  },
  "schedulers": {
    "not_supported": "Nicht unterstützter Scheduler: {scheduler}. Verfügbare Optionen: {available}",
    "created": "Scheduler erfolgreich erstellt: {scheduler} mit Parametern {params}",
    "creation_failed": "Erstellung des Schedulers fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für den Scheduler: {error}",
    "config_missing_type": "'type' in der Scheduler-Konfiguration fehlt",
    "recommendations_removed": "Scheduler-Empfehlungen wurden aus dem System entfernt"
  },
  "weight_init": {
    "not_supported": "Nicht unterstützte Initialisierung: {init}. Verfügbare Optionen: {available}",
    "applied": "Gewichtsinitialisierung angewendet: {init} auf {module} mit Parametern {params}",
    "application_failed": "Anwendung der Gewichtsinitialisierung fehlgeschlagen: {error}",
    "invalid_params": "Ungültige Parameter für die Gewichtsinitialisierung: {error}",
    "recommendations_removed": "Initialisierungs-Empfehlungen wurden aus dem System entfernt"
  },
  "heatmap": {
    "no_images": "Keine Bilder für Projekt {project} gefunden",
    "checkpoint_missing": "Modell-Checkpoint nicht im Verzeichnis gefunden: {dir}",
    "project_or_dataset_missing": "Projekt {project} oder dessen Dataset nicht gefunden",
    "generated": "Heatmap für {project} erstellt mit Bild {image} (Klasse {clazz})"
  },
  "app": {
    "brand": "Hootsight"
  },
  "nav": {
    "training_group": "Training",
    "projects": "Projekte",
    "dataset": "Dataset",
    "training_setup": "Training einrichten",
    "augmentation": "Augmentation",
    "status_group": "Status",
    "status": "Status",
    "heatmap": "Heatmap",
    "memory": "Speicher",
    "system_group": "System",
    "updates": "Updates"
  },
  "page": {
    "projects": "Projekte",
    "dataset": "Dataset",
    "training": "Training einrichten",
    "augmentation": "Augmentation",
    "status": "Status",
    "heatmap": "Heatmap",
    "memory": "Speicher",
    "updates": "Updates"
  },
  "config": {
    "sections": {
      "training": "Training",
      "optimizers": "Optimizers",
      "schedulers": "Schedulers",
      "losses": "Losses",
      "models": "Modelle"
    },
    "entities": {
      "optimizers": {
        "sgd": "SGD (Stochastic Gradient Descent)",
        "adam": "Adam Optimizer",
        "adamw": "AdamW Optimizer",
        "adamax": "AdaMax Optimizer",
        "nadam": "Nesterov Adam",
        "radam": "Rectified Adam",
        "rmsprop": "RMSprop Optimizer",
        "rprop": "Resilient Backpropagation",
        "adagrad": "AdaGrad Optimizer",
        "adadelta": "AdaDelta Optimizer",
        "sparse_adam": "Sparse Adam",
        "lbfgs": "L-BFGS Optimizer",
        "asgd": "Averaged SGD"
      },
      "schedulers": {
        "step_lr": "Step Learning Rate",
        "multi_step_lr": "Multi-Step Learning Rate",
        "exponential_lr": "Exponential Learning Rate",
        "cosine_annealing_lr": "Cosine Annealing",
        "cosine_annealing_warm_restarts": "Cosine Annealing with Warm Restarts",
        "reduce_lr_on_plateau": "Reduce LR on Plateau",
        "cyclic_lr": "Cyclic Learning Rate",
        "one_cycle_lr": "One Cycle Learning Rate",
        "polynomial_lr": "Polynomial Learning Rate",
        "linear_lr": "Linear Learning Rate",
        "lambda_lr": "Lambda Learning Rate",
        "multiplicative_lr": "Multiplicative Learning Rate"
      },
      "losses": {
        "cross_entropy": "Cross-Entropy Loss",
        "nll_loss": "Negative Log-Likelihood",
        "bce_loss": "Binary Cross-Entropy",
        "bce_with_logits": "BCE with Logits",
        "multi_margin": "Multi-Class Margin Loss",
        "multi_label_margin": "Multi-Label Margin Loss",
        "multi_label_soft_margin": "Multi-Label Soft Margin",
        "mse_loss": "Mean Squared Error",
        "l1_loss": "L1 Loss (MAE)",
        "smooth_l1": "Smooth L1 Loss",
        "huber_loss": "Huber Loss",
        "kl_div": "KL Divergence",
        "margin_ranking": "Margin Ranking Loss",
        "hinge_embedding": "Hinge Embedding Loss",
        "triplet_margin": "Triplet Margin Loss",
        "cosine_embedding": "Cosine Embedding Loss",
        "ctc_loss": "CTC Loss",
        "poisson_nll": "Poisson NLL Loss",
        "gaussian_nll": "Gaussian NLL Loss"
      }
    }
  },
  "groups": {
    "model_settings": "Model Settings",
    "task_configuration": "Task Configuration",
    "training_parameters": "Training Parameters",
    "optimizer_settings": "Optimizer Settings",
    "scheduler_settings": "Scheduler Settings",
    "loss_configuration": "Loss Configuration",
    "data_loading": "Data Loading",
    "normalization": "Normalization",
    "checkpointing": "Checkpointing",
    "weight_initialization": "Weight Initialization"
  },
  "actions": {
    "save_config": "Konfiguration speichern",
    "export": "Exportieren",
    "save_training_config": "Training-Konfiguration speichern",
    "save_system_settings": "Globale Einstellungen speichern"
  },
  "footer": {
    "tagline": "Konfigurationsgetrieben",
    "generated": "",
    "ready": "Bereit"
  },
  "field": {
    "training_model_type": "Modelltyp",
    "training_model_name": "Modellname",
    "training_pretrained": "Vortrainiert",
    "training_task": "Aufgabe",
    "training_batch_size": "Batch-Größe",
    "training_epochs": "Epochen",
    "training_learning_rate": "Lernrate",
    "training_weight_decay": "Weight Decay",
    "training_input_size": "Eingabegröße",
    "training_val_ratio": "Validierungsanteil",
    "training_optimizer_type": "Optimizer-Typ",
    "training_scheduler_type": "Scheduler-Typ",
    "training_loss_type": "Loss-Typ",
    "training_dataloader": "DataLoader",
    "training_dataloader_num_workers": "Num Worker",
    "training_dataloader_pin_memory": "Pin Memory",
    "training_dataloader_persistent_workers": "Persistent Workers",
    "training_dataloader_prefetch_factor": "Prefetch-Faktor",
    "training_normalize": "Normalisieren",
    "training_normalize_mean": "Mittelwert",
    "training_normalize_std": "Std",
    "training_checkpoint": "Checkpoint",
    "training_checkpoint_save_best_only": "Nur Bestes speichern",
    "training_checkpoint_save_frequency": "Speicherhäufigkeit",
    "training_checkpoint_max_checkpoints": "Max Checkpoints",
    "training_checkpoint_checkpoint_dir": "Checkpoint-Verzeichnis",
    "training_checkpoint_best_model_filename": "Dateiname Bestes Modell",
    "training_checkpoint_training_history_filename": "Dateiname Trainingshistorie",
    "training_weight_init": "Gewichtsinitialisierung",
    "training_weight_init_type": "Init-Typ",
    "training_weight_init_params": "Init-Parameter",
    "training_optimizer_params_adamw_lr": "Lernrate",
    "training_optimizer_params_adamw_betas": "Betas",
    "training_optimizer_params_adamw_eps": "Eps",
    "training_optimizer_params_adamw_weight_decay": "Weight Decay",
    "training_optimizer_params_adamw_amsgrad": "Amsgrad",
    "training_scheduler_params_step_lr_step_size": "Step-Größe",
    "training_scheduler_params_step_lr_gamma": "Gamma",
    "training_scheduler_params_step_lr_last_epoch": "Letzte Epoche",
    "training_loss_params_bce_with_logits_weight": "Gewicht",
    "training_loss_params_bce_with_logits_size_average": "Größenmittel",
    "training_loss_params_bce_with_logits_reduce": "Reduzieren",
    "training_loss_params_bce_with_logits_reduction": "Reduktion",
    "training_loss_params_bce_with_logits_pos_weight": "Positives Gewicht"
  },
  "ui": {
    "generate_heatmap": "Heatmap generieren",
    "no_heatmap_generated": "Noch keine Heatmap erzeugt.",
    "no_data_available": "Keine Daten verfügbar.",
    "page_not_implemented": "Seite nicht implementiert",
    "error": "Fehler",
    "schema_not_loaded": "Schema noch nicht geladen. Bitte warte...",
    "config_not_loaded": "Konfiguration noch nicht geladen. Bitte warte...",
    "augmentation_phase": "Augmentation {phase}",
    "add": "Hinzufügen",
    "remove": "Entfernen",
    "transform": "Transform",
    "no_project_loaded": "Kein Projekt geladen",
    "load_project_first": "Lade zuerst ein Projekt aus dem Reiter Projekte.",
    "go_to_projects": "Zu Projekte",
    "dataset_overview": "Dataset-Übersicht",
    "balance_analysis": "Balance-Analyse",
    "label_distribution": "Label-Verteilung (Top 20)",
    "recommendations": "Empfehlungen",
    "failed_to_load_dataset": "Dataset-Informationen konnten nicht geladen werden.",
    "current_project": "AKTUELLES PROJEKT",
    "load": "Laden",
    "start_training": "Training starten",
    "stop_training": "Training stoppen",
    "stop_training_disabled": "Kein aktives Training für dieses Projekt.",
    "training_in_progress": "Training läuft",
    "memory": "Speicher",
    "loading": "Lädt...",
    "training_status": "Training-Status",
    "idle": "Leerlauf",
    "prediction": "Vorhersage",
    "predictions": "Vorhersagen",
    "no_predictions_above_threshold": "Keine Vorhersagen über dem Schwellenwert",
    "image": "Bild",
    "checkpoint": "Checkpoint",
    "auto": "auto",
    "value": "Wert",
    "one_number_per_line": "Eine Zahl pro Zeile",
    "empty_object": "Leeres Objekt",
    "language_warning": "Sprachwechsel lädt das System neu",
    "language_select_title": "Sprache wählen",
    "not_available": "N/V",
    "unknown": "Unbekannt",
    "configuration_empty": "Keine Konfigurationssektionen verfügbar",
    "configuration_schema_missing": "Konfigurationsschema ist noch nicht geladen."
  },
  "augmentation_ui": {
    "page_title": "Datenaugmentation",
    "page_description": "Konfiguriere Bildtransformationen, um die Generalisierung und Robustheit des Modells zu verbessern.",
    "train_title": "Training-Augmentations",
    "train_description": "Wird beim Training angewendet, um die visuelle Vielfalt zu erhöhen, ohne Labels zu verändern.",
    "val_title": "Validierungs-Augmentations",
    "val_description": "Wird bei der Validierung angewendet, um eine deterministische Bewertung zu gewährleisten.",
    "toggle_help": "Schalte eine Augmentation ein oder aus für diese Phase.",
    "no_options": "Keine Augmentationsoptionen verfügbar.",
    "custom_warning": "Folgende Transforms werden beibehalten, sind hier aber nicht editierbar:",
    "unknown_transform": "Unbekannte Transformation",
    "random_resized_crop": "Random Resized Crop",
    "random_resized_crop_description": "Schneidet zufällig aus und skaliert auf die Zielgröße, unter Beachtung von Skalen- und Seitenverhältnissen.",
    "random_horizontal_flip": "Random Horizontal Flip",
    "random_horizontal_flip_description": "Spiegelt das Bild horizontal mit der konfigurierten Wahrscheinlichkeit.",
    "random_vertical_flip": "Random Vertical Flip",
    "random_vertical_flip_description": "Spiegelt das Bild vertikal mit der konfigurierten Wahrscheinlichkeit.",
    "random_rotation": "Random Rotation",
    "random_rotation_description": "Wendet eine zufällige Rotation innerhalb des definierten Gradbereichs an.",
    "color_jitter": "Color Jitter",
    "color_jitter_description": "Verändert zufällig Helligkeit, Kontrast, Sättigung und Farbton.",
    "random_grayscale": "Random Grayscale",
    "random_grayscale_description": "Konvertiert Bilder mit der konfigurierten Wahrscheinlichkeit in Graustufen.",
    "random_erasing": "Random Erasing",
    "random_erasing_description": "Maskiert zufällig rechteckige Regionen, um räumliche Robustheit zu fördern.",
    "random_perspective": "Random Perspective",
    "random_perspective_description": "Wendet eine zufällige Perspektivverzerrung mit konfigurierter Stärke und Wahrscheinlichkeit an.",
    "center_crop": "Center Crop",
    "center_crop_description": "Schneidet den zentralen Bereich auf die Zielgröße zu, für konsistente Validierungseingaben.",
    "random_resized_crop.size_label": "Ausgabegröße",
    "random_resized_crop.size_description": "Endkante in Pixeln nach dem Skalieren.",
    "random_resized_crop.scale_min_label": "Skala Minimum",
    "random_resized_crop.scale_min_description": "Untere Grenze für die relative Flächenskala (0-1).",
    "random_resized_crop.scale_max_label": "Skala Maximum",
    "random_resized_crop.scale_max_description": "Obere Grenze für die relative Flächenskala.",
    "random_resized_crop.ratio_min_label": "Seitenverhältnis Minimum",
    "random_resized_crop.ratio_min_description": "Untere Grenze für das Seitenverhältnis vor dem Skalieren.",
    "random_resized_crop.ratio_max_label": "Seitenverhältnis Maximum",
    "random_resized_crop.ratio_max_description": "Obere Grenze für das Seitenverhältnis vor dem Skalieren.",
    "random_horizontal_flip.p_label": "Flip-Wahrscheinlichkeit",
    "random_horizontal_flip.p_description": "Wahrscheinlichkeit für horizontales Spiegeln.",
    "random_vertical_flip.p_label": "Flip-Wahrscheinlichkeit",
    "random_vertical_flip.p_description": "Wahrscheinlichkeit für vertikales Spiegeln.",
    "random_rotation.min_label": "Min Grad",
    "random_rotation.min_description": "Untere Rotationsgrenze in Grad (negative Werte drehen im Uhrzeigersinn).",
    "random_rotation.max_label": "Max Grad",
    "random_rotation.max_description": "Obere Rotationsgrenze in Grad (positive Werte drehen gegen den Uhrzeigersinn).",
    "color_jitter.brightness_label": "Helligkeits-Jitter",
    "color_jitter.brightness_description": "Maximale Helligkeitsabweichung pro Kanal.",
    "color_jitter.contrast_label": "Kontrast-Jitter",
    "color_jitter.contrast_description": "Maximale Kontrastskalierung.",
    "color_jitter.saturation_label": "Sättigungs-Jitter",
    "color_jitter.saturation_description": "Maximale Sättigungsänderung im HSV-Raum.",
    "color_jitter.hue_label": "Farbton-Jitter",
    "color_jitter.hue_description": "Maximale Farbtonverschiebung (0-0.5).",
    "random_grayscale.p_label": "Graustufen-Wahrscheinlichkeit",
    "random_grayscale.p_description": "Wahrscheinlichkeit, dass ein Bild grau wird.",
    "random_erasing.p_label": "Erasing-Wahrscheinlichkeit",
    "random_erasing.p_description": "Wahrscheinlichkeit, dass pro Bild eine Region gelöscht wird.",
    "random_erasing.scale_min_label": "Skala Minimum",
    "random_erasing.scale_min_description": "Untere Grenze für die gelöschte Flächenskala relativ zum Bild.",
    "random_erasing.scale_max_label": "Skala Maximum",
    "random_erasing.scale_max_description": "Obere Grenze für die gelöschte Flächenskala relativ zum Bild.",
    "random_erasing.ratio_min_label": "Seitenverhältnis Minimum",
    "random_erasing.ratio_min_description": "Untere Grenze für das Seitenverhältnis der gelöschten Fläche.",
    "random_erasing.ratio_max_label": "Seitenverhältnis Maximum",
    "random_erasing.ratio_max_description": "Obere Grenze für das Seitenverhältnis der gelöschten Fläche.",
    "random_erasing.value_label": "Füllwert",
    "random_erasing.value_description": "Pixelwert zum Füllen der gelöschten Region (0-1).",
    "random_erasing.inplace_label": "In-place",
    "random_erasing.inplace_description": "Lösche direkt im Eingabetensor ohne Kopie.",
    "random_perspective.distortion_scale_label": "Verzerrungsstärke",
    "random_perspective.distortion_scale_description": "Steuert die Stärke der Perspektivverzerrung (0-1).",
    "random_perspective.p_label": "Perspektive-Wahrscheinlichkeit",
    "random_perspective.p_description": "Wahrscheinlichkeit für eine Perspektivverzerrung.",
    "center_crop.size_label": "Crop-Größe",
    "center_crop.size_description": "Zielkante in Pixeln für den zentralen Crop.",
    "preview_section_title": "Vorschau",
    "preview_description": "Wende die aktuelle Pipeline auf ein zufälliges Dataset-Bild an.",
    "preview_button": "Vorschau prüfen",
    "preview_idle": "Klicke auf 'Vorschau prüfen', um das augmentierte Bild zu sehen.",
    "preview_loading": "Erzeuge Vorschau...",
    "preview_no_project": "Lade ein Projekt, um Augmentations-Vorschauen zu sehen.",
    "preview_empty_pipeline": "Konfiguriere mindestens eine Transformation für eine Vorschau.",
    "preview_generic_error": "Vorschau konnte nicht erstellt werden.",
    "preview_original_label": "Original",
    "preview_augmented_label": "Augmentiert",
    "preview_image_path_label": "Bildpfad"
  },
  "training_ui": {
    "page_title": "Training einrichten",
    "page_description": "Konfiguriere Modellarchitektur, Trainingsparameter und Optimierungseinstellungen.",
    "optimizer_params_title": "Optimizer-Parameter",
    "scheduler_params_title": "Scheduler-Parameter",
    "loss_params_title": "Loss-Parameter",
    "select_type_first": "Wähle zuerst einen Typ, um Parameter zu sehen.",
    "no_extra_params": "Keine zusätzlichen Parameter für diese Auswahl."
  },
  "dataset_ui": {
    "page_title": "Dataset",
    "page_description": "Erkunde und analysiere Struktur, Labels und Datenverteilung deines Datasets.",
    "summary": {
      "project": "Projekt",
      "dataset_type": "Dataset-Typ",
      "total_images": "Bilder insgesamt",
      "total_labels": "Labels insgesamt",
      "balance_status": "Balance-Status",
      "balance_score": "Balance-Score",
      "images_per_label_ideal": "Bilder pro Label (Ideal)",
      "min_images": "Min Bilder",
      "max_images": "Max Bilder",
      "max_min_ratio": "Max/Min Verhältnis"
    },
    "table": {
      "label": "Label",
      "count": "Anzahl",
      "percentage": "Prozent"
    }
  },
  "projects_ui": {
    "page_title": "Projekte",
    "page_description": "Verwalte und wechsle zwischen verschiedenen ML-Projekten und Datasets.",
    "card": {
      "images": "Bilder",
      "labels": "Labels",
      "balance_score": "Balance-Score",
      "balance_status": "Balance",
      "dataset_type": "Dataset-Typ",
      "status": {
        "balanced": "Ausgeglichen",
        "imbalanced": "Unausgeglichen",
        "critical": "Kritisch",
        "warning": "Warnung",
        "good": "Gut",
        "poor": "Schlecht",
        "excellent": "Ausgezeichnet",
        "fair": "Akzeptabel",
        "ok": "OK",
        "unstable": "Instabil"
      }
    }
  },
  "status_ui": {
    "page_title": "Status",
    "page_description": "Überwache Trainingsfortschritt, Systemzustand und Performance-Metriken in Echtzeit."
  },
  "heatmap_ui": {
    "page_title": "Heatmap",
    "page_description": "Erstelle und visualisiere Modell-Attention-Maps, um Vorhersage-Fokus zu verstehen."
  },
  "memory_ui": {
    "page_title": "Speicher",
    "page_description": "Überwache Speicherverbrauch und optimiere Batch-Größen für effizientes Training."
  },
  "environment": {
    "venv_creating": "Erzeuge Python-virtuelle Umgebung unter {path}...",
    "venv_created": "Virtuelle Umgebung bereit.",
    "venv_create_failed": "Erstellen der virtuellen Umgebung fehlgeschlagen: {error}",
    "venv_exists": "Virtuelle Umgebung bereits vorhanden.",
    "pip_upgrading": "Aktualisiere pip in der virtuellen Umgebung...",
    "pip_upgraded": "Pip-Update abgeschlossen.",
    "pip_upgrade_failed": "Pip-Aktualisierung fehlgeschlagen: {error}",
    "cuda_debug_nvcc": "nvcc --version Ausgabe:\n{output}",
    "cuda_debug_nvcc_error": "nvcc-Abfrage fehlgeschlagen: {error}",
    "cuda_debug_nvidia_smi": "nvidia-smi Ausgabe:\n{output}",
    "cuda_debug_nvidia_smi_error": "nvidia-smi-Abfrage fehlgeschlagen: {error}",
    "cuda_debug_detected": "Erkannte CUDA-Version: {version}",
    "pytorch_install": "Installiere PyTorch für CUDA {cuda} auf {platform}...",
    "pytorch_installed": "PyTorch-Installation abgeschlossen.",
    "pytorch_install_failed": "PyTorch-Installation fehlgeschlagen: {error}",
    "xformers_already_installed": "xFormers ist bereits installiert und aktuell.",
    "xformers_installing": "Installiere xFormers (CUDA {cuda_version})...",
    "xformers_installed": "xFormers-Installation abgeschlossen.",
    "xformers_install_failed": "xFormers-Installation fehlgeschlagen: {error}",
    "pytorch_skip": "Überspringe PyTorch-Installation (erkanntes CUDA={cuda}, Plattform={platform}).",
    "config_loading": "Lade Umgebungs-Konfiguration...",
    "config_loaded": "Umgebungs-Konfiguration geladen.",
    "using_compatible_xformers": "Verwende CUDA-Index {cuda_version} für xFormers.",
    "env_packages_all_installed": "Umgebungs-Pakete sind bereits installiert.",
    "env_packages_progress_desc": "Installiere Umgebungs-Pakete",
    "env_package_installing": "Installiere Paket {package}...",
    "env_package_installed": "Paket installiert: {package}",
    "env_package_install_failed": "Paketinstallation fehlgeschlagen {package}: {error}",
    "env_vars_configured": "{count} Umgebungsvariable(n) für den Training-Prozess vorbereitet.",
    "env_vars_config_failed": "Konfigurieren der Umgebungsvariablen fehlgeschlagen: {error}",
    "entry_not_found": "Entry-Skript nicht gefunden unter {path}.",
    "venv_python_not_found": "Python-Ausführbar der virtuellen Umgebung fehlt: {path}.",
    "venv_python_test_failed": "Python der virtuellen Umgebung hat --version Test nicht bestanden: {error}",
    "re_exec_starting": "Starte Trainings-Entry via {venv_python} -> {entry_py} (root {root}).",
    "re_exec_timeout": "Neustart-Zeitüberschreitung.",
    "re_exec_failed": "Starten des Trainingsprozesses fehlgeschlagen: {error}",
    "re_exec_unexpected_error": "Unerwarteter Fehler beim Starten des Trainings: {error}"
  },
  "updates_ui": {
    "page_title": "System-Updates",
    "page_description": "Halte deine Installation mit dem Referenz-Repository synchron, ohne projekt-spezifische Konfigs zu überschreiben.",
    "card_title": "Update-Manager",
    "intro": "Vergleiche lokale Dateien mit dem Referenz-Repo und synchronisiere fehlende Fixes, ohne config.json zu ändern.",
    "check_button": "Auf Updates prüfen",
    "apply_button": "Updates anwenden",
    "apply_disabled_hint": "Führe zuerst eine Prüfung aus, um Updates zu aktivieren.",
    "status_idle": "Es wurden noch keine Update-Prüfungen durchgeführt.",
    "status_checking": "Prüfe auf Updates...",
    "status_ready": "Update-Zusammenfassung vorbereitet.",
    "status_up_to_date": "Alles ist bereits auf dem neuesten Stand.",
    "status_failed": "Update-Prüfung fehlgeschlagen.",
    "status_applying": "Wende Dateien an...",
    "status_applied": "Updates erfolgreich angewendet.",
    "status_apply_failed": "Einige Updates konnten nicht angewendet werden.",
    "table_header_file": "Datei",
    "table_header_status": "Status",
    "table_header_local": "Lokal",
    "table_header_remote": "Remote",
    "table_row_missing": "Lokal fehlt",
    "table_row_outdated": "Checksum mismatch",
    "table_footnote": "Hashes gekürzt zur Lesbarkeit.",
    "no_updates": "Alle verfolgten Dateien sind aktuell.",
    "hash_missing": "—",
    "orphaned_title": "Ungetrackte lokale Dateien",
    "orphaned_none": "Keine zusätzlichen lokalen Dateien gefunden."
  },
  "schema": {
    "description": "JSON-Schema für Hootsight config.json - definiert Typen, Bereiche und Struktur aller konfigurierbaren Einstellungen",
    "general_description": "Allgemeine Anwendungseinstellungen",
    "general_language_description": "UI-Sprachcode",
    "api_description": "API-Server-Konfiguration",
    "api_host_description": "API-Server-Host",
    "api_port_description": "API-Server-Port",
    "ui_description": "Benutzeroberflächen-Einstellungen",
    "ui_title_description": "Fenstertitel der Anwendung",
    "ui_width_description": "Fensterbreite in Pixel",
    "ui_height_description": "Fensterhöhe in Pixel",
    "ui_resizable_description": "Ob das Fenster skalierbar ist",
    "system_description": "Systemweite Einstellungen",
    "system_max_threads_description": "Maximale Anzahl Threads",
    "system_fallback_batch_size_description": "Fallback-Batchgröße wenn Auto-Berechnung fehlschlägt",
    "system_memory_cleanup_interval_description": "Intervall für Speicherbereinigung in Sekunden",
    "system_thread_pool_timeout_description": "Timeout für Thread-Pool in Sekunden",
    "system_startup_wait_seconds_description": "Start-Wartezeit in Sekunden",
    "memory_description": "Speicherverwaltungs-Einstellungen",
    "memory_target_memory_usage_description": "Ziel-Speichernutzungs-Anteil (0.0-1.0)",
    "memory_safety_margin_description": "Sicherheitsmarge für Speicherberechnung (0.0-1.0)",
    "memory_augmentation_threads_description": "Anzahl Threads für Datenaugmentation",
    "training_description": "Training-Konfiguration",
    "training_model_type_description": "Modelltyp",
    "training_model_name_description": "Konkreter Modellname",
    "training_pretrained_description": "Initialisiere Netzwerk mit ImageNet-Vortrainierten Gewichten wenn verfügbar; deaktivieren, um von null zu trainieren",
    "training_task_description": "Art der ML-Aufgabe",
    "training_batch_size_description": "Batch-Größe fürs Training",
    "training_epochs_description": "Anzahl der Trainingsepochen",
    "training_learning_rate_description": "Lernrate",
    "training_weight_decay_description": "Weight Decay (L2-Regularisierung)",
    "training_input_size_description": "Kantenlänge in Pixel für das quadratische Eingabetensor (konsistent mit Augmentation)",
    "training_normalize_description": "Bildnormalisierungs-Parameter",
    "training_normalize_mean_description": "Mittelwerte für RGB-Kanäle",
    "training_normalize_std_description": "Standardabweichungen für RGB-Kanäle",
    "training_val_ratio_description": "Validation-Split-Anteil (0.0-1.0)",
    "training_dataloader_description": "DataLoader-Konfiguration",
    "training_dataloader_num_workers_description": "Anzahl Worker-Prozesse",
    "training_dataloader_pin_memory_description": "Pin Memory für schnelleren GPU-Transfer",
    "training_dataloader_persistent_workers_description": "Worker zwischen Epochen persistent halten",
    "training_dataloader_prefetch_factor_description": "Anzahl Batches zum Prefetch pro Worker",
    "training_augmentation_description": "Datenaugmentation-Konfiguration",
    "training_augmentation_train_description": "Trainings-Augmentations",
    "training_augmentation_val_description": "Validierungs-Augmentations",
    "training_optimizer_type_description": "Optimizer-Typ",
    "training_optimizer_params_description": "Optimizer-Parameter",
    "training_scheduler_type_description": "Scheduler-Typ",
    "training_scheduler_params_description": "Scheduler-Parameter",
    "training_loss_type_description": "Loss-Typ",
    "training_loss_params_description": "Loss-Parameter",
    "training_weight_init_description": "Gewichtsinitialisierungs-Konfiguration",
    "training_checkpoint_description": "Checkpoint-Konfiguration",
    "training_early_stopping_description": "Early-Stopping-Konfiguration",
    "training_gradient_description": "Gradient-Konfiguration",
    "training_runtime_description": "Runtime-Optimierungseinstellungen",
    "training_runtime_mixed_precision_description": "Automatisches Mixed-Precision-Training aktivieren",
    "training_runtime_channels_last_description": "Channels-last Speicherformat für bessere GPU-Auslastung",
    "training_runtime_allow_tf32_description": "TF32 für schnellere Matrix-Operationen auf Ampere+ GPUs erlauben",
    "training_runtime_cudnn_benchmark_description": "cuDNN Benchmark aktivieren für optimierte Convolution-Algorithmen",
    "dataset_description": "Dataset-Konfiguration",
    "dataset_image_extensions_description": "Unterstützte Bilddateiendungen",
    "optimizers_description": "Standard-Overrides für Optimizer",
    "optimizers_defaults_description": "Standard-Parameter für Optimizer",
    "schedulers_description": "Standard-Overrides für Scheduler",
    "schedulers_defaults_description": "Standard-Parameter für Scheduler",
    "schedulers_defaults_lambda_lr_lr_lambda_description": "Lambda-Funktion als String, z.B. 'lambda epoch: 0.95 ** epoch'",
    "schedulers_defaults_multiplicative_lr_lr_lambda_description": "Lambda-Funktion als String, z.B. 'lambda epoch: 0.95'",
    "losses_description": "Standard-Overrides für Losses",
    "losses_defaults_description": "Standard-Parameter für Loss-Funktionen",
    "models_description": "Modellkonfigurationen",
    "models_resnet_description": "ResNet-Familie Einstellungen",
    "models_resnet_variants_description": "ResNet-Varianten",
    "models_resnext_description": "ResNeXt-Familie Einstellungen",
    "models_resnext_variants_description": "ResNeXt-Varianten",
    "models_mobilenet_description": "MobileNet-Familie Einstellungen",
    "models_mobilenet_variants_description": "MobileNet-Varianten",
    "models_shufflenet_description": "ShuffleNet-Familie Einstellungen",
    "models_shufflenet_variants_description": "ShuffleNet-Varianten",
    "models_squeezenet_description": "SqueezeNet-Familie Einstellungen",
    "models_squeezenet_variants_description": "SqueezeNet-Varianten",
    "models_efficientnet_description": "EfficientNet-Familie Einstellungen",
    "models_efficientnet_variants_description": "EfficientNet-Varianten",
    "models_supported_types_description": "Unterstützte Modelltypen",
    "general_language_enum_descriptor": {
      "en": "Englische Sprache - Steuert die komplette UI-Lokalisierung inklusive Menülabels, Fehlermeldungen, Tooltips und Validierungstexte. Beeinflusst alle Textdarstellungen im Web-Interface und API-Antworten. Bestimmt das Laden der Sprachpakete beim Anwendungsstart. Aktuell die einzige unterstützte Sprachoption und daher Standard für alle Installationen."
    },
    "system_max_threads_enum_descriptor": {
      "auto": "Automatische Thread-Anzahl - Berechnet dynamisch die optimale Thread-Pool-Größe basierend auf verfügbaren CPU-Kernen (typisch Kerne - 1). Beeinflusst paralleles Datenladen, Bildvorverarbeitung, Modell-Inferenz-Batching und Hintergrundaufgaben. Steuert Thread-Zuteilung für PyTorch DataLoader-Worker, Bildaugmentations-Pipelines und gleichzeitige HTTP-Request-Behandlung. Skaliert automatisch mit Hardware-Fähigkeiten und passt sich an verfügbaren Systemspeicher an."
    },
    "memory_augmentation_threads_enum_descriptor": {
      "auto": "Automatische Augmentations-Thread-Anzahl - Berechnet optimale Thread-Anzahl für parallele Bildaugmentation basierend auf CPU-Kernen und verfügbarem RAM. Beeinflusst Pipeline-Durchsatz für Bildtransformationen inklusive Rotation, Skalierung, Farbveränderung und Normalisierung. Steuert Speicherzuteilung für Augmentations-Puffer und Zwischenspeicherung. Balanciert CPU-Auslastung gegen Speicherdruck um Systemüberlastung während intensiver Vorverarbeitung zu vermeiden."
    },
    "training_model_type_enum_descriptor": {
      "resnet": "ResNet (Residual Network) - Tiefes CNN mit Skip-Verbindungen für sehr tiefe Netzwerke (18-152 Schichten). Beeinflusst Gradientenfluss, Trainings-Stabilität, Feature-Repräsentations-Tiefe und Modellkapazität. Verwendet Residualblöcke mit Batch-Normalisierung und ReLU-Aktivierung. Steuert architektonische Komplexität von 11M Parametern (ResNet-18) bis 60M Parametern (ResNet-152). Beeinflusst Speicherverbrauch, Trainingszeit, Inferenz-Geschwindigkeit und finale Modellgenauigkeit bei Bildklassifikation.",
      "resnext": "ResNeXt (Aggregated Residual Transformations) - ResNet-Evolution mit Kardinalität (gruppierte Convolutions) für höhere Modellkapazität ohne signifikanten Parameter-Anstieg. Beeinflusst Feature-Lern-Vielfalt, Modell-Expressivität und Recheneffizienz. Steuert parallele Transformationspfade in jedem Residualblock. Beeinflusst GPU-Speicherverbrauch, Trainingszeit und erreicht höhere Genauigkeit als Standard ResNet bei ähnlichen Rechenkosten.",
      "mobilenet": "MobileNet - Leichtgewichts-CNN mit depthwise separablen Convolutions zur Reduktion von Modellgröße und Rechenanforderungen. Beeinflusst Inferenz-Latenz, Energieverbrauch, Modell-Speichergröße und Deployment-Machbarkeit auf mobilen Geräten. Steuert Trade-off zwischen Genauigkeit und Effizienz durch Width-Multiplier und Auflösungsparameter. Beeinflusst Batterielaufzeit in mobilen Apps, Echtzeit-Verarbeitungsfähigkeit und Edge-Device-Kompatibilität.",
      "shufflenet": "ShuffleNet - Extrem effizientes CNN mit Channel-Shuffle-Operationen und pointwise Gruppen-Convolutions. Beeinflusst Speicher-Bandbreiten-Auslastung, Rechenkosten pro Inferenz, Modellgröße und Verarbeitungsgeschwindigkeit. Steuert Channel-Kommunikation zwischen Gruppen-Convolutions zur Aufrechterhaltung des Informationsflusses. Optimiert für ARM-Prozessoren und stromsparende Geräte. Beeinflusst Echtzeit-Performance-Anforderungen und ressourcenbeschränkte Deployment-Szenarien.",
      "squeezenet": "SqueezeNet - Ultra-kompaktes CNN mit Fire-Modulen (squeeze + expand Schichten) für AlexNet-Level-Genauigkeit mit 50x weniger Parametern. Beeinflusst Modell-Speicheranforderungen, Download-Zeit, Cache-Effizienz und Deployment-Bandbreite. Steuert Parameter-Anzahl durch aggressive Dimensionsreduzierung gefolgt von Expansion. Minimiert Festplatten-Footprint bei akzeptabler Genauigkeit für grundlegende Klassifikationsaufgaben.",
      "efficientnet": "EfficientNet - Compound-Scaling-CNN das Netzwerk-Tiefe, -Breite und -Auflösung einheitlich skaliert mittels Neural Architecture Search. Beeinflusst Recheneffizienz, Genauigkeitsskalierung, Trainings-Ressourcenanforderungen und Inferenz-Optimierung. Steuert Modellkomplexität durch Compound-Koeffizienten der alle drei Dimensionen simultan balanciert. Bietet überlegene Genauigkeits-Effizienz-Trade-offs verglichen mit traditionellen Skalierungsmethoden."
    },
    "training_task_enum_descriptor": {
      "classification": "Einzel-Label-Klassifikation - Weist genau ein sich gegenseitig ausschließendes Klassenlabel pro Eingabebild zu. Beeinflusst finale Schicht-Architektur (Softmax-Aktivierung), Loss-Funktion-Auswahl (kategorische Kreuz-Entropie), Ausgabe-Dimensionalität (Anzahl Klassen) und Vorhersage-Konfidenz-Interpretation. Steuert Modell-Entscheidungsgrenzen, Klassen-Wahrscheinlichkeits-Verteilung und Trainings-Konvergenz-Muster. Erfordert ausgewogene Dataset-Verteilung und klare Klassen-Trennbarkeit für optimale Performance.",
      "multi_label": "Multi-Label-Klassifikation - Weist null, eins oder mehrere nicht-ausschließende Klassenlabels simultan pro Eingabebild zu. Beeinflusst Ausgabe-Schicht-Aktivierung (Sigmoid pro Klasse), Loss-Funktion-Komposition (binäre Kreuz-Entropie pro Label), Schwellenwert-Auswahl für positive Vorhersagen und Evaluations-Metriken (F1-Score, mAP). Steuert unabhängige Klassen-Vorhersage-Pfade, Label-Korrelations-Behandlung und unausgewogene Klassen-Gewichtungs-Strategien. Behandelt komplexe Real-World-Szenarien wo Bilder mehrere semantische Konzepte enthalten.",
      "detection": "Objekterkennung - Lokalisiert und klassifiziert simultan mehrere Objektinstanzen in Bildern mittels Bounding-Box-Vorhersagen. Beeinflusst Modell-Architektur-Komplexität (Feature-Pyramiden-Netzwerke, Anchor-Generierung), Loss-Funktion-Komposition (Klassifikation + Bounding-Box-Regression), Trainings-Daten-Anforderungen (annotierte Bounding-Boxes), Post-Processing-Pipeline (Non-Maximum-Suppression) und Rechenaufwand. Steuert räumliche Feature-Extraktion-Tiefe, Multi-Skala-Objekterkennung, Region-Proposal-Mechanismen und Intersection-over-Union-Berechnungen.",
      "segmentation": "Semantische Segmentierung - Führt dichte Pixel-weise Klassifikation durch um semantische Klassenlabels jedem Pixel im Eingabebild zuzuweisen. Beeinflusst Speicheranforderungen (Vollauflösung-Feature-Maps), Modell-Architektur (Encoder-Decoder mit Skip-Verbindungen), Loss-Funktion-Design (pixelweise Kreuz-Entropie, Focal Loss für Klassen-Ungleichgewicht), Trainings-Komplexität (Klassen-Ungleichgewicht auf Pixel-Level) und Ausgabe-Auflösungs-Beschränkungen. Steuert Upsampling-Strategien, Rand-Verfeinerung-Qualität, räumliche Präzision und kontextuelle Reasoning-Fähigkeiten."
    },
    "training_epochs_enum_descriptor": {
      "auto": "Automatische Epochen-Bestimmung - Überwacht Validierungsverlust und Genauigkeitstrends zur Bestimmung optimaler Trainingsdauer mittels Early-Stopping-Kriterien. Beeinflusst gesamte Trainingszeit, Modell-Konvergenz-Qualität, Rechenressourcen-Verbrauch und Overfitting-Vermeidung. Verfolgt Validierungs-Metrik-Verbesserungen über Gedulds-Perioden und stoppt automatisch Training wenn kein signifikanter Fortschritt erkannt wird. Balanciert Trainings-Gründlichkeit gegen Recheneffizienz."
    },
    "training_optimizer_type_enum_descriptor": {
      "sgd": "Stochastic Gradient Descent - Deterministischer Gradientenschritt mit optionalem Momentum und Nesterov-Lookahead. Exponiert learning_rate, momentum, dampening und weight_decay als kritische Regler. Funktioniert am besten wenn du einen Scheduler vorplanen kannst und enge Kontrolle über Generalisierung willst. Erwarte starke Ergebnisse auf großen Vision-Datasets wenn gepaart mit Cosinus- oder Step-Decay, aber sei bereit Momentum zu tunen (0.9 ist typischer Startpunkt) und halte Lernraten zwischen 0.01-0.1 je nach Batch-Größe.",
      "adam": "Adam Optimizer - Adaptive Erstordnungsmethode die gleitende Durchschnitte von Gradienten (beta1) und quadrierten Gradienten (beta2) speichert. Standard-Betas von 0.9/0.999 und eps von 1e-8 passen für die meisten Workloads. Behandelt rauschende oder spärliche Gradienten ohne manuelle Lernraten-Skalierung, was es zu einer verlässlichen Baseline für Klassifikation und Transfer-Learning macht. Achte auf träge Konvergenz wenn Weight Decay mit Adams adaptiven Updates gekoppelt ist; erwäge AdamW wenn Regularisierung wichtig ist.",
      "adamw": "AdamW Optimizer - Entkoppelt Weight Decay von Adams adaptiven Updates damit L2-Regularisierung sich wie beabsichtigt verhält. Behält dieselben Beta-Parameter und Epsilon-Defaults wie Adam bei während weight_decay als echter Regularisierer exponiert wird. Bevorzugt für Vision Transformers, ResNet-Finetuning oder jedes Modell wo du stabiles Training mit vorhersagbarer Generalisierung willst. Starte mit weight_decay um 0.01 und tune learning_rate zwischen 3e-5 und 3e-4 für Transfer-Learning-Szenarien.",
      "adamax": "AdaMax Optimizer - Adam-Variante die Unendlichkeitsnorm für zweite Moment-Verfolgung nutzt. Ähnliche Hyperparameter wie Adam, aber belastbarer wenn Gradienten sporadische Spitzen haben. Nützlich wenn Adam instabil wird durch extreme Gradienten-Magnituden, besonders in GAN- oder Reinforcement-Workloads. Halte beta2 nahe 0.999 und behandle learning_rate wie Standard-Adam; erwarte etwas langsamere Konvergenz aber weniger katastrophale Sprünge.",
      "nadam": "Nesterov-beschleunigter Adam - Fügt Nesterov-Momentum zu Adams adaptiver Skalierung hinzu. Teilt dieselben Betas und Epsilon aber führt Lookahead-Gradienten-Evaluation durch, was Konvergenz bei glatten Zielfunktionen straffen kann. Plane für moderaten Rechenaufwand pro Schritt. Empfohlen wenn Adam konvergiert aber früh stagniert; tune learning_rate etwas niedriger als einfacher Adam um Overshooting zu vermeiden (z.B. 1e-4 statt 3e-4).",
      "radam": "Rectified Adam - Adam mit automatischem Warmup-Mechanismus abgeleitet von Varianz-Rektifikation. Eliminiert Bedarf für manuellen Warmup-Schedule durch Schrumpfen der Schrittgrößen bis laufende Varianz stabilisiert. Hyperparameter entsprechen Adam-Defaults. Nutze es wenn du adaptives Verhalten brauchst aber dein Training sensitiv auf die ersten paar hundert Schritte ist. Funktioniert gut für kleine Datasets wo manueller Warmup überfitten würde.",
      "rmsprop": "RMSprop Optimizer - Hält exponentiellen Durchschnitt quadrierter Gradienten (alpha) zur Normalisierung von Updates. Standard alpha=0.99 und eps=1e-8. Historisch populär für rekurrente Netzwerke und Reinforcement Learning, funktioniert immer noch gut wenn Gradienten stark oszillieren und Adam zu aggressiv wirkt. Paare es mit abnehmenden learning_rate Schedule; typische Startwerte liegen um 1e-3 mit Momentum deaktiviert oder niedrig gesetzt (≤0.1).",
      "rprop": "Resilient Backpropagation - Vorzeichenbasierter Optimizer der per-Parameter Schrittgrößen adaptiert durch Gradienten-Vorzeichen-Wendungen. Ignoriert Batch-Größe weil es Full-Batch-Updates annimmt, daher selten geeignet für Mini-Batch-CNN-Training. Nutze es nur in deterministischen Settings (z.B. kleine Datasets mit Full-Batch-Passes) wo du schnelle second-order-ähnliche Konvergenz ohne Hessian-Speicherung willst. Hyperparameter eta_plus (1.2) und eta_minus (0.5) steuern Schritt-Adaptation.",
      "adagrad": "AdaGrad Optimizer - Akkumuliert quadrierte Gradienten und schrumpft Lernrate für häufig aktualisierte Gewichte. Fast wartungsfrei bei spärlichen Features, aber kumulative Summe zwingt effektive Lernrate gegen Null bei langen Läufen. Nutze es für Feature-Embeddings oder klassische spärliche NLP-Probleme, nicht für tiefe CNNs die hunderte Epochen trainieren. Typische initiale learning_rate ist 1e-2 mit Epsilon um 1e-10 um Division durch Null zu vermeiden.",
      "adadelta": "Adadelta Optimizer - Behebt Adagrads verschwindende Lernrate durch Tracking eines gleitenden Fensters quadrierter Gradienten und Updates. Erfordert fast kein manuelles Tuning jenseits rho (0.9) und eps (1e-6). Funktioniert bei rauschenden Zielfunktionen wo Adam zu aggressiv sein mag, obwohl finale Genauigkeit oft hinter AdamW zurückbleibt. Bevorzuge es wenn du manuelle Lernraten-Schedules vermeiden musst und trotzdem adaptives Verhalten brauchst.",
      "sparse_adam": "Sparse Adam - Adam mit Updates nur auf Indices die Gradienten erhalten, reduziert Speicher und Rechnung für Embedding-Tabellen. Nutzt dieselben Hyperparameter wie Adam aber nimmt an dass Gradienten fast überall Null sind. Essentiell für NLP-Modelle mit riesigen Vokabularen. Überspringe es für dichte konvolutionale Modelle; die sparse Update-Buchhaltung verschwendet nur Zeit.",
      "lbfgs": "L-BFGS Optimizer - Limited-Memory Quasi-Newton-Methode die inverse Hessian mittels vergangener Gradienten approximiert. Erfordert Full-Batch-Gradienten und Line-Search pro Schritt, also musst du ein Closure implementieren das Loss und Gradienten neu berechnet. Exzellent für Finetuning kleiner Modelle oder Lösen konvexer Probleme mit hoher Präzision. Nicht lebensfähig für großes Mini-Batch-Training weil jeder Schritt teuer ist und Speicher mit History-Größe wächst (max_iter und history_size steuern es).",
      "asgd": "Averaged SGD - Hält laufenden Durchschnitt von Parametern um Oszillationen durch rauschende Gradienten zu dämpfen. Du tunest immer noch die Basis-SGD learning_rate, aber Durchschnittsbildung schaltet nach averaging_start Epoche ein um Konvergenz zu glätten. Erwäge es wenn einfaches SGD am Ende wackelt aber du nicht zu Adam wechseln willst. Funktioniert am besten mit konstanten oder langsam abnehmenden Lernraten und ausgeschaltetem Momentum."
    },
    "training_scheduler_type_enum_descriptor": {
      "step_lr": "Step Learning Rate Scheduler - Multipliziert Lernrate mit Gamma alle step_size Epochen. Perfekt wenn du bereits die Epochen kennst wo Fortschritt nachlässt (z.B. 30/60/90 auf ImageNet). Wähle Gamma zwischen 0.1 und 0.3 und richte step_size nach deinem gesamten Epochen-Budget aus. Ohne Vorwissen kann es etwas abrupt wirken, also überwache Validierungs-Metriken um zu bestätigen dass Reduktionen helfen.",
      "multi_step_lr": "Multi-Step Learning Rate Scheduler - Verallgemeinerte Schritt-Schedule die Liste von Meilenstein-Epochen akzeptiert. Ermöglicht Staffelung mehrerer Raten-Reduktionen an beliebigen Punkten, ideal für Portierung von Schedules aus Papers oder vorherigen Experimenten. Halte Gamma identisch zwischen Meilensteinen außer du hast Grund es zu ändern, und stelle sicher dass Meilensteine streng steigende Ganzzahlen sind.",
      "exponential_lr": "Exponential Learning Rate Scheduler - Wendet lr_t = lr_0 * gamma^t an, gibt dir glatten Verfall im Tausch für sorgfältiges Gamma-Tuning. Funktioniert für sehr lange Läufe wo du graduellen Gleitflug statt diskreter Sprünge willst. Typische Gamma-Werte sitzen zwischen 0.97 und 0.995 für per-Epoche Updates. Kombiniere mit Warmup falls initiale Steigung zu steil für dein Modell ist.",
      "cosine_annealing_lr": "Cosine Annealing Learning Rate Scheduler - Fegt Lernrate runter entlang Cosinus-Kurve über T_max Epochen und startet optional bei eta_min neu. Bietet sanfte Landungen die finale Genauigkeit bei Vision-Modellen boosten. Setze T_max auf Anzahl Epochen in einem Zyklus und eta_min auf kleinen Floor wie lr_0 / 100. Nutze es wenn du automatisches Finetuning am Ende ohne manuelle Meilensteine willst.",
      "cosine_annealing_warm_restarts": "Cosine Annealing mit Warm Restarts - Wiederholt Cosinus-Verfall-Zyklen, setzt auf initiale Lernrate nach jedem Zyklus zurück. Großartig zum Entkommen aus flachen Minima während langer Trainings-Sessions. T_0 definiert erste Zyklus-Länge und T_mult skaliert nachfolgende Zyklus-Längen. Halte eta_min klein aber nicht Null um Einfrieren des Optimizers zu vermeiden.",
      "reduce_lr_on_plateau": "Reduce Learning Rate on Plateau - Überwacht Metrik (meist Validierungsverlust) und reduziert Lernrate um Faktor wenn Verbesserung für patience Epochen stagniert. Essentiell wenn du Plateau-Timing nicht vorhersagen kannst. Konfiguriere Cooldown um aufeinanderfolgende Trigger zu vermeiden und nutze Threshold um rauschende Metriken zu filtern. Gamma zwischen 0.1 und 0.5 trifft typischerweise das richtige Gleichgewicht.",
      "cyclic_lr": "Cyclic Learning Rate Scheduler - Zykliert Lernrate zwischen base_lr und max_lr über kurze Fenster, optional schrumpfende Amplitude mit Mode. Nützlich für schnelle Konvergenz bei schwierigen Zielfunktionen oder für LR-Bereichstests. Setze step_size_up/down auf Anzahl Iterationen pro Halbzyklus; halte max_lr etwa 3-10× base_lr. Paare mit Momentum-Cycling falls du cycle_momentum aktivierst.",
      "one_cycle_lr": "One Cycle Learning Rate Policy - Einzelner Sweep der Lernrate auf max_lr rampt dann zu Bruchteil des Basis-Werts abklingt während Momentum invertiert wird. Liefert schnelle Konvergenz wenn totale Trainings-Schritte bekannt sind. Gib entweder total_steps oder (epochs × steps_per_epoch) an; setze pct_start um Warmup-Anteil zu definieren (0.3 ist üblich). Funktioniert am besten mit SGD oder AdamW und erwartet keine zusätzlichen Scheduler.",
      "polynomial_lr": "Polynomial Learning Rate Scheduler - Verfällt Lernrate zu Null folgend (1 - t/T)^power. Wähle total_iters als Anzahl Optimizer-Schritte im Schedule und power um Krümmung zu steuern (1 für linear, 2 für quadratisch). Nützlich für Segmentierung und Detection-Workloads wo du deterministischen Gleitflug zu Null bis finale Iteration willst.",
      "linear_lr": "Linear Learning Rate Scheduler - Einfache lineare Interpolation zwischen start_factor und end_factor über total_iters Schritte. Ideal für Warmup (start_factor < 1) oder Cool-down-Phasen. Halte total_iters ausgerichtet mit Anzahl Iterationen die du für den Ramp abdecken willst; kombiniere mit anderem Scheduler für verbleibendes Trainings-Fenster.",
      "lambda_lr": "Lambda Learning Rate Scheduler - Direkter Hook der Basis-Lernrate mit deiner benutzerdefinierten lambda(epoch) Funktion multipliziert. Gibt volle Kontrolle für Forschungs-Schedules oder Curriculum Learning. Gib Python-Ausdruck der zu Float evaluiert; denk daran er wird string-evaluiert im Trainings-Prozess. Validiere Funktion sorgfältig—Syntax-Fehler oder negative Outputs töten deinen Lauf.",
      "multiplicative_lr": "Multiplicative Learning Rate Scheduler - Ähnlich lambda_lr aber erwartet Callable das Multiplikator pro Schritt zurückgibt, oft für epochenbasierte Skalierung. Gib Lambda das von Optimizer-Schritt-Anzahl abhängt statt Epoche falls du per-Iteration Kontrolle willst. Halte Multiplikatoren positiv und begrenzt; Werte >1 vergrößern Lernrate und können Training schnell destabilisieren."
    },
    "training_loss_type_enum_descriptor": {
      "cross_entropy": "Cross-Entropy Loss - Softmax + negative Log-Likelihood in einem Aufruf. Die erste Wahl für Single-Label-Klassifikation. Akzeptiert rohe Logits, behandelt Klassen-Ungleichgewicht via Gewicht oder label_smoothing und bietet kalibrierte Wahrscheinlichkeiten. Halte reduction='mean' für stabile Gradienten und überwache label_smoothing damit du Minderheits-Klassen nicht löschst.",
      "nll_loss": "Negative Log-Likelihood Loss - Dieselbe Mathematik wie Cross-Entropy aber erwartet dass du log_softmax selbst aufrufst. Nützlich wenn Modell bereits Log-Wahrscheinlichkeiten ausgibt (z.B. benutzerdefinierte Temperatur-Skalierung oder manuelles Mixed Precision). Stelle sicher dass Inputs Log-Wahrscheinlichkeiten sind; rohe Logits füttern wird still Müll produzieren.",
      "bce_loss": "Binary Cross-Entropy Loss - Funktioniert mit Wahrscheinlichkeiten in [0,1], also paare es mit explizitem Sigmoid. Geeignet für binäre Klassifikation wenn du Aktivierung separat kontrollieren willst. Hüte dich vor numerischem Underflow bei extremen Logits—clippe Inputs oder wechsle zu BCEWithLogitsLoss falls du NaNs siehst.",
      "bce_with_logits": "Binary Cross-Entropy mit Logits - Numerisch stabiler BCE der Sigmoid intern anwendet. Standard-Option für Multi-Label-Klassifikation und binäre Aufgaben. Unterstützt pos_weight für Klassen-Ungleichgewicht ohne manuelle Gewichtungs-Tricks. Outputs unbegrenzter Loss falls du vergisst Targets zu {0,1} zu clampen.",
      "multi_margin": "Multi-Class Margin Loss - Margin-basierte Klassifikations-Zielfunktion (Hinge-Stil) die korrekte Klassen-Score über andere um mindestens Margin drückt. Bietet optionale L1 oder L2 Normen via Parameter p. Nutze es wenn du Large-Margin-Verhalten statt probabilistische Cross-Entropy willst, aber beachte dass es langsamer konvergieren kann ohne sorgfältige Lernraten-Kontrolle.",
      "multi_label_margin": "Multi-Label Margin Loss - Erweitert Margin-Loss zu Multi-Label-Problemen durch Ranking positiver Klassen vor negativen. Erfordert Targets als Index-Listen kodiert und ist daher schwierig mit dichten Label-Tensoren zu integrieren. Reserviere es für Forschungs-Szenarien die explizit Margin-Ranking im Multi-Label-Raum aufrufen.",
      "multi_label_soft_margin": "Multi-Label Soft Margin Loss - Wendet Soft-Margin-Formulierung über Sigmoid-Aktivierungen an, produziert glattere Gradienten als harte Margin-Losses. Besser beim Umgang mit überlappenden Labels und Ungleichgewicht als vanilla BCE. Targets müssen noch {0,1} sein; erwäge Threshold-Tuning bei Inferenz um glattere Trainings-Landschaft auszunutzen.",
      "mse_loss": "Mean Squared Error Loss - Klassische L2 Regressions-Strafe. Bestraft große Fehler quadratisch was Outlier-Impact verstärkt. Großartig für Autoencoder und rauscharme Regression, aber erwäge Clipping extremer Targets oder Wechsel zu Huber wenn du Gradienten-Explosionen siehst.",
      "l1_loss": "L1 Loss (Mean Absolute Error) - Lineare Strafe auf absoluten Fehler, bietet Robustheit zu Outliern zum Preis langsamerer Konvergenz nahe Null. Nutze es wenn du Median-ähnliches Verhalten brauchst oder wenn deine Evaluations-Metrik MAE ist. Gradienten sind konstante Magnitude, also paare mit glatten Schedulern um Jitter zu vermeiden.",
      "smooth_l1": "Smooth L1 Loss - Huber-Stil Loss mit Beta-Region um L2-ähnlich nahe Null und L1 außerhalb zu sein. Standard für Bounding-Box-Regression (beta ≈ 1). Tune Beta falls deine Skala signifikant unterscheidet; kleineres Beta verengt quadratisches Fenster und gibt schärfere Strafen zu mittleren Fehlern.",
      "huber_loss": "Huber Loss - Ähnlich SmoothL1 aber parametrisiert durch Delta statt Beta. Gibt explizite Kontrolle über Switch-Punkt zwischen quadratischen und linearen Strafen. Exzellent für Regressions-Aufgaben mit gelegentlichen Outliern; setze Delta nahe erwartetem Rausch-Standard-Deviation.",
      "kl_div": "Kullback-Leibler Divergence Loss - Misst Divergenz zwischen vorhergesagter und Ziel-Verteilung. Erfordert Log-Wahrscheinlichkeiten als Input und rohe Wahrscheinlichkeiten als Target standardmäßig (oder umgekehrt mit log_target). Essentiell für Knowledge Distillation und variational Modelle. Doppel-Check Reduction-Modus; 'batchmean' bewahrt KL-Theorie (Division über Klassen und Mittelung über Batch).",
      "margin_ranking": "Margin Ranking Loss - Operiert auf Score-Paaren (x1, x2) mit Ground-Truth-Ordering y ∈ {−1, 1}. Trainiert Modell um x1 über x2 um Margin zu ranken wenn y=1. Kombiniere mit sorgfältigem Sampling positiver/negativer Paare oder Triplets—zufällige Paare geben selten nützliches Signal.",
      "hinge_embedding": "Hinge Embedding Loss - Für Ähnlichkeits-Learning wo Labels anzeigen ob Paare nah (+1) oder fern (−1) sein sollen. Bestraft Distanzen die spezifizierte Margin verletzen. Nutze es wenn du nur binäre Same/Different-Supervision bekommst und Embeddings entsprechend clustern willst.",
      "triplet_margin": "Triplet Margin Loss - Konsumiert Anchor, positive und negative Embeddings und erzwingt Margin zwischen positiven und negativen Distanzen. Erfordert harte oder semi-harte Triplet-Mining um zu glänzen; naive zufällige Triplets verschwenden meist Rechnung. Margin defaultet zu 1.0 aber tune es basierend auf Embedding-Skala (kleiner für normalisierte Vektoren).",
      "cosine_embedding": "Cosine Embedding Loss - Optimiert Cosinus-Ähnlichkeit direkt, betont Winkel-Distanz über Magnitude. Ideal wenn Vektoren normalisiert sind oder wenn Richtung Semantik trägt (z.B. Gesichtserkennung). Stelle sicher dass Embeddings normalisiert sind um Magnitude-Effekte zurück zu vermeiden.",
      "ctc_loss": "Connectionist Temporal Classification Loss - Richtet variable-Längen Inputs zu Ziel-Label-Sequenzen ohne Frame-Level-Annotation aus. Erfordert Log-Wahrscheinlichkeiten mit Größe (T, N, C) und Ziel-Sequenzen ohne eingefügte Blanks (Loss handhabt Blanks). Konfiguriere Blank-Index und stelle sicher Targets sind nach Sample sortiert; falsche Ziel-Längen werfen Runtime-Fehler.",
      "poisson_nll": "Poisson Negative Log-Likelihood Loss - Für Modellierung von Zähldaten wo Targets nicht-negative Ganzzahlen sind. Akzeptiert log_input um positive Vorhersagen zu erzwingen oder volle Logits mit Clamp über Null. Setze full=True falls dein Modell rohe Raten vorhersagt. Füttere keine negativen Targets; Verteilungs-Annahme bricht sofort zusammen.",
      "gaussian_nll": "Gaussian Negative Log-Likelihood Loss - Trainiert Modell um sowohl Mittelwert als auch Varianz für kontinuierliche Targets vorherzusagen. Erwartet dass Modell (Mittelwert, Varianz) Tensoren zurückgibt. Unterstützt volle Kovarianz via cholesky_factor; sonst muss Varianz positiv bleiben. Großartig für Uncertainty-aware Regression; füge kleines Epsilon zu Varianz hinzu um log(0) Probleme zu vermeiden."
    },
    "training_loss_reduction_enum_descriptor": {
      "mean": "Mean Reduction - Berechnet durchschnittlichen Loss über alle Batch-Elemente durch Division des totalen Loss durch Batch-Größe. Beeinflusst Gradienten-Magnitude-Normalisierung, Batch-Größen-Unabhängigkeit, Trainings-Stabilität und Lernraten-Sensitivität. Steuert Loss-Skalierung um konsistente Gradienten unabhängig von Batch-Größen-Variationen zu bieten. Standard-Wahl für die meisten Trainings-Szenarien da es Gradienten-Magnituden proportional zu individuellen Sample-Fehlern statt Batch-Größe hält.",
      "sum": "Sum Reduction - Berechnet totalen Loss durch Summierung aller individuellen Sample-Losses im Batch ohne Normalisierung. Beeinflusst Gradienten-Magnitude-Skalierung, Batch-Größen-Abhängigkeit, Lernraten-Anforderungen und Trainings-Dynamik. Steuert Loss-Akkumulation die größere Gradienten für größere Batches resultiert, erfordert Lernraten-Anpassung proportional zu Batch-Größe. Nützlich wenn du willst dass Gradienten-Magnitude mit Anzahl verarbeiteter Samples skaliert.",
      "none": "No Reduction - Gibt individuelle Loss-Werte für jedes Sample im Batch ohne Aggregations-Operation zurück. Beeinflusst benutzerdefinierte Loss-Gewichtungs-Fähigkeiten, sample-spezifische Analyse, manuelle Loss-Kombination und erweiterte Trainings-Strategien. Steuert individuellen Sample-Loss-Zugang um benutzerdefinierte Reduktions-Schemes, Sample-Wichtigkeits-Gewichtung oder detaillierte Loss-Analyse zu implementieren. Essentiell für erweiterte Anwendungen die per-Sample Loss-Manipulation erfordern."
    },
    "training_early_stopping_monitor_enum_descriptor": {
      "val_loss": "Validation Loss Monitoring - Early-Stopping-Mechanismus der Validierungs-Loss-Werte verfolgt um zu bestimmen wann Training wegen mangelnder Verbesserung angehalten werden soll. Beeinflusst Overfitting-Vermeidung, Trainings-Dauer-Optimierung, Modell-Generalisierungs-Qualität und Rechenressourcen-Verbrauch. Steuert Trainings-Termination basierend auf Loss-Plateauing, was typisch anzeigt dass Modell generalisierbare Muster gelernt hat und weiteres Training zu Overfitting führen kann. Besonders effektiv für Regressions-Aufgaben und Situationen wo Loss-Minimierung direkt mit Modell-Qualität korreliert.",
      "val_accuracy": "Validation Accuracy Monitoring - Early-Stopping-Mechanismus der Validierungs-Genauigkeits-Metriken verfolgt um optimalen Trainings-Terminations-Punkt zu bestimmen. Beeinflusst Modell-Performance-Optimierung, Overfitting-Detektion, Trainings-Effizienz und finale Modell-Qualität. Steuert Trainings-Halt basierend auf Genauigkeits-Plateaus, fokussiert auf Klassifikations-Performance statt Loss-Minimierung. Am geeignetsten für ausgewogene Klassifikations-Aufgaben wo Genauigkeit primäre Erfolgs-Metrik ist und gut mit Modell-Generalisierungs-Fähigkeit korreliert."
    },
    "optimizers_defaults_lbfgs_line_search_fn_oneOf[1]_enum_descriptor": {
      "strong_wolfe": "Strong Wolfe Line Search - Erweiterte Line-Search-Algorithmus für L-BFGS-Optimierung der sowohl ausreichende Abnahme (Armijo-Bedingung) als auch Krümmungs-Bedingungen (starke Wolfe-Bedingungen) sicherstellt. Beeinflusst Optimierungs-Konvergenz-Qualität durch Garantie angemessener Schrittgrößen die mathematische Optimalitäts-Kriterien erfüllen. Steuert Schritt-Längen-Auswahl durch rigorose mathematische Bedingungen die Konvergenz-Eigenschaften sicherstellen während Recheneffizienz beibehalten wird. Essentiell für L-BFGS theoretische Garantien und bietet robuste Schrittgrößen-Auswahl für Quasi-Newton-Optimierungs-Methoden."
    },
    "schedulers_defaults_reduce_lr_on_plateau_mode_enum_descriptor": {
      "min": "Minimum Mode - Überwacht Metriken wo niedrigere Werte bessere Performance anzeigen (wie Validierungs-Loss). Beeinflusst Lernraten-Reduktions-Trigger durch Verfolgung wann überwachte Metrik aufhört unter Threshold für spezifizierte Gedulds-Periode zu fallen. Steuert Scheduler-Verhalten um Lernrate zu reduzieren wenn Loss plateaut, verhindert Trainings-Stagnation. Optimal für Loss-basierte Überwachung wo fallende Werte Trainings-Fortschritt repräsentieren.",
      "max": "Maximum Mode - Überwacht Metriken wo höhere Werte bessere Performance anzeigen (wie Validierungs-Genauigkeit). Beeinflusst Lernraten-Reduktions-Trigger durch Verfolgung wann überwachte Metrik aufhört über Threshold für spezifizierte Gedulds-Periode zu steigen. Steuert Scheduler-Verhalten um Lernrate zu reduzieren wenn Genauigkeit plateaut, ermöglicht weiteres Finetuning. Optimal für Genauigkeits-basierte Überwachung wo steigende Werte Trainings-Fortschritt repräsentieren."
    },
    "schedulers_defaults_reduce_lr_on_plateau_threshold_mode_enum_descriptor": {
      "rel": "Relative Threshold Mode - Definiert Verbesserungs-Threshold als Prozentsatz des aktuellen besten Metrik-Werts. Beeinflusst Sensitivität zu Metrik-Verbesserungen durch Erfordern proportionaler Änderungen relativ zu aktuellem Performance-Level. Steuert adaptive Threshold-Skalierung die stringenter wird je besser Modell-Performance wird. Nützlich wenn Verbesserungs-Magnitude mit aktuellen Metrik-Werten skalieren soll, verhindert vorzeitige Lernraten-Reduktion in gut-performenden Modellen.",
      "abs": "Absolute Threshold Mode - Definiert Verbesserungs-Threshold als festen absoluten Wert der überschritten werden muss unabhängig von aktuellem Metrik-Level. Beeinflusst Verbesserungs-Detektions-Sensitivität durch konstante Threshold-Anforderungen unabhängig von aktueller Performance. Steuert einheitliche Verbesserungs-Standards während gesamtem Training unabhängig von Metrik-Magnitude. Nützlich wenn konsistente Verbesserungs-Level erforderlich sind unabhängig von aktuellem Modell-Performance-Zustand."
    },
    "schedulers_defaults_cyclic_lr_mode_enum_descriptor": {
      "triangular": "Triangular Cycle Mode - Erstellt grundlegende dreieckige Lernraten-Zyklen mit konstanter Amplitude während gesamtem Training. Beeinflusst Lernraten-Oszillations-Muster durch lineare Anstiege und Abnahmen zwischen base_lr und max_lr Grenzen. Steuert konsistente Exploration-Exploitation-Balance mit fixem Lernraten-Bereich. Bietet einfache zyklische Lernraten-Muster geeignet für Finden optimaler Lernraten-Bereiche ohne Verfall.",
      "triangular2": "Triangular2 Cycle Mode - Erstellt dreieckige Lernraten-Zyklen mit Amplituden-Halbierung nach jedem kompletten Zyklus. Beeinflusst Lernraten-Bereichs-Reduktion über Zeit während dreieckiges Oszillations-Muster beibehalten wird. Steuert gradueller Lernraten-Verfall innerhalb zyklischen Frameworks, kombiniert Vorteile des Cycling mit progressiver Verfeinerung. Ermöglicht aggressive initiale Exploration mit zunehmend konservativen Lernraten-Anpassungen.",
      "exp_range": "Exponential Range Mode - Skaliert Lernraten-Zyklus-Amplitude exponentiell mittels Gamma-Faktor für dynamische Bereichs-Anpassung. Beeinflusst Lernraten-Grenz-Modifikation durch exponentielles Skalieren von max_lr relativ zu Zyklus-Nummer. Steuert sophisticated Amplituden-Evolution die Zyklus-Magnitude basierend auf Gamma-Wert erhöhen oder verringern kann. Bietet erweiterte zyklische Lernraten-Muster mit exponentieller Amplituden-Modulation."
    },
    "schedulers_defaults_cyclic_lr_scale_mode_enum_descriptor": {
      "cycle": "Cycle-based Scaling Mode - Wendet Skalierungs-Funktion basierend auf abgeschlossener Zyklus-Anzahl an statt individueller Iterations-Schritte. Beeinflusst Skalen-Funktions-Evaluations-Häufigkeit und Lernraten-Amplituden-Modifikations-Rhythmus. Steuert Skalierungs-Anwendung an Zyklus-Grenzen, ermöglicht verschiedene Amplituden-Anpassungen für jeden kompletten Lernraten-Zyklus. Nützlich wenn Skalierungs-Verhalten diskret zwischen Zyklen ändern soll statt kontinuierlich während Training.",
      "iterations": "Iteration-based Scaling Mode - Wendet Skalierungs-Funktion basierend auf totaler Iterations-Anzahl seit Training-Start an, bietet kontinuierliche Skalierungs-Evolution. Beeinflusst Skalen-Funktions-Evaluation bei jedem Schritt, ermöglicht glatte Amplituden-Übergänge während Trainings-Prozess. Steuert feinkörnige Skalierungs-Anwendung die kontinuierlich entwickelt statt an diskreten Zyklus-Grenzen. Nützlich wenn graduelle, kontinuierliche Skalierungs-Änderungen bevorzugt werden."
    },
    "schedulers_defaults_one_cycle_lr_anneal_strategy_enum_descriptor": {
      "cos": "Cosine Annealing Strategy - Nutzt Cosinus-Funktion für glatte Lernraten-Übergänge mit graduellen Änderungen an Extremen und steileren Änderungen in mittleren Regionen. Beeinflusst Lernraten-Trajektorie-Glätte durch Bereitstellung natürlicher Beschleunigungs- und Verlangsamungs-Phasen. Steuert sinusoidale Lernraten-Evolution die natürliche Optimierungs-Dynamik nachahmt. Besonders effektiv für Erreichen glatter Konvergenz mit reduzierten Oszillationen nahe Lernraten-Grenzen.",
      "linear": "Linear Annealing Strategy - Nutzt lineare Interpolation für konstante Raten-Lernraten-Änderungen während Zyklus. Beeinflusst Lernraten-Übergänge durch einheitliche Progression ohne Beschleunigungs- oder Verlangsamungs-Phasen. Steuert vorhersagbare, stetige Lernraten-Evolution mit konsistenter Änderungs-Rate. Einfachere Alternative wenn glatte Cosinus-Übergänge nicht erforderlich sind und einheitliche Lernraten-Progression bevorzugt wird."
    },
    "losses_defaults_multi_margin_p_enum_descriptor": {
      "1": "L1 Norm (Manhattan Distance) - Nutzt absolute Unterschiede für Margin-Berechnung in Multi-Margin-Loss-Funktion. Beeinflusst Loss-Berechnung durch lineare Strafe die alle Fehler einheitlich behandelt unabhängig von Magnitude. Steuert Distanz-Messung mittels Summe absoluter Unterschiede zwischen vorhergesagten und Ziel-Werten. Bietet robuste Margin-Berechnung weniger sensitiv zu Outliern verglichen mit L2-Norm, geeignet wenn Trainings-Daten signifikantes Rauschen oder extreme Werte enthalten.",
      "2": "L2 Norm (Euclidean Distance) - Nutzt quadrierte Unterschiede für Margin-Berechnung in Multi-Margin-Loss-Funktion. Beeinflusst Loss-Berechnung durch quadratische Strafe die größere Fehler stark bestraft während nachsichtig bei kleineren ist. Steuert Distanz-Messung mittels Summe quadrierter Unterschiede zwischen vorhergesagten und Ziel-Werten. Bietet glatte Gradienten-Charakteristika und starke Strafe für große Margin-Verletzungen, geeignet für saubere Daten wo große Fehler stark entmutigt werden sollen."
    },
    "losses_defaults_kl_div_reduction_enum_descriptor": {
      "none": "No Reduction - Gibt individuelle KL-Divergenz-Werte für jedes Sample ohne Aggregation zurück. Beeinflusst Loss-Ausgabe-Dimensionalität durch Bewahrung per-Sample Loss-Werte für benutzerdefinierte Verarbeitung oder Analyse. Steuert individuellen Sample-Loss-Zugang ermöglicht sample-spezifische Gewichtung, Filterung oder detaillierte Loss-Untersuchung. Essentiell für erweiterte Trainings-Strategien die per-Sample Loss-Manipulation erfordern oder bei Implementierung benutzerdefinierter Reduktions-Schemes.",
      "mean": "Mean Reduction - Berechnet durchschnittliche KL-Divergenz über alle Elemente im Batch inklusive räumlicher Dimensionen. Beeinflusst Gradienten-Skalierung durch Normalisierung der Loss-Magnitude relativ zu totaler Anzahl Elemente statt Batch-Größe allein. Steuert Loss-Skalierung die sowohl Batch-Größe als auch räumliche Dimensionen in dichten Vorhersage-Aufgaben berücksichtigt. Bietet konsistente Gradienten unabhängig von Input-Auflösung oder Batch-Komposition.",
      "sum": "Sum Reduction - Berechnet totale KL-Divergenz durch Summierung aller individuellen Element-Divergenzen ohne Normalisierung. Beeinflusst Gradienten-Magnitude-Skalierung proportional zu totaler Anzahl Elemente in Batch und räumlichen Dimensionen. Steuert Loss-Akkumulation die größere Gradienten für größere Batches oder höhere Auflösungs-Inputs resultiert. Erfordert sorgfältige Lernraten-Anpassung wenn Batch-Größen oder Input-Dimensionen signifikant variieren.",
      "batchmean": "Batch Mean Reduction - Berechnet mittlere KL-Divergenz über Batch-Dimension nur, bewahrt räumliche Dimensions-Beiträge. Beeinflusst Loss-Berechnung durch Mittelung über Samples während voller räumlicher Loss-Beitrag beibehalten wird. Steuert Standard KL-Divergenz-Reduktion die auf sample-weise Mittelung fokussiert statt element-weise Normalisierung. Empfohlene Reduktions-Methode für KL-Divergenz-Loss da es theoretische Eigenschaften bewahrt während stabile Trainings-Dynamik bietet."
    },
    "models_resnet_default_optimizer_type_enum_descriptor": {
      "adamw": "AdamW Optimizer für ResNet - Adam mit entkoppeltem Weight Decay spezifisch getuned für ResNet-Architekturen. Beeinflusst Regularisierungs-Effektivität und Trainings-Stabilität durch ordnungsgemäße Trennung gradienten-basierter Adaptation und L2-Strafe. Steuert Weight Decay unabhängig von Gradienten-Updates, verhindert Regularisierungs-Interferenz mit adaptiven Lernraten. Empfohlene Standard-Wahl für ResNet-Modelle wegen überlegener Generalisierungs-Performance und stabiler Trainings-Charakteristika über verschiedene ResNet-Tiefen und Datasets.",
      "adam": "Adam Optimizer für ResNet - Klassischer adaptive Moment Estimation Algorithmus bietet automatische Lernraten-Skalierung für ResNet-Training. Beeinflusst Konvergenz-Geschwindigkeit und Hyperparameter-Sensitivität durch per-Parameter Lernraten-Adaptation. Steuert Momentum und quadrierte Gradienten-Schätzungen um Exploration und Exploitation während ResNet-Optimierung zu balancieren. Gute Allzweck-Wahl bietet verlässliche Performance über verschiedene ResNet-Varianten mit minimalen Hyperparameter-Tuning-Anforderungen.",
      "sgd": "SGD mit Momentum für ResNet - Traditioneller Stochastic Gradient Descent mit Momentum spezifisch konfiguriert für ResNet-Training. Beeinflusst Trainings-Dynamik durch Momentum-Akkumulation und erfordert sorgfältige Lernraten-Scheduling für optimale Performance. Steuert Parameter-Updates durch Momentum-basierte Beschleunigung während deterministisches Optimierungs-Verhalten beibehalten wird. Traditionelle Wahl die exzellente Ergebnisse mit ordnungsgemäßem Tuning erreicht aber sorgfältigere Hyperparameter-Auswahl erfordert verglichen mit adaptiven Optimizern."
    },
    "models_resnet_default_scheduler_type_enum_descriptor": {
      "step_lr": "Step Learning Rate für ResNet - Schritt-basierter Lernraten-Verfall mit vorbestimmten Meilenstein-Epochen optimiert für ResNet-Trainings-Phasen. Beeinflusst Lernraten-Reduktion bei spezifischen Trainings-Stadien entsprechend ResNet-Konvergenz-Mustern. Steuert deterministischen Lernraten-Schedule der sich an typische ResNet-Trainings-Progression und Loss-Landschafts-Charakteristika ausrichtet. Geeignet wenn optimale Verfall-Epochen aus vorherigen ResNet-Experimenten oder Forschungs-Papers mit ähnlichen Datasets und Architekturen bekannt sind.",
      "cosine_annealing_lr": "Cosine Annealing für ResNet - Glatter Cosinus-basierter Lernraten-Schedule der oft ResNet finale Genauigkeit durch graduellen Lernraten-Verfall verbessert. Beeinflusst Trainings-Dynamik durch Bereitstellung glatter Übergänge die ResNet-Modellen helfen bessere finale Konvergenz zu erreichen. Steuert sinusoidale Lernraten-Progression die Trainings-Oszillationen reduziert und feingetunete Parameter-Anpassung in späteren Trainings-Phasen ermöglicht. Erreicht häufig überlegene finale Genauigkeit verglichen mit schritt-basierten Schedules für ResNet-Architekturen.",
      "reduce_lr_on_plateau": "Adaptives Scheduling für ResNet - Plateau-basierte Lernraten-Reduktion die automatisch auf ResNet-Trainings-Stagnations-Perioden reagiert. Beeinflusst Lernraten-Adaptation durch Validierungs-Metrik-Überwachung, reduziert Lernrate wenn ResNet-Trainings-Fortschritt plateaut. Steuert automatische Schedule-Anpassung die auf aktuelle Trainings-Dynamik reagiert statt vorbestimmte Meilensteine. Optimal für ResNet-Training wenn optimales Verfall-Timing unbekannt ist oder wenn Trainings-Charakteristika über verschiedene Datasets oder experimentelle Bedingungen variieren."
    },
    "paths_description": "Pfad-Konfiguration",
    "paths_projects_dir_description": "Verzeichnis mit Projekt-Ordnern",
    "paths_ui_dir_description": "Verzeichnis mit UI-Assets",
    "paths_config_dir_description": "Verzeichnis mit Konfigurationsdateien",
    "paths_localizations_dir_description": "Verzeichnis mit Lokalisierungsdateien",
    "paths_packages_file_description": "Pfad zur packages.jsonc Datei",
    "paths_mappings_file_description": "Pfad zur Mapping-Datei",
    "paths_cache_dir_description": "Verzeichnis für Cache-Dateien"
  },
  "status_graph": {
    "epoch_accuracy": "Epoch-Accuracy",
    "epoch_loss": "Epoch-Loss",
    "step_loss": "Step-Loss",
    "learning_rate": "Lernrate",
    "loss": "Loss",
    "no_data": "Warten auf Updates",
    "no_training": "Kein aktives Training.",
    "active_count": "Aktive Trainings: {count}",
    "label_training_id": "Training-ID",
    "label_status": "Status",
    "label_phase": "Phase",
    "label_epoch": "Epoche",
    "label_step": "Schritt",
    "badge_training": "Training: {project}",
    "footer_training": "Training {project} — Epoche {epoch} • Schritt {step}"
  },
  "updates": {
    "log": {
      "check_started": "Prüfe Upstream-Checksums...",
      "check_complete": "Update-Check abgeschlossen. Ausstehende Dateien: {count}",
      "check_failed": "Update-Check fehlgeschlagen: {error}",
      "remote_config_failed": "Remote config.json konnte nicht geladen werden: {error}",
      "remote_checksum_failed": "Remote-Checksum-Manifest konnte nicht geladen werden: {error}",
      "remote_payload_invalid": "Remote-Payload von {url} war kein Mapping.",
      "local_checksum_missing": "Lokale checksum.json fehlt; nehme leeres Manifest an.",
      "local_checksum_invalid": "Lokale checksum.json konnte nicht geparst werden: {error}",
      "path_escape": "Unsicherer Pfad blockiert: {path}",
      "apply_started": "Wende Updates an...",
      "apply_failed": "Konnte Updates nicht anwenden: {error}",
      "apply_nothing": "Keine Updates erforderlich.",
      "apply_file_success": "Aktualisiert: {path}",
      "apply_file_failed": "Aktualisierung fehlgeschlagen: {path}: {error}",
      "apply_partial": "{updated} Updates angewendet mit {failed} Fehlern.",
      "apply_complete": "{count} Updates angewendet."
    },
    "api": {
      "check_success": "Update-Check abgeschlossen. {count} Datei(en) ausstehend.",
      "check_no_updates": "Alles ist auf dem neuesten Stand.",
      "check_failed": "Update-Check fehlgeschlagen: {error}",
      "apply_success": "Updates erfolgreich angewendet. {updated} Datei(en) aktualisiert.",
      "apply_partial": "Updates teilweise angewendet: {updated} erfolgreich, {failed} fehlgeschlagen.",
      "apply_failed": "Updates konnten nicht angewendet werden: {error}",
      "apply_nothing": "Keine Updates notwendig."
    },
    "status": {
      "missing": "Lokal fehlt",
      "outdated": "Checksum mismatch"
    }
  },
  "status": {
    "project_load_failed": "Projekt-Ladevorgang fehlgeschlagen",
    "project_loading": "Lade Projekt {projectName}...",
    "project_loaded_custom": "Projekt {projectName} mit benutzerdefinierter Konfig geladen",
    "project_loaded_defaults": "Projekt {projectName} mit globalen Standardwerten geladen",
    "project_load_error": "Projekt {projectName} konnte nicht geladen werden: {error}",
    "no_project_loaded": "Kein Projekt geladen",
    "validation_errors": "Behebe Validierungsfehler vor dem Speichern",
    "saving_training_config": "Speichere Trainingskonfiguration...",
    "training_config_saved": "Trainingskonfiguration gespeichert",
    "save_failed": "Speichern fehlgeschlagen",
    "loading_schema": "Lade Schema & Konfiguration...",
    "init_failed": "Initialisierung fehlgeschlagen",
    "memory_load_failed": "Speicher laden fehlgeschlagen",
    "checking_updates": "Prüfe auf Updates...",
    "updates_ready": "Updates verfügbar",
    "updates_none": "Keine Updates verfügbar",
    "updates_check_failed": "Update-Prüfung fehlgeschlagen",
    "updates_applying": "Wende Updates an...",
    "updates_applied": "Updates erfolgreich angewendet",
    "updates_apply_failed": "Update-Anwendung fehlgeschlagen",
    "generating_heatmap": "Erzeuge Heatmap...",
    "heatmap_generated": "Heatmap erzeugt",
    "heatmap_generation_failed": "Heatmap-Erzeugung fehlgeschlagen",
    "saving_system_settings": "Speichere Systemeinstellungen...",
    "system_settings_saved": "Systemeinstellungen gespeichert",
    "starting_training": "Starte Training...",
    "training_started": "Training gestartet",
    "training_stopping": "Stoppe Training...",
    "training_stop_requested": "Trainingsstopp angefordert",
    "training_stop_failed": "Stoppen des Trainings fehlgeschlagen",
    "training_start_failed": "Starten des Trainings fehlgeschlagen",
    "switching_language": "Wechsle Sprache...",
    "language_switched": "Sprache erfolgreich gewechselt",
    "language_switch_failed": "Sprachwechsel fehlgeschlagen",
    "augmentation_preview_ready": "Augmentations-Vorschau erstellt"
  }
}